source_dir: /Users/allwefantasy/projects/byzer-llm
target_file: /Users/allwefantasy/projects/byzer-llm/output.txt
urls: /tmp/README.md

model: kimi_chat
# model_max_length: 2000
# model_max_input_length: 6000
# anti_quota_limit: 5

index_model: sparkdesk_chat
index_model_max_length: 2000
index_model_max_input_length: 10000
index_model_anti_quota_limit: 1

skip_build_index: false
index_filter_level: 0


execute: true
auto_merge: true
human_as_model: true

query: >
  给  src/byzerllm/__init__.py 中的 prompt 类增加测试用例，放到 ./tests/test_prompt.py 中。  

  参考下面的示例来写 pytest 测试用例：

  ```python
  import byzerllm
  import inspect 
  byzerllm.connect_cluster()
  llm = byzerllm.ByzerLLM()
  model = "qianwen_chat"
  llm.setup_default_model_name(model)
  llm.setup_template(model,"auto")

  class Wow:
      def __init__(self,llm):
          self.llm = llm

      @byzerllm.prompt(lambda self:self.llm)
      def test(self,s:str)->str: 
          '''
          Hello, {{ s }}!
          '''

      @byzerllm.prompt()
      def test2(self,s:str)->str: 
          '''
          Hello, 现在假设你的名字是威廉。{{ s }}!
          '''    

  @byzerllm.prompt(llm=llm,options={"llm_config":{}})
  def test(s:str)->str: 
      '''
      Hello, {{ s }}!
      '''  


  w = Wow(llm)

  print(test("你是谁"))    
  print(w.test.prompt("你是谁"))
  print(w.test.options({"model":"kimi_8k_chat","llm_config":{"max_length":1000}}).run("你是谁"))
  print(w.test("你是谁"))
  print(test.run("你是谁"))
  print(test.prompt("你是谁"))
  llm.chat_oai("你是谁")
  print(w.test.with_llm(llm).run("你是谁")) 

  ```

  注意：过滤的时候，请只过滤 byzerllm.__init__.py 文件。
