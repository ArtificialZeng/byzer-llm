source_dir: /Users/allwefantasy/projects/byzer-llm
target_file: /Users/allwefantasy/projects/byzer-llm/output.txt
urls: https://raw.githubusercontent.com/abetlen/llama-cpp-python/main/README.md

model: gpt3_5_chat
enable_multi_round_generate: true
index_model: haiku_chat

skip_build_index: false
index_filter_level: 0

execute: true
auto_merge: true
human_as_model: true

query: >
  参考 README.md 以及 auto/__init__.py 文件， 重新实现 backend_llama_cpp.py （对应的类名为 LlamaCppBackend） 
  注意：
  1. 请让 generate 函数返回的结果是这样的：

  [(generated_text,{"metadata":{
        "request_id":final_output.request_id,
        "input_tokens_count":input_tokens_count,
        "generated_tokens_count":generated_tokens_count,
        "time_cost":time_cost,
        "first_token_time":first_token_time-current_time_milliseconds,
        "speed":float(generated_tokens_count)/time_cost*1000,
        "prob":prob
    }})] 

  2. generate 支持 stream 参数  ，将生成的结果仿照 async_vllm_chat 中的实现，数据放入 VLLM_STREAM_SERVER 中。

  3. 在 tests/auto 目录下生成合适的测试用例。




