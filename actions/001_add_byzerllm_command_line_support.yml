source_dir: /home/winubuntu/projects/byzer-llm/saas
target_file: /home/winubuntu/projects/byzer-llm/output.txt

model: qianwen_short_chat
model_max_length: 2000
anti_quota_limit: 5

urls: https://raw.githubusercontent.com/allwefantasy/byzer-llm/master/README.md

search_engine: bing
search_engine_token: ENV {{BING_SEARCH_TOKEN}}

query: |
  在 src/byzerllm 目录下新增一个 byzerllm.py 文件。在该文件中使用args 实现命令行支持。 参考 README.md 中的使用方法来增加命令行参数。
  主要支持：
  1. 部署模型相关参数
  2. 运行推理相关阐述

  比如部署模型，一般代码是这样的：
  
  ```python
  ray.init(address="auto",namespace="default",ignore_reinit_error=True)
  llm = ByzerLLM()

  llm.setup_gpus_per_worker(4).setup_num_workers(1)
  llm.setup_infer_backend(InferBackend.transformers)

  llm.deploy(model_path="/home/byzerllm/models/openbuddy-llama2-13b64k-v15",
            pretrained_model_type="custom/llama2",
            udf_name="llama2_chat",infer_params={})
  ```
  此时你需要有 address, num_workers, gpus_per_worker, model_path, pretrained_model_type, udf_name, infer_params 这些参数可以通过命令行传递。

  最终形态是： 

  byzerllm deploy --model_path /home/byzerllm/models/openbuddy-llama2-13b64k-v15 --pretrained_model_type custom/llama2 --udf_name llama2_chat --infer_params {}

  同理推理是也是。比如一般推理代码是：

  ```python
  llm_client = ByzerLLM()
  llm_client.setup_template("llama2_chat","auto")

  v = llm.chat_oai(model="llama2_chat",conversations=[{
      "role":"user",
      "content":"hello",
  }])

  print(v[0].output)
  ```
  此时你需要有 model, conversations 这些参数可以通过命令行传递。

  此时你的命令行形态是：
  
  byzerllm query --model llama2_chat --query "hello" --template "auto"
  