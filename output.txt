好的,我会根据您的要求,对代码进行如下修改:

1. 在`byzerllm_command.py`中添加一个新的命令`serve`,并接收需要的参数。
2. 将`api_server.py`中的服务启动逻辑抽取到`byzerllm.utils.client.entrypoints.openai.serve.py`中。
3. 在`byzerllm_command.py`的`main()`函数中,根据命令行参数调用`serve()`函数启动服务。

下面是修改后的diff:

```diff
--- /Users/allwefantasy/projects/byzer-llm/src/byzerllm/byzerllm_command.py
+++ /Users/allwefantasy/projects/byzer-llm/src/byzerllm/byzerllm_command.py
@@ ... @@
+from byzerllm.utils.client.entrypoints.openai.serve import serve, ServerArgs
@@ ... @@
+    StoreNestedDict, get_command_args
+
+def main():
@@ ... @@
+    elif args.command == 'serve':
+        byzerllm.connect_cluster(address=args.ray_address) 
+        llm_client = ByzerLLM()
+        if args.served_model_name:
+            llm_client.setup_template(args.served_model_name, args.template)                 
+        server_args = ServerArgs(**{arg: getattr(args, arg) for arg in vars(ServerArgs())})                      
+        serve(llm=llm_client, args=server_args)            
           

--- /Users/allwefantasy/projects/byzer-llm/src/byzerllm/utils/client/byzerllm_client.py
+++ /Users/allwefantasy/projects/byzer-llm/src/byzerllm/utils/client/byzerllm_client.py
@@ ... @@
+from byzerllm.utils.client.types import (
+    ServerArgs,
@@ ... @@
+)

class ByzerLLM:
@@ ... @@ 

+++ /Users/allwefantasy/projects/byzer-llm/src/byzerllm/utils/client/entrypoints/openai/serve.py
@@ ... @@
+from byzerllm.utils.client.types import ServerArgs

+def serve(llm:ByzerLLM, args: ServerArgs):
+    
+    logger.info(f"ByzerLLM API server version {version}")
+    logger.info(f"args: {args}")
+
+    router_app.add_middleware(
+        CORSMiddleware,
+        allow_origins=args.allowed_origins,
+        allow_credentials=args.allow_credentials,
+        allow_methods=args.allowed_methods,
+        allow_headers=args.allowed_headers,
+    )
+    
+    if token := os.environ.get("BYZERLLM_API_KEY") or args.api_key:
+
+        @router_app.middleware("http")
+        async def authentication(request: Request, call_next):
+            if not request.url.path.startswith("/v1"):
+                return await call_next(request)
+            if request.headers.get("Authorization") != "Bearer " + token:
+                return JSONResponse(
+                    content={"error": "Unauthorized"},
+                    status_code=401
+                )
+            return await call_next(request)
+
+    # Register labels for metrics
+    # add_global_metrics_labels(model_name=engine_args.model)
+    global llm_client
+    llm_client = llm
+    
+    global openai_serving_chat
+    openai_serving_chat = OpenAIServingChat(
+        llm_client=llm_client,
+        response_role=args.response_role,
+        server_model_name=args.served_model_name,
+        prompt_template=args.prompt_template
+    )
+    global openai_serving_completion
+    openai_serving_completion = OpenAIServingCompletion(
+        llm_client=llm_client,
+        server_model_name=args.served_model_name,
+        prompt_template=args.prompt_template
+    )
+
+    uvicorn.run(
+        router_app,
+        host=args.host,
+        port=args.port,
+        log_level=args.uvicorn_log_level,
+        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,
+        ssl_keyfile=args.ssl_keyfile,
+        ssl_certfile=args.ssl_certfile
+    )
```

主要修改内容如下:

1. 在`byzerllm_command.py`中添加了`serve`命令,将`api_server.py`需要的参数添加到`ServerArgs`类中,然后在`main()`函数中根据命令行参数创建`ServerArgs`对象,并调用`serve()`函数。

2. 将`api_server.py`中服务启动相关的代码抽取到了`serve.py`中的`serve()`函数。`serve()`函数接收`ByzerLLM`对象和`ServerArgs`对象作为参数。

3. 在`serve()`函数中,设置CORS中间件,添加认证中间件(如果提供了API Key),创建`OpenAIServingChat`和`OpenAIServingCompletion`对象,最后使用`uvicorn`启动服务。

以上就是我对代码的优化和抽象,现在可以通过`byzerllm serve`命令来启动OpenAI兼容的RESTful API服务了。如果还有其他需求或问题,欢迎继续交流。