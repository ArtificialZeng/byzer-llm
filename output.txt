
下面是一些文件路径以及每个文件对应的源码：

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/tokenizer.py
# After vLLM > 0.2.7 , vLLM brings lora support, 
# Then the tokenizer of the model.engine is TokenizerGroup,
# you can get the original tokenizer by tokenizer.tokenizer
# or get lora_toeknizer by get_lora_tokenizer

def validate_args_engine_use_ray():
    try:
        from vllm.transformers_utils.tokenizer import TokenizerGroup
        return True
    except ImportError:
        return False



def get_local_tokenizer(llm,engine_args):    
    from vllm.engine.arg_utils import AsyncEngineArgs
    engine_args: AsyncEngineArgs = engine_args    

    if engine_args.engine_use_ray:  
        from vllm.transformers_utils.tokenizer import TokenizerGroup
        engine_configs = engine_args.create_engine_configs()    
        model_config = engine_configs[0]
        scheduler_config = engine_configs[3]
        lora_config = engine_configs[5]      
        init_kwargs = dict(
                enable_lora=bool(lora_config),
                max_num_seqs=scheduler_config.max_num_seqs,
                max_input_length=None,
                tokenizer_mode=model_config.tokenizer_mode,
                trust_remote_code=model_config.trust_remote_code,
                revision=model_config.tokenizer_revision)
        tokenizer: TokenizerGroup = TokenizerGroup(model_config.tokenizer, **init_kwargs)
        return tokenizer
    else:
        return llm.engine.tokenizer

def get_real_tokenizer(tokenizer):        
    is_tokenizer_group = hasattr(tokenizer,"get_lora_tokenizer")
    if is_tokenizer_group:
        final_tokenizer = tokenizer.tokenizer
    else:
        final_tokenizer = tokenizer
    return final_tokenizer    



##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/types.py
from transformers import StoppingCriteria
class StopSequencesCriteria(StoppingCriteria):
    import torch
    """
     skip_check_min_length is used to skip the the stop sequence check if the input_ids is short
     than the min_length. 
    """
    def __init__(self, tokenizer,stops = [],input_start=0, skip_check_min_length=0):
    
      super().__init__()      
      self.stops = stops
      self.input_start = input_start
      self.skip_check_min_length = skip_check_min_length
      self.stop_words= [tokenizer.decode(item,skip_special_tokens=True) for item in stops]
      self.tokenizer = tokenizer   

    def to_str(self,s):
        return self.tokenizer.decode(s,skip_special_tokens=True)     

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):                   
      for index,stop in enumerate(self.stops):                        
        if  self.to_str(input_ids[0][-(len(stop)+10):]).endswith(self.stop_words[index]):
            return True
      return False

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/openai_utils.py
import logging
from typing import Any, Callable, Type, Union

import openai
from openai import (
    Completion,
    ChatCompletion,
    APITimeoutError,
    APIConnectionError,
    RateLimitError,
    APIError,
)

from tenacity import (
    before_sleep_log,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

logger = logging.getLogger(__name__)

CompletionClientType = Union[Type[Completion], Type[ChatCompletion]]


def _create_retry_decorator(max_retries: int) -> Callable[[Any], Any]:
    min_seconds = 4
    max_seconds = 10
    # Wait 2^x * 1 second between each retry starting with
    # 4 seconds, then up to 10 seconds, then 10 seconds afterwards
    return retry(
        reraise=True,
        stop=stop_after_attempt(max_retries),
        wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),
        retry=(
                retry_if_exception_type(APITimeoutError)
                | retry_if_exception_type(APIError)
                | retry_if_exception_type(APIConnectionError)
                | retry_if_exception_type(RateLimitError)
        ),
        before_sleep=before_sleep_log(logger, logging.WARNING),
    )


def completion_with_retry(is_chat_model: bool, max_retries: int, **kwargs: Any) -> Any:
    """Use tenacity to retry the completion call."""
    retry_decorator = _create_retry_decorator(max_retries=max_retries)

    @retry_decorator
    def _completion_with_retry(**kwargs: Any) -> Any:
        client = get_completion_endpoint(is_chat_model)
        return client.create(**kwargs)

    return _completion_with_retry(**kwargs)


async def async_completion_with_retry(is_chat_model: bool, max_retries: int, **kwargs: Any) -> Any:
    """Use tenacity to retry the async completion call."""
    retry_decorator = _create_retry_decorator(max_retries=max_retries)

    @retry_decorator
    async def _completion_with_retry(**kwargs: Any) -> Any:
        # Use OpenAI's async api https://github.com/openai/openai-python#async-api
        client = get_completion_endpoint(is_chat_model)
        return await client.acreate(**kwargs)

    return await _completion_with_retry(**kwargs)


def get_completion_endpoint(is_chat_model: bool) -> CompletionClientType:
    if is_chat_model:
        return openai.ChatCompletion
    else:
        return openai.Completion



##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/emb.py
from langchain.embeddings.base import Embeddings
from typing import List, Union
import torch
import torch.nn.functional as F
from transformers import pipeline

class ByzerSentenceTransformerEmbeddings(Embeddings):
    
    def __init__(self, model,tokenizer, device="auto"):         
        if model:
            self.model = model
        if tokenizer:
            self.model = tokenizer            
        
        if device == "auto":
            self.device = (
                torch.cuda.current_device() if torch.cuda.is_available() else "cpu"
            )
        else:
            self.device = device
        
        
    def _encode(self,texts: List[str],extract_params={}):        
        embeddings = [emb.tolist() for emb in self.model.encode(texts,**extract_params)]
        return embeddings
        
    def embed_documents(self, texts: List[str],extract_params={}) -> List[List[float]]:        
        embeddings = self._encode(texts,extract_params)
        return embeddings

    def embed_query(self, text: str,extract_params={}) -> List[float]:    
        embedding = self._encode([text],extract_params)
        return embedding[0]
        

class ByzerLLMEmbeddings(Embeddings):
    def __init__(self, model,tokenizer, device="auto", use_feature_extraction=False):         
        self.model = model
        self.tokenizer = tokenizer
        
        if device == "auto":
            self.device = (
                torch.cuda.current_device() if torch.cuda.is_available() else "cpu"
            )
        else:
            self.device = device

        self.pipeline = None
        if use_feature_extraction:
            self.pipeline = pipeline("feature-extraction", model = model, tokenizer = tokenizer,device=0)
        
    def _encode(self,texts: List[str],extract_params={}):
        if self.pipeline:
            return [self.pipeline(text)[0][-1] for text in texts]
        else:
            _, embeddings = self.get_embedding_with_token_count(texts)
            embeddings = embeddings.detach().cpu()
            embeddings = embeddings.numpy()
            embeddings = [emb.tolist() for emb in embeddings]
            return embeddings
        
    def embed_documents(self, texts: List[str],extract_params={}) -> List[List[float]]:        
        embeddings = self._encode(texts,extract_params)
        return embeddings

    def embed_query(self, text: str, extract_params={}) -> List[float]:    
        embedding = self._encode([text],extract_params)
        return embedding[0]

    # copied from https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2#usage-huggingface-transformers
    def get_embedding_with_token_count(self, sentences: Union[str, List[str]], ignore_keys:List[str]=["token_type_ids"]):
        # Mean Pooling - Take attention mask into account for correct averaging
        def mean_pooling(model_output, attention_mask):
            # First element of model_output contains all token embeddings
            token_embeddings = model_output[0]
            input_mask_expanded = (
                attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            )
            return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
                input_mask_expanded.sum(1), min=1e-9
            )

        # Tokenize sentences
        encoded_input = self.tokenizer(
            sentences, padding=True, truncation=True, return_tensors="pt"
        )
        inputs = encoded_input.to(self.device)
        token_count = inputs["attention_mask"].sum(dim=1).tolist()[0]
        # Compute token embeddings

        for ignore_key in ignore_keys:
            if hasattr(inputs, ignore_key):
                del inputs[ignore_key]

        model_output = self.model(**inputs)
        # Perform pooling
        sentence_embeddings = mean_pooling(model_output, inputs["attention_mask"])
        # Normalize embeddings
        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)

        return token_count, sentence_embeddings

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/text_generator.py
from typing import List,Tuple,Any,Dict
import json
from byzerllm.utils.tokenizer import get_real_tokenizer
from .emb import ByzerLLMEmbeddings,ByzerSentenceTransformerEmbeddings

class ByzerLLMGenerator:
    def __init__(self,model,tokenizer,use_feature_extraction=False) -> None:
        self.model = model        
        self.embedding = None
        self.tokenizer = None

        if hasattr(model,"embed_query") or hasattr(model,"embed_rerank"):
            self.embedding = model            

        if tokenizer:
            self.tokenizer = get_real_tokenizer(tokenizer)
            from sentence_transformers import SentenceTransformer
            if isinstance(model, SentenceTransformer) or isinstance(self.tokenizer, SentenceTransformer):
                self.embedding = ByzerSentenceTransformerEmbeddings(model,self.tokenizer)
            else:    
                self.embedding = ByzerLLMEmbeddings(model,self.tokenizer,use_feature_extraction=use_feature_extraction)
    
    def extract_history(self,input)-> List[Dict[str,str]]:
        history = input.get("history",[])
        return history
    
    def predict(self,query:Dict[str,Any]):
        ins = query["instruction"]
        
        if query.get("tokenizer",False):
            if not self.tokenizer:
                raise Exception("This model do not support text tokenizer service")
            return self.tokenizer(ins,return_token_type_ids=False,return_tensors="pt")["input_ids"].tolist()
        
        if query.get("apply_chat_template",False):
                if not self.tokenizer:
                    raise Exception("This model do not support tokenizer service")
                messages = json.loads(ins)
                return self.tokenizer.apply_chat_template(messages,tokenize=False, add_generation_prompt=False)
        
        if query.get("embedding",False):
            if not self.embedding:
                raise Exception("This model do not support text emedding service")
            new_params = {}
            for k,v in query.items():
                if k.startswith("gen."):
                    new_params[k[len("gen."):]] = v
                if k.startswith("generation."):
                    new_params[k[len("generation."):]] = v 
            
            if hasattr(self.embedding.model,"embed_query"):
                return self.embedding.model.embed_query(ins,extract_params=new_params)
            
            return self.embedding.embed_query(ins,extract_params=new_params)
        
        if query.get("meta",False):
            if hasattr(self.model,"get_meta"):
                return self.model.get_meta()
            return [{"model_deploy_type":"proprietary"}]

        if not self.model:
            raise Exception("This model do not support text generation service")

        his = self.extract_history(query)
        
        # notice that not all parameters in query are used in model stream_chat function
        # only the following parameters and the name starts with "gen." or "generation." are used
        # the prefix "gen." or "generation." will be removed when passing to model stream_chat function
        new_params = {}
        
        if "image" in query:
            new_params["image"] = query["image"] 
        
        for p in ["inference_mode","stopping_sequences","timeout_s","stopping_sequences_skip_check_min_length"]:
            if p in query:
                new_params[p] = query[p]

        for k,v in query.items():
            if k.startswith("gen."):
                new_params[k[len("gen."):]] = v
            if k.startswith("generation."):
                new_params[k[len("generation."):]] = v     

        response = self.model.stream_chat(self.tokenizer, 
        ins, his, 
        max_length=int(query.get("max_length",1024)), 
        top_p=float(query.get("top_p",0.7)),
        temperature=float(query.get("temperature",0.9)),**new_params)                
        
        return response[-1]  

    async def async_predict(self,query:Dict[str,Any]):
            ins = query["instruction"]

            if query.get("tokenizer",False):
                if not self.tokenizer:
                    raise Exception("This model do not support text tokenizer service")
                return self.tokenizer(ins,return_token_type_ids=False,return_tensors="pt")["input_ids"].tolist()

            if query.get("apply_chat_template",False):
                if not self.tokenizer:
                    raise Exception("This model do not support tokenizer service")
                messages = json.loads(ins)
                return self.tokenizer.apply_chat_template(messages,tokenize=False, add_generation_prompt=True)

            if query.get("embedding",False):
                if not self.embedding:
                    raise Exception("This model do not support text emedding service")
                new_params = {}
                for k,v in query.items():
                    if k.startswith("gen."):
                        new_params[k[len("gen."):]] = v
                    if k.startswith("generation."):
                        new_params[k[len("generation."):]] = v 

                if query.get("embed_rerank", False):
                    return self.embedding.embed_rerank(ins,extract_params=new_params)

                if hasattr(self.embedding.model,"embed_query"):
                    return self.embedding.model.embed_query(ins,extract_params=new_params)
                
                return self.embedding.embed_query(ins,extract_params=new_params)                        

            if query.get("meta",False):
                if hasattr(self.model,"async_get_meta"):
                    return await self.model.async_get_meta()
                elif hasattr(self.model,"get_meta"):
                    return self.model.get_meta()
                return [{"model_deploy_type":"proprietary"}]

            if not self.model:
                raise Exception("This model do not support text generation service")

            his = self.extract_history(query) 
            
            # notice that not all parameters in query are used in model stream_chat function
            # only the following parameters and the name starts with "gen." or "generation." are used
            # the prefix "gen." or "generation." will be removed when passing to model stream_chat function
            new_params = {}
            
            if "image" in query:
                new_params["image"] = query["image"] 
            
            for p in ["inference_mode","stopping_sequences","timeout_s","stopping_sequences_skip_check_min_length"]:
                if p in query:
                    new_params[p] = query[p]

            for k,v in query.items():
                if k.startswith("gen."):
                    new_params[k[len("gen."):]] = v
                if k.startswith("generation."):
                    new_params[k[len("generation."):]] = v     
            
            if hasattr(self.model, "async_stream_chat"):
                response = await self.model.async_stream_chat(self.tokenizer, 
                ins, his, 
                max_length=int(query.get("max_length",1024)), 
                top_p=float(query.get("top_p",0.7)),
                temperature=float(query.get("temperature",0.9)),**new_params)
            else:
                response = self.model.stream_chat(self.tokenizer, 
                ins, his, 
                max_length=int(query.get("max_length",1024)), 
                top_p=float(query.get("top_p",0.7)),
                temperature=float(query.get("temperature",0.9)),**new_params)                
            
            return response[-1]


async def simple_predict_func(model,v):
    (model,tokenizer) = model
    llm = ByzerLLMGenerator(model,tokenizer)
    data = [json.loads(item) for item in v]
    
    results=[]
    for item in data:        
        v = await llm.async_predict(item)
        
        if item.get("tokenizer",False) or item.get("embedding",False) or item.get("meta",False) or item.get("apply_chat_template",False):
            results.append({
            "predict":v,
            "metadata":{},
            "input":item})
        else:            
            metadata = {}
            if isinstance(v[1],dict) and "metadata" in v[1]:
                metadata = v[1]["metadata"] 

            results.append({
                "predict":v[0],
                "metadata":metadata,
                "input":item})

    return {"value":[json.dumps(results,ensure_ascii=False)]}


def chatglm_predict_func(model,v):
    (trainer,tokenizer) = model
    llm = ByzerLLMGenerator(trainer,tokenizer,use_feature_extraction=True)
    data = [json.loads(item) for item in v]
    
    results=[]
    for item in data:
        if "system" in item:
            item["instruction"] = f'{item["system"]}\n{item["instruction"]}'
        v = llm.predict(item)

        if item.get("tokenizer",False) or item.get("embedding",False) or item.get("meta",False) or item.get("apply_chat_template",False):
            results.append({
            "predict":v,
            "metadata":{},
            "input":item})
        else:            
            metadata = {}
            if isinstance(v[1],dict) and "metadata" in v[1]:
                metadata = v[1]["metadata"]            

            results.append({
                "predict":v[0],
                "metadata":metadata,
                "input":item})
        
    return {"value":[json.dumps(results,ensure_ascii=False)]}

def qa_predict_func(model,v):        
    data = [json.loads(item) for item in v]
    
    results=[]
    for item in data:
        if "system" in item:
            item["instruction"] = f'{item["system"]}\n{item["instruction"]}'
        v = model.predict(item)
        results.append({
            "predict":v,
            "input":item})
        
    return {"value":[json.dumps(results,ensure_ascii=False)]}

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/connect_ray.py
from typing import List,Optional
import os
import subprocess
import re

def _check_java_version(java_home:str):
    try:
        output = subprocess.check_output([f"{java_home}/bin/java", "-version"], stderr=subprocess.STDOUT, universal_newlines=True)
        version_line = output.splitlines()[0]
        version_match = re.search(r'version "(\d+)', version_line)
        if version_match:
            version = version_match.group(1)
            version_parts = version.split(".")
            major_version = int(version_parts[0])
            print(major_version)
            if major_version < 21:
                raise ValueError(f"Java version {version} is not supported. JDK 21 or higher is required.")
        else:
            raise ValueError("Could not determine Java version.")
    except (subprocess.CalledProcessError, ValueError) as e:
        raise ValueError(f"Error checking Java version: {str(e)}")



def connect_cluster(address:str="auto",java_home:Optional[str]=None,
                code_search_path:Optional[List[str]]=None):
    import ray
    java_home=java_home if java_home else os.environ.get("JAVA_HOME")
    path = os.environ.get("PATH")    
    env_vars = {"JAVA_HOME": java_home,
                "PATH":f'''{os.path.join(java_home,"bin")}:{path}'''}
    
    

    job_config = None
    if code_search_path:
        if java_home:
            _check_java_version(java_home)
        job_config = ray.job_config.JobConfig(code_search_path=code_search_path,
                                                        runtime_env={"env_vars": env_vars})
        
    ray.init(address=address,namespace="default",ignore_reinit_error=True,
                    job_config=job_config) 

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/langutil.py
import anyio
from contextlib import contextmanager, closing
from contextlib2 import nullcontext
from functools import wraps, partial
import inspect
import os
import socket
from typing import Any, Optional
import re
from urllib.parse import urlparse
import warnings
from rich.console import Console

console = Console(highlight=False)


@contextmanager
def switch_cwd(path):
    old_cwd = os.getcwd()
    try:
        os.chdir(path)
        yield
    finally:
        os.chdir(old_cwd)

@contextmanager
def patch(obj, attr, val):
    old_val = getattr(obj, attr)
    try:
        setattr(obj, attr, val)
        yield
    finally:
        setattr(obj, attr, old_val)


def asyncfy(func):
    """Decorator that makes a function async. Note that this does not actually make
    the function asynchroniously running in a separate thread, it just wraps it in
    an async function. If you want to actually run the function in a separate thread,
    consider using asyncfy_with_semaphore.

    Args:
        func (function): Function to make async
    """

    if inspect.iscoroutinefunction(func):
        return func

    @wraps(func)
    async def async_func(*args, **kwargs):
        return func(*args, **kwargs)

    return async_func


def asyncfy_with_semaphore(
    func, semaphore: Optional[anyio.Semaphore], timeout: Optional[float] = None
):
    """Decorator that makes a function async, as well as running in a separate thread,
    with the concurrency controlled by the semaphore. If Semaphore is None, we do not
    enforce an upper bound on the number of concurrent calls (but it is still bound by
    the number of threads that anyio defines as an upper bound).

    Args:
        func (function): Function to make async. If the function is already async,
            this function will add semaphore and timeout control to it.
        semaphore (anyio.Semaphore or None): Semaphore to use for concurrency control.
            Concurrent calls to this function will be bounded by the semaphore.
        timeout (float or None): Timeout in seconds. If the function does not return
            within the timeout, a TimeoutError will be raised. If None, no timeout
            will be enforced. If the function is async, one can catch the CancelledError
            inside the function to handle the timeout.
    """
    if inspect.iscoroutinefunction(func):

        @wraps(func)
        async def async_func(*args, **kwargs):
            semaphore_ctx = semaphore if semaphore is not None else nullcontext()
            timeout_ctx = anyio.fail_after(timeout) if timeout else nullcontext()
            with timeout_ctx:
                async with semaphore_ctx:
                    return await func(*args, **kwargs)

        return async_func

    else:

        @wraps(func)
        async def async_func(*args, **kwargs):
            semaphore_ctx = semaphore if semaphore is not None else nullcontext()
            timeout_ctx = anyio.fail_after(timeout) if timeout else nullcontext()
            with timeout_ctx:
                async with semaphore_ctx:
                    return await anyio.to_thread.run_sync(
                        partial(func, *args, **kwargs), cancellable=True
                    )

        return async_func


def is_valid_url(candidate_str: Any) -> bool:
    if not isinstance(candidate_str, str):
        return False
    parsed = urlparse(candidate_str)
    return parsed.scheme != "" and parsed.netloc != ""


# backward compatible function name
def _is_valid_url(candidate_str: Any) -> bool:
    warnings.warn("_is_valid_url is deprecated. Please use is_valid_url instead.")
    return is_valid_url(candidate_str)


def _is_local_url(candidate_str: str) -> bool:
    parsed = urlparse(candidate_str)
    local_hosts = ["localhost", "127.0.0.1", "0.0.0.0", "::1"]
    return parsed.hostname in local_hosts


def find_available_port(port=None):
    if port is None:
        with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
            s.bind(("", 0))
            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            return s.getsockname()[1]

    def is_port_occupied(port):
        """
        Returns True if the port is occupied, False otherwise.
        """
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            return s.connect_ex(("localhost", port)) == 0

    while is_port_occupied(port):
        console.print(
            f"Port [yellow]{port}[/] already in use. Incrementing port number to"
            " find an available one."
        )
        port += 1
    return port

def to_bool(s: str) -> bool:
    """
    Convert a string to a boolean value.
    """
    if not isinstance(s, str):
        raise TypeError(f"Expected a string, got {type(s)}")
    true_values = ("yes", "true", "t", "1", "y", "on", "aye", "yea")
    false_values = ("no", "false", "f", "0", "n", "off", "nay", "")
    s = s.lower()
    if s in true_values:
        return True
    elif s in false_values:
        return False
    else:
        raise ValueError(
            f"Invalid boolean value: {s}. Valid true values: {true_values}. Valid false"
            f" values: {false_values}."
        )



##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/ray_utils.py
from ray.util.placement_group import (    
    remove_placement_group,
    PlacementGroup
)
from ray._private.utils import hex_to_binary
from ray._raylet import PlacementGroupID
from ray.util.state import (
        StateApiClient,
        get_log,
        list_logs,
        summarize_actors,
        summarize_objects,
        summarize_tasks,
    )
from ray.util.state.common import (
    DEFAULT_LIMIT,
    DEFAULT_LOG_LIMIT,
    DEFAULT_RPC_TIMEOUT,
    GetApiOptions,
    ListApiOptions,
    PredicateType,
    StateResource,
    StateSchema,
    SupportedFilterType,
    resource_to_schema,
)
from ray.util.state.exception import RayStateApiException
from ray.util.annotations import PublicAPI

def cancel_placement_group(group_id:str):
    remove_placement_group(PlacementGroup(
                PlacementGroupID(hex_to_binary(group_id))
            ))

def get_actor_info(actor):            
    resource = StateResource("actors".replace("-", "_"))
    # Create the State API server and put it into context
    client = StateApiClient(address="auto")
    options = GetApiOptions(timeout=30)
    # If errors occur, exceptions will be thrown. Empty data indicate successful query.
    try:
        state = client.get(
            resource,
            options=options,
            id=actor._ray_actor_id.hex(),
            _explain=True,
        )
        return state
    except RayStateApiException as e:
        raise e
    

    
  
    

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/object_store_ref_util.py
import ray
from ray._private.client_mode_hook import client_mode_wrap

@client_mode_wrap
def get_locations(blocks):
    core_worker = ray.worker.global_worker.core_worker
    return [
        core_worker.get_owner_address(block)
        for block in blocks
    ]

def get_object_ids(blocks):
    object_ids = [block.binary() for block in blocks]
    return object_ids

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/__init__.py
from pathlib import Path
from functools import wraps
import time
import json
import hashlib
import threading
from typing import TYPE_CHECKING,TypeVar,Dict, List, Optional, Union,Any,Tuple,get_type_hints,Annotated,get_args,Callable
import typing
from ray.util.client.common import ClientActorHandle, ClientObjectRef
import inspect
import pydantic
import sys
import traceback
import io

T = TypeVar("T")

def print_flush(*args, **kwargs):
    print(*args, **kwargs, flush=True)

import signal
from contextlib import contextmanager
class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException()

@contextmanager
def timeout(duration: float):
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(duration)
    try:
        yield
    finally:
        signal.alarm(0)

def timeit(func):
    """
    Decorator to time a function.
    """

    @wraps(func)
    def inner(*args, **kwargs):
        start_time = time.monotonic()
        ret = func(*args, **kwargs)
        time_taken = time.monotonic() - start_time
        print(f"{func} took {time_taken} s to complete",flush=True)
        return ret

    return inner

def generate_instruction_from_history(ins:str,his:List[Dict[str,str]],role_mapping:Dict[str,str]={        
        "user":"User",        
        "assistant":"Assistant",
    }):

    new_his = []    
    for item in his:
        if item["role"] == "system":
            new_his.append(item["content"])
            continue        
        new_his.append(f"{role_mapping[item['role']]}:{item['content']}")            

    # here we should make sure the user build the conversation string manually also
    # works. This means if the user do not provide  the history, then
    # we should treat ins as conversation string which the user build manually
    if len(new_his) > 0 and ins != "":
        new_his.append(f"{role_mapping['user']}:{ins}")
        new_his.append(f"{role_mapping['assistant']}:")

    if len(new_his) > 0 and ins == "":
        new_his.append(f"{role_mapping['assistant']}:")            
    
    if len(new_his) == 0:
        new_his.append(ins)    

    fin_ins = "\n".join(new_his)
    return fin_ins  

def compute_max_new_tokens(tokens,max_length:int):
    input_length = tokens["input_ids"].shape[1]
    max_new_tokens = max_length - input_length
    if max_new_tokens <= 0:
        raise Exception(f"Input is too long ({input_length}). Try to reduce the length of history or use a larger `max_length` value (now:{max_length})")
    return max_new_tokens

def tokenize_string(tokenizer, key: str) -> Union[int, List[int]]:
    """Tokenize a string using a tokenizer.

    Args:
        tokenizer (PreTrainedTokenizer): Tokenizer to use.
        key (str): String to tokenize.
    """
    token_ids = tokenizer.encode(key, add_special_tokens=False)
    return token_ids

def tokenize_stopping_sequences_where_needed(
    tokenizer,
    stopping_sequences: List[Union[str, int, List[int]]],
) -> List[Union[List[int], int]]:
    """If any sequence is a string, tokenize it.

    Args:
        tokenizer (PreTrainedTokenizer): Tokenizer to use.
        stopping_sequences (List[Union[str, int, List[int]]]): Stopping sequences to
            tokenize. Can be ids, sequences of ids or strings.
    """
    if not stopping_sequences:
        return None
    return [
        tokenize_string(tokenizer, sequence) if isinstance(sequence, str) else sequence
        for sequence in stopping_sequences
    ]

def  tokenize_stopping_sequences(tokenizer,stop_words):
    stop_words_ids = []
    for stop_word in stop_words:
        w = tokenize_string(tokenizer, stop_word)
        # remove the first token which is empty token 
        # this should work for only llama model
        # if w[0] == 29871 and tokenizer.decode([w[0]],skip_special_tokens=False) == "":
        #     w = w[1:]
        stop_words_ids.append(w)    
    return stop_words_ids



def load_json_str(json_str:str):        
    return json.loads(json_str) 


def generate_file_md5(file_path: str) -> str:
    md5_hash = hashlib.md5()
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b''):
            md5_hash.update(chunk)
    return md5_hash.hexdigest()

def generate_str_md5(s: str) -> str:
    md5_hash = hashlib.md5()
    md5_hash.update(s.encode("utf-8"))
    return md5_hash.hexdigest() 

class SingleOutputMeta:
    def __init__(self, input_tokens_count:int=0, generated_tokens_count:int=0):        
        self.input_tokens_count = input_tokens_count
        self.generated_tokens_count = generated_tokens_count    

class SingleOutput:
    def __init__(self, text:str,metadata:SingleOutputMeta=SingleOutputMeta()):
        self.text = text
        self.metadata = metadata
        
class StreamOutputs: 
    def __init__(self, outputs:List[SingleOutput]):
        self.outputs = outputs        

class BlockVLLMStreamServer:
    def __init__(self):
        self.cache = {}
        self.cache_status = {} 
        self.lock = threading.Lock()

    def add_item(self, request_id, item):
        with self.lock:            
            self.cache[request_id]=item
            self.cache_status[request_id]=int(time.time()*1000)
    
    def mark_done(self, request_id):
        if len(self.cache_status) > 30:
            now = int(time.time()*1000)
            with self.lock:
                for k in list(self.cache_status.keys()):
                    if now - self.cache_status[k] > 10*60*60*1000:
                        del self.cache_status[k]
                        del self.cache[k] 
        with self.lock:            
            self.cache_status[request_id] = 0

    def get_item(self, request_id):                
        with self.lock:
            v = self.cache.get(request_id, None)     
            if request_id in self.cache_status and self.cache_status[request_id] == 0:
                del self.cache[request_id]
                del self.cache_status[request_id]
            return v     

class VLLMStreamServer:
    def __init__(self):
        self.cache = {}
        self.cache_status = {} 
        self.lock = threading.Lock()

    async def add_item(self, request_id, item):
        with self.lock:            
            self.cache[request_id]=item
            self.cache_status[request_id]=int(time.time()*1000)
    
    async def mark_done(self, request_id):
        if len(self.cache_status) > 30:
            now = int(time.time()*1000)
            with self.lock:
                for k in list(self.cache_status.keys()):
                    if now - self.cache_status[k] > 10*60*60*1000:
                        del self.cache_status[k]
                        del self.cache[k] 
        with self.lock:            
            self.cache_status[request_id] = 0

    async def get_item(self, request_id):                
        with self.lock:
            v = self.cache.get(request_id, None)     
            if request_id in self.cache_status and self.cache_status[request_id] == 0:
                del self.cache[request_id]
                del self.cache_status[request_id]
            return v
        
def get_type_name(t):
    name = str(t)
    if "list" in name or "dict" in name:
        return name
    else:
        return t.__name__
    
def is_annotated_type(hint):
    if hasattr(typing, '_AnnotatedAlias'):  # Python 3.9 and later
        return isinstance(hint, typing._AnnotatedAlias)
    elif hasattr(typing, '_SpecialForm'):  # Python versions before 3.9
        # Check if it's a _SpecialForm and its name is 'Annotated'
        return isinstance(hint, typing._SpecialForm) and hint.__name__ == 'Annotated'
    else:
        return False    
    
def serialize_function_to_json(func):
    if isinstance(func, str):
        return func
    
    signature = inspect.signature(func)
    type_hints = get_type_hints(func)

    # return_type = type_hints.get('return', 'void')
    # if return_type is None:
    #     return_type_str = 'void'
    # else:
    #     return_type_str = return_type.__name__

    function_info = {
        "name": func.__name__,
        "description": func.__doc__,
        "parameters": {
            "type": "object",
            "properties": {}
        },
        # "returns": return_type_str
    }

    for name, parameter in signature.parameters.items():
        param_type = get_type_name(type_hints.get(name, type(None)))
        param_annotated= func.__annotations__.get(name, '')

        function_info["parameters"]["properties"][name]  = {}
        properties = function_info["parameters"]["properties"][name] 

        
        if is_annotated_type(param_annotated):
            _, *metadata = get_args(param_annotated)
        else:
            metadata = []  
   
        param_desc = ""
        for meta in metadata:
            if isinstance(meta, str):
                param_desc = meta 
            if isinstance(meta, Dict):
                param_desc = meta.get("description", "")
                if "enum" in meta:
                    properties["enum"] = meta["enum"]

        properties["type"] = param_type
        properties["description"] = param_desc
        
        if parameter.default is not inspect.Parameter.empty:
            properties["default"] = parameter.default                            

    return json.dumps(function_info,ensure_ascii=False, indent=2)


class FunctionCall(pydantic.BaseModel):
    '''
    函数名称和函数参数列表
    '''        
    name: str = pydantic.Field(description="函数名")
    arguments: Dict[str,Any] = pydantic.Field(description="函数参数")

class FunctionCallWrapper(pydantic.BaseModel):    
    function: FunctionCall = pydantic.Field(description="函数调用")

class FunctionCallList(pydantic.BaseModel):
    '''
    函数调用列表    
    '''
    tool_calls: List[FunctionCallWrapper] = pydantic.Field(description="函数调用列表")
    id: str = pydantic.Field(description="工具调用的唯一标识符,无需生成")
    type: str = pydantic.Field("function",description="工具调用的类型，固定为 function，无需生成")

FUNCTION_CALLING_SCHEMA = FunctionCallList.schema_json(ensure_ascii=False, indent=2) 


def exec_capture_output(code: str,target_names:Dict[str,Any]={}) -> Tuple[int,str,Any]:
    buffer = io.StringIO()
    sys.stdout = buffer
    sys.stderr = buffer

    try:
        variables = {}
        exec(code,variables)
        response = {}
        for name,v in target_names.items():
            if name in variables:
                response[name] = variables[name]
    except Exception:
        return 1,traceback.format_exc(),{}

    sys.stdout = sys.__stdout__
    sys.stderr = sys.__stderr__

    return 0,buffer.getvalue(),response

def function_impl_format(prompt:str,func:Optional[Union[Callable,str]],
                             cls:Union[pydantic.BaseModel,str])->str:
    
    tool_choice_ser = serialize_function_to_json(func)    
    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)

    example = '''{
  "name": "caculate_current_time",
  "description": "\n    计算当前时间\n    ",
  "parameters": {
    "type": "object",
    "properties": {}
  }
}'''    
    example_output_format='''
{"title": "CurrentTime", "description": "当前时间    ", "type": "object", "properties": {"time": {"title": "Time", "description": "开始时间.时间格式为 yyyy-MM-dd", "type": "string"}}, "required": ["time"]}
'''
    example_output = '''
from datetime import datetime

def caculate_current_time():
    # 获取当前日期和时间
    now = datetime.now()
    
    # 将日期和时间格式化为"yyyy-MM-dd"的形式
    time_str = now.strftime("%Y-%m-%d")
    
    return {"time": time_str}
'''
    
    msg = f''''你非常擅长 Python 语言。根据用户提供的一些信息以及问题，对提供了没有实现空函数函数进行实现。

示例：
你需要实现的函数的签名如下：

```json
{example}
```

生成的函数的返回值必须是 Json 格式，并且满足如下 OpenAPI 3.1. 规范：

```json
{example_output_format}
```

最后，你生成的函数的代码如下：

```python
{example_output}
```

现在，你需要实现函数的签名如下：

```json
{tool_choice_ser}
```

同时，你生成的函数的返回值必须是 Json 格式，并且满足如下 OpenAPI 3.1. 规范：

```json
{_cls}
```

用户的问题是：{prompt}

在满足上述提及的约束的情况下，请你实现这个函数。
注意：
1. 任何情况下都不要拆分成多段代码输出，请一次性生成完整的代码片段，确保代码的完整性
2. 回复的内容只有一个代码块，且代码块的语言为 Python
3. 不要演示如何调用你生成的函数的代码
'''
    return msg   



def function_calling_format(prompt:str,tools:List[Union[Callable,str]],tool_choice:Optional[Union[Callable,str]])->str:
    tool_serializes = []
    for v in tools:
        tool_serializes.append(serialize_function_to_json(v))

    force_prompt = ""
    if tool_choice is not None:
        tool_choice_ser = serialize_function_to_json(tool_choice)
        force_prompt = f''''
你必须使用如下的工具来解决用户的问题：        
```json
{tool_choice_ser}
```
'''  
   
    if tool_choice is None and len(tools) == 0:
        return prompt                   

    tools_str = "\n".join(tool_serializes)
    
    function_example = '''
{
  "name": "compute_date_range",
  "description": "\n    计算日期范围\n    ",
  "parameters": {
    "type": "object",
    "properties": {
      "count": {
        "type": "int",
        "description": "时间跨度，数值类型,如果用户说的是几天，几月啥的，比较模糊，务必使用默认值",
        "default": 3
      },
      "unit": {
        "enum": [
          "day",
          "week",
          "month",
          "year"
        ],
        "type": "str",
        "description": "",
        "default": "day"
      }
    }
  }
}
'''
    output_example = '''
{
  "id": "unique_id_1",
  "type": "function",
  "tool_calls": [
    {
      "function": {
        "name": "compute_date_range",
        "arguments": {
          "count": 3,
          "unit": "day"
        }
      }
    }
  ]
}
'''

    msg = f'''You are a helpful assistant with access to the following functions:

```json
{tools_str}
```

当用户的问题可以使用上面的一个或者多个函数解决时,你需要通过符合 OpenAPI 3.1 规范的 Json 格式告诉我你需要调用哪些函数。

下面Json文本描述了你需要返回的格式,它符合 OpenAPI 3.1 规范:

```json
{FUNCTION_CALLING_SCHEMA}
```

示例：

当你选择下面的函数时：

```
{function_example}
```

你应该使用如下的 Json 格式告诉我你需要调用这个函数：

```json
{output_example}
```

{force_prompt}

现在用户的问题是：{prompt}

请选择合适的一个或者多个函数按要求的 Json 格式返回给我。

注意：
1. 如果你无法使用上述函数解决用户的问题，请如实告诉我你没有办法回答。
''' 
    return msg  


def response_class_format(prompt:str,cls:Union[pydantic.BaseModel,str])->str:

    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)    
        
    example='''
{"title": "Item", "description": "时间抽取的返回结果", "type": "object", "properties": {"time": {"title": "Time", "description": "时间信息,比如内容里会提到天， 月份，年等相关词汇", "type": "string"}, "other": {"title": "Other", "description": "除了时间以外的其他部分", "type": "string"}}, "required": ["time", "other"]}
'''
    example_output = '''{
  "time": "最近三个月",
  "other": "奔驰的销量趋势如何"
}'''
    msg = f'''当你回答用户问题的时候，你需要使用 Json 格式进行回复。

示例：

当你被要求按如下格式输出时,它符合 OpenAPI 3.1 规范：

```json
{example}
```
你的输出应该是这样的：

```json
{example_output}
```

现在用户的问题是：{prompt}

下面Json文本描述了你需要返回的格式,它符合 OpenAPI 3.1 规范:

```json
{_cls}
```

请根据自己生成的内容并以 Json 格式回复我。
''' 
    return msg

def response_class_format_after_chat(cls:Union[pydantic.BaseModel,str])->str:
 
    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)
    example='''
{"title": "Item", "description": "时间抽取的返回结果", "type": "object", "properties": {"time": {"title": "Time", "description": "时间信息,比如内容里会提到天， 月份，年等相关词汇", "type": "string"}, "other": {"title": "Other", "description": "除了时间以外的其他部分", "type": "string"}}, "required": ["time", "other"]}
'''
    example_output = '''{
  "car": {
    "name": "奔驰"
  },
  "metric": {
    "name": "销量趋势"
  }'''    
    msg = f'''你需要以 Json 格式重新组织内容回复我。

示例：

当你被要求按如下格式输出时,它符合 OpenAPI 3.1 规范：

```json
{example}
```
你的输出应该是这样的：

```json
{example_output}
```
把你刚才回答我的内容重新做组织，以 Json 格式回复我

下面Json文本描述了你需要返回的格式,它符合 OpenAPI 3.1 规范:

```json
{_cls}
```
''' 
    return msg 


def base_ability_format(prompt:Optional[str]=None)->str:
    RESPONSE_WITH_CLASS_example_0='''{"title": "Item", "description": "时间抽取的返回结果", "type": "object", "properties": {"time": {"title": "Time", "description": "时间信息,比如内容里会提到天， 月份，年等相关词汇", "type": "string"}, "other": {"title": "Other", "description": "除了时间以外的其他部分", "type": "string"}}, "required": ["time", "other"]}'''
    RESPONSE_WITH_CLASS_example_output_0 = '''{
    "time": "最近三个月",
    "other": "奔驰的销量趋势如何"
    }'''

    RESPONSE_WITH_CLASS_example='''{"title": "Info", "type": "object", "properties": {"car": {"title": "Car", "description": "车的信息", "allOf": [{"$ref": "#/definitions/Car"}]}, "metric": {"title": "Metric", "description": "计算的指标信息", "allOf": [{"$ref": "#/definitions/Metric"}]}}, "required": ["car", "metric"], "definitions": {"Car": {"title": "Car", "type": "object", "properties": {"name": {"title": "Name", "description": "品牌名称", "type": "string"}}, "required": ["name"]}, "Metric": {"title": "Metric", "type": "object", "properties": {"name": {"title": "Name", "description": "指标名称", "type": "string"}}, "required": ["name"]}}}'''
    RESPONSE_WITH_CLASS_example_output = '''{
  "car": {
    "name": "奔驰"
  },
  "metric": {
    "name": "销量趋势"
  }
}'''
    RESPONSE_WITH_CLASS_example='''{"title": "Info", "type": "object", "properties": {"car": {"title": "Car", "description": "车的信息", "allOf": [{"$ref": "#/definitions/Car"}]}, "metric": {"title": "Metric", "description": "计算的指标信息", "allOf": [{"$ref": "#/definitions/Metric"}]}}, "required": ["car", "metric"], "definitions": {"Car": {"title": "Car", "type": "object", "properties": {"name": {"title": "Name", "description": "品牌名称", "type": "string"}}, "required": ["name"]}, "Metric": {"title": "Metric", "type": "object", "properties": {"name": {"title": "Name", "description": "指标名称", "type": "string"}}, "required": ["name"]}}}'''
    RESPONSE_WITH_CLASS_example_output = '''{
  "car": {
    "name": "奔驰"
  },
  "metric": {
    "name": "销量趋势"
  }
}'''

    FUNCTION_CALLING_example = '''
{
  "name": "compute_date_range",
  "description": "\n    计算日期范围\n    ",
  "parameters": {
    "type": "object",
    "properties": {
      "count": {
        "type": "int",
        "description": "时间跨度，数值类型,如果用户说的是几天，几月啥的，比较模糊，务必使用默认值",
        "default": 3
      },
      "unit": {
        "enum": [
          "day",
          "week",
          "month",
          "year"
        ],
        "type": "str",
        "description": "",
        "default": "day"
      }
    }
  }
}
'''
    FUNCTION_CALLING_example_output = '''
{
  "id": "unique_id_1",
  "type": "function",
  "tool_calls": [
    {
      "function": {
        "name": "compute_date_range",
        "arguments": {
          "count": 3,
          "unit": "day"
        }
      }
    }
  ]
}
'''

    FUNCTION_IMPL_example = '''{
  "name": "caculate_current_time",
  "description": "\n    计算当前时间\n    ",
  "parameters": {
    "type": "object",
    "properties": {}
  }
}'''    
    FUNCTION_IMPL_example_output_schema='''
{"title": "CurrentTime", "description": "当前时间    ", "type": "object", "properties": {"time": {"title": "Time", "description": "开始时间.时间格式为 yyyy-MM-dd", "type": "string"}}, "required": ["time"]}
'''
    FUNCTION_IMPL_example_output = '''
from datetime import datetime

def caculate_current_time():
    # 获取当前日期和时间
    now = datetime.now()
    
    # 将日期和时间格式化为"yyyy-MM-dd"的形式
    time_str = now.strftime("%Y-%m-%d")
    
    return {"time": time_str}
'''
    msg = f'''下面是你具备的基础能力，当你回答用户问题的时候，随时回顾这些能力。

JSON 格式是一种轻量级的数据交换格式，JSON Schema 是基于 JSON 的一个描述 JSON 数据结构的元数据，可以用来描述 JSON 数据的结构和内容，以及定义 JSON 数据的合法值范围。
OpenAPI Specification (OAS) 使用 JSON Schema 来描述 Json 数据的结构和内容，你需要遵循 OpenAPI 3.1.0 版本的规范。

===================RESPONSE_WITH_CLASS===================

下面是一个根据用户的问题，并且结合 JSON Schema 生成对应的 JSON 数据的例子：

输入：

最近三个月奔驰的销量趋势如何？

JSON Schema：

```json
{RESPONSE_WITH_CLASS_example_0}
```

输出：

```json
{RESPONSE_WITH_CLASS_example_output_0}
```

下面生成的 Json 数据有有嵌套结构的例子：

输入：

最近三个月奔驰的销量趋势如何？

JSON Schema：

```json
{RESPONSE_WITH_CLASS_example}
```

输出：

```json
{RESPONSE_WITH_CLASS_example_output}
```

当用户提到 RESPONSE_WITH_CLASS 时，请回顾该能力。

===================FUNCTION_CALLING===================

用户会提供一个函数列表给你,你需要根据用户的问题，选择一个或者多个函数返回给用户。如果你无法使用上述函数解决用户的问题，请如实告诉我你没有办法回答。
下面假设你已经选择了一个函数作为输入，并且结合 JSON Schema 生成对应的 JSON 数据的例子：

输入：

```json
{FUNCTION_CALLING_example}
```

JSON Schema：

```json
{FUNCTION_CALLING_SCHEMA}
```

输出：

```json
{FUNCTION_CALLING_example_output}
```

当用户提到 FUNCTION_CALLING 时，请回顾该能力。

===================FUNCTION_IMPL===================

你非常擅长 Python 语言。根据用户提供的一些信息以及问题，对用户提供的没有实现空函数函数进行实现。
下面假设用户提供了一个需要实现的函数的签名，你需要结合用户的问题，函数的签名，以及函数文档，生成对应的 Python 代码，函数的返回值
必须是 Json 格式，并且需要符合对应的 JSON Schema 规范。

下面提供了一个示例：

输入：

```json
{FUNCTION_IMPL_example}
```

JSON Schema：

```json
{FUNCTION_IMPL_example_output_schema}
```

输出：

```python
{FUNCTION_IMPL_example_output}
```

注意：
1. 任何情况下都不要拆分成多段代码输出，请一次性生成完整的代码片段，确保代码的完整性
2. 回复的内容只有一个代码块，且代码块的语言为 Python
3. 不要展示如何调用你生成的函数的代码
4. 不要展示你函数执行的结果

当用户提到 FUNCTION_IMPL 时，请回顾该能力。

===================OTHERS===================
'''
    
    return msg


def sys_response_class_format(prompt:str,cls:Union[pydantic.BaseModel,str])->str:
    
    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)

    msg = f'''
请使用 RESPONSE_WITH_CLASS 相关的能力，解决用户的问题。

输入：

{prompt}

JSON Schema：

```json
{_cls}
```

输出：
'''
    return msg

def sys_function_calling_format(prompt:str,tools:List[Union[Callable,str]],tool_choice:Optional[Union[Callable,str]])->str:
    tool_serializes = []
    for v in tools:
        tool_serializes.append(serialize_function_to_json(v))

    force_prompt = ""
    if tool_choice is not None:
        tool_choice_ser = serialize_function_to_json(tool_choice)
        force_prompt = f''''
你必须使用如下的工具来解决用户的问题：        
```json
{tool_choice_ser}
```
'''  
   
    if tool_choice is None and len(tools) == 0:
        return prompt                   

    tools_str = "\n".join(tool_serializes)
        
    msg = f'''
请使用 FUNCTION_CALLING 相关的能力，解决用户的问题。

你有如下的函数可以使用：

```json
{tools_str}
```
{force_prompt}

输入：

{prompt}

JSON Schema：

```json
{FUNCTION_CALLING_SCHEMA}
```

输出:
''' 
    return msg 

def sys_function_impl_format(prompt:str,func:Optional[Union[Callable,str]],
                             cls:Union[pydantic.BaseModel,str])->str:
    
    tool_choice_ser = serialize_function_to_json(func)    
    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)

    
    msg = f''''请使用 FUNCTION_IMPL 相关的能力，解决用户的问题。
根据用户提供的一些信息以及函数签名，对函数进行实现。

用户问题： {prompt}

输入：

```json
{tool_choice_ser}
```

JSON Schema：

```json
{_cls}
```

输出:
'''
    return msg  

def format_prompt(func,**kargs): 
    from langchain import PromptTemplate
    doc = func.__doc__       
    lines = doc.splitlines()
    # get the first line to get the whitespace prefix
    first_non_empty_line = next(line for line in lines if line.strip())
    prefix_whitespace_length = len(first_non_empty_line) - len(first_non_empty_line.lstrip())    
    prompt = "\n".join([line[prefix_whitespace_length:] for line in lines])
    tpl = PromptTemplate.from_template(prompt)
    return tpl.format(**kargs)

def format_prompt_jinja2(func,**kargs):
    from jinja2 import Template
    doc = func.__doc__       
    lines = doc.splitlines()
    # get the first line to get the whitespace prefix
    first_non_empty_line = next(line for line in lines if line.strip())
    prefix_whitespace_length = len(first_non_empty_line) - len(first_non_empty_line.lstrip())    
    prompt = "\n".join([line[prefix_whitespace_length:] for line in lines])
    tpl = Template(prompt)
    return tpl.render(kargs)
  





##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/retrieval/rest.py
from fastapi import Body, FastAPI
import ray
from ray import serve
from pydantic import BaseModel,Field
from typing import List,Dict,Any,Annotated,Optional
import json
from byzerllm.records import (ClusterSettings, 
                                      EnvSettings, 
                                      JVMSettings, 
                                      TableSettings,
                                      SearchQuery,
                                      ResourceRequirement,
                                      ResourceRequirementSettings)
from byzerllm.utils.retrieval import ByzerRetrieval

class ClusterSettingsParam(BaseModel):
    name:str
    location:str
    numNodes:int
    
    def cluster_settings(self):
        return ClusterSettings(**self.dict())

class EnvSettingsParam(BaseModel):
    javaHome:str
    path:str

    def env_settings(self):
        return EnvSettings(**self.dict())

class JVMSettingsParam(BaseModel):
    options:list[str]

    def jvm_settings(self):
        return JVMSettings(**self.dict())

class ResourceRequirementParam(BaseModel):
    name:str
    resourceQuantity:float

    def resource_requirement(self):
        return ResourceRequirement(**self.dict())

class ResourceRequirementSettingsParam(BaseModel):
    resourceRequirements: List[ResourceRequirementParam]

    def resource_requirement_settings(self):
        return ResourceRequirementSettings([item.resource_requirement() for item in self.resourceRequirements])

class TableSettingsParam(BaseModel):
    database:str
    table:str
    my_schema: str = Field(alias='schema')    
    location:str
    num_shards:int

    def table_settings(self):
        v = self.dict()        
        return TableSettings(**v)
    
    def dict(self):
        t = self.__dict__
        t["schema"]=t["my_schema"]
        del t["my_schema"]
        return t

class SearchQueryParam(BaseModel):
    keyword:Optional[str]
    fields:list[str]
    vector:list[float]
    vectorField:Optional[str]
    limit:int  

    def search_query(self):
        return SearchQuery(**self.dict())  
    
    def json(self):
        return json.dumps(self.dict(),ensure_ascii=False)
     
app = FastAPI()

@serve.deployment()
@serve.ingress(app)
class SimpleRest:
    
    def __init__(self):                        
        self.retrieval = ByzerRetrieval() 
        self.retrieval.launch_gateway()    
        

    @app.post("/cluster/create")
    def cluster(self,   cluster_settings:ClusterSettingsParam,                       
                        env_settings:EnvSettingsParam, 
                        jvm_settings:JVMSettingsParam,
                        resource_requirement_settings:ResourceRequirementSettingsParam):        
        return {
            "status":self.retrieval.start_cluster(cluster_settings.cluster_settings(),
                                            env_settings.env_settings(),
                                            jvm_settings.jvm_settings(),
                                            resource_requirement_settings.resource_requirement_settings())
        }
    
    @app.get("/cluster/get/{name}")                                        
    def cluster_info(self,name:str):
        return self.retrieval.cluster_info(name)
    
    @app.post("/cluster/restore")                                        
    def restore_from_cluster_info(self,cluster_info:str) :
        return {
            "status":self.retrieval.restore_from_cluster_info(json.loads(cluster_info))
        }
    
    @app.post("/table/create/{cluster_name}")                                        
    def create_table(self,cluster_name:str,table_settings:TableSettingsParam):        
        return {
            "status":self.retrieval.create_table(cluster_name,table_settings.table_settings())
        }
    
    @app.post("/table/data") 
    def build(self, cluster_name: Annotated[str, Body()], database:Annotated[str, Body()], 
              table:Annotated[str, Body()], data:Annotated[List[Dict[str,Any]], Body()]):        
        data_refs = []        
        for item in data:
            itemref = ray.put(json.dumps(item,ensure_ascii=False))
            data_refs.append(itemref)

        return {
            "status":self.retrieval.build(cluster_name,database,table,data_refs)
        }
    
    @app.post("/table/commit")
    def commit(self,cluster_name: Annotated[str, Body()], database: Annotated[str, Body()], table: Annotated[str, Body()]):
        return {
            "status":self.retrieval.commit(cluster_name,database,table)
        }
    
    @app.post("/table/search")
    def search(self,cluster_name:Annotated[str, Body()], 
               database:Annotated[str, Body()], 
               table:Annotated[str, Body()], 
               query:SearchQueryParam):        
        return self.retrieval.search(cluster_name,database,table,query.search_query())
        
    
    @app.post("/table/close")
    def close(self,cluster_name:Annotated[str, Body()],database:Annotated[str, Body()],table:Annotated[str, Body()]):
        return {
            "status":self.retrieval.close(cluster_name,database,table)
        }
    
    @app.post("/table/truncate")
    def close(self,cluster_name:Annotated[str, Body()],database:Annotated[str, Body()],table:Annotated[str, Body()]):
        return {
            "status":self.retrieval.truncate(cluster_name,database,table)
        }
    
    @app.post("/table/close_and_delete_file")
    def closeAndDeleteFile(self,cluster_name:Annotated[str, Body()],database:Annotated[str, Body()],table:Annotated[str, Body()]):
        return {
            "status":self.retrieval.closeAndDeleteFile(cluster_name,database,table)
        }


def deploy_retrieval_rest_server(**kargs):
    # route_prefix="/retrievel",host="0.0.0.0",
    new_kargs = {**kargs}
    if "route_prefix" not in kargs:
        new_kargs["route_prefix"] = "/retrieval"
    if "host" not in kargs:
        new_kargs["host"] = "127.0.0.1"    
    serve.run(SimpleRest.bind(), **new_kargs)


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/retrieval/udf.py
import ray
from pyjava import RayContext
from pyjava.udf import UDFMaster,UDFWorker,UDFBuilder,UDFBuildInFunc
from typing import Any, NoReturn, Callable, Dict, List
from ray.util.client.common import ClientActorHandle, ClientObjectRef
from byzerllm import consume_model
from byzerllm.records import SearchQuery
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.records import SearchQuery
import json

def search_func(model,v):
    data = [json.loads(item) for item in v]
    results=[]
    for item in data:
        cluster_name = item["clusterName"]
        database = item["database"]
        table = item["table"]
        vector = []
        if "query.vector" in item:
            vector_str = item["query.vector"] 
            if vector_str.startswith("["):
                vector = json.loads(vector_str)
            else:
                vector = [float(i) for i in vector_str.split(",")]

        fields = []
        if "query.fields" in item:
            fields_str = item["query.fields"]
            if fields_str.startswith("["):
                fields = json.loads(fields_str)
            else:
                fields = fields_str.split(",")

        keyword = None
        if "query.keyword"  in item:
            keyword = item["query.keyword"]

        vector_field = None
        if "query.vectorField" in item:
            vector_field = item["query.vectorField"]    

        query = SearchQuery(
            keyword=keyword,
            fields=fields,
            vector=vector,
            vectorField=vector_field,
            limit=int(item.get("query.limit",10)),
        )
        docs = model.search(cluster_name,database,table,query)
        results.append(docs)
    return {"value":[json.dumps(docs,ensure_ascii=False)]}
                 
def init_retrieval_client(model_refs: List[ClientObjectRef], conf: Dict[str, str]) -> Any:
    consume_model(conf)
    byzer = ByzerRetrieval()
    byzer.launch_gateway()
    return byzer



##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/retrieval/__init__.py

import ray 
from ray.types import ObjectRef
from byzerllm.records import ClusterSettings, EnvSettings, JVMSettings, TableSettings,SearchQuery,ResourceRequirementSettings,ResourceRequirement
from typing import List,Dict,Any,Optional,Union
import byzerllm.utils.object_store_ref_util as ref_utils
import json

class ClusterBuilder:

    def __init__(self,br:'ByzerRetrieval') -> None:
        self.name = None
        self.location = None
        self.numNodes = 1
        self.nodeMemory = "2g"
        self.nodeCPU = 1
        self.enableZGC = True
        self.javaHome = None
        self.path = None

        self.cluster_settings = None
        self.env_settings = None
        self.jvm_settings = None
        self.resource_requirement_settings = None

        self.custom_resources = {}

        self.br = br

    def set_name(self,name:str):
        self.name = name
        return self
    
    def set_location(self,location:str):
        self.location = location
        return self
    
    def set_num_nodes(self,numNodes:int):
        self.numNodes = numNodes
        return self
    
    def set_node_memory(self,nodeMemory:str):
        self.nodeMemory = nodeMemory
        return self
    
    def set_custom_resource(self,k:str,v:float):
        self.custom_resources[k] = v
        return self

    
    def set_node_cpu(self,nodeCPU:int):
        self.nodeCPU = nodeCPU
        return self
    
    def set_enable_zgc(self):
        self.enableZGC = True
        return self
    
    def set_java_home(self,javaHome:str):
        self.javaHome = javaHome
        return self
    
    def set_path(self,path:str):
        self.path = path
        return self
        

    def build(self):     

        if self.name is None:
            raise Exception("name is required")
        
        if self.location is None:
            raise Exception("location is required")
        
        self.cluster_settings = ClusterSettings(self.name,self.location,self.numNodes)

        if self.javaHome is None:
            raise Exception("javaHome is required")
        
        if self.path is None:
            raise Exception("path is required")        

        self.env_settings = EnvSettings(javaHome=self.javaHome,path=self.path)

        jvmOptions = []
        resourceOptions = []
        if self.enableZGC:
            jvmOptions.append("-XX:+UseZGC")

        if self.nodeMemory:
            jvmOptions.append(f"-Xmx{self.nodeMemory}")

        if self.nodeCPU:
           resourceOptions.append(ResourceRequirement("CPU",self.nodeCPU))

        if self.custom_resources:
            for k,v in self.custom_resources.items():
                resourceOptions.append(ResourceRequirement(k,v))   

        self.jvm_settings = JVMSettings(jvmOptions)
        self.resource_requirement_settings = ResourceRequirementSettings(resourceOptions)        
    
    def start_cluster(self)-> bool:     
        self.build()
        return self.br.start_cluster(self.cluster_settings,self.env_settings,self.jvm_settings,self.resource_requirement_settings)


class ByzerRetrieval:
    
    def __init__(self):
        self.launched = False
        self.retrieval_gateway = None
        self.clusters = {}

    def launch_gateway(self)-> ray.actor.ActorHandle:
        
        try:
           self.retrieval_gateway = ray.get_actor("RetrievalGateway")
        except Exception:
            pass   

        if self.retrieval_gateway:
            self.launched = True
            return self.retrieval_gateway

        if self.launched:
            return ray.get_actor("RetrievalGateway")
        
        retrieval_gateway_launcher_clzz = ray.cross_language.java_actor_class("tech.mlsql.retrieval.RetrievalGatewayLauncher")
        retrieval_gateway_launcher = retrieval_gateway_launcher_clzz.remote()
        ray.get(retrieval_gateway_launcher.launch.remote()) 
        retrieval_gateway = ray.get_actor("RetrievalGateway")
        self.launched = True
        self.retrieval_gateway = retrieval_gateway
        return retrieval_gateway  

    def gateway(slef) -> ray.actor.ActorHandle:
        return ray.get_actor("RetrievalGateway")

    def cluster_builder(self) -> ClusterBuilder:
        br = self
        return ClusterBuilder(br)

    def start_cluster(self, cluster_settings:ClusterSettings,                       
                      env_settings:EnvSettings, 
                      jvm_settings:JVMSettings,
                      resource_requirement_settings:ResourceRequirementSettings = ResourceRequirementSettings([])) -> bool:                      
        if not self.launched:
            raise Exception("Please launch gateway first")
        
        if cluster_settings.name in self.clusters:
            raise Exception(f"Cluster {cluster_settings.name} already exists")
        
        try:
            ray.get_actor(cluster_settings.name)
            raise Exception(f"Cluster {cluster_settings.name} already exists")   
        except ValueError:
            pass
        
        obj_ref1 = self.retrieval_gateway.buildCluster.remote(
                    cluster_settings.json(),                    
                    env_settings.json(),
                    jvm_settings.json(),
                    resource_requirement_settings.json()
                    )
        
        return ray.get(obj_ref1) 
    
    def cluster(self,name:str) -> ray.actor.ActorHandle:
        if not self.launched:
            raise Exception("Please launch gateway first")
        
        if name in self.clusters:
            return self.clusters[name]
        
        cluster_ref = self.retrieval_gateway.getCluster.remote(name)
        # master_ref.buildFromRayObjectStore.remote("db1","table1",data_refs)
        cluster = ray.get(cluster_ref)
        self.clusters[name] = cluster
        return cluster
    
    def cluster_info(self,name:str) -> Dict[str,Any]:
        cluster = self.cluster(name)
        return json.loads(ray.get(cluster.clusterInfo.remote()))
    
    def is_cluster_exists(self,name:str) -> bool:
        try:
            ray.get_actor(name)
            return True
        except ValueError:
            return False
    
    def get_table_settings(self,cluster_name:str, database:str, table:str) -> Optional[TableSettings]:               
        cluster_info = self.cluster_info(cluster_name)
        target_table_settings = None
        for table_settings_dict in cluster_info["tableSettingsList"]:
            table_settings = TableSettings(**table_settings_dict)
            if table_settings.database == database and table_settings.table == table:
                target_table_settings = table_settings
                break        
        return target_table_settings
    
    def check_table_exists(self,cluster_name:str, database:str, table:str) -> bool:
        return self.get_table_settings(cluster_name,database,table) is not None
        
    
    def restore_from_cluster_info(self,cluster_info:Dict[str,Any]) -> bool:        
        return ray.get(self.retrieval_gateway.restoreFromClusterInfo.remote(json.dumps(cluster_info,ensure_ascii=False)))

    def create_table(self,cluster_name:str, tableSettings:TableSettings)-> bool:
        
        if self.check_table_exists(cluster_name,tableSettings.database,tableSettings.table):
            raise Exception(f"Table {tableSettings.database}.{tableSettings.table} already exists in cluster {cluster_name}")

        cluster = self.cluster(cluster_name)
        return ray.get(cluster.createTable.remote(tableSettings.json()))     

    def build(self, cluster_name:str, database:str, table:str, object_refs:List[ObjectRef[str]])-> bool:
        
        if not self.check_table_exists(cluster_name,database,table):
            raise Exception(f"Table {database}.{table} not exists in cluster {cluster_name}")
        
        cluster = self.cluster(cluster_name)
        
        data_ids = ref_utils.get_object_ids(object_refs)
        locations = ref_utils.get_locations(object_refs)
        return ray.get(cluster.buildFromRayObjectStore.remote(database,table,data_ids,locations))
    
    def build_from_dicts(self, cluster_name:str, database:str, table:str, data:List[Dict[str,Any]])-> bool:
        data_refs = []

        for item in data:
            itemref = ray.put(json.dumps(item ,ensure_ascii=False))
            data_refs.append(itemref)
        
        return self.build(cluster_name,database,table,data_refs)

    def delete_by_ids(self,cluster_name:str, database:str, table:str,ids:List[Any])-> bool:

        if not self.check_table_exists(cluster_name,database,table):
            raise Exception(f"Table {database}.{table} not exists in cluster {cluster_name}")

        cluster = self.cluster(cluster_name)
        return ray.get(cluster.deleteByIds.remote(database,table,json.dumps(ids,ensure_ascii=False)))
    
    def get_tables(self,cluster_name:str) -> List[TableSettings]:
        cluster_info = self.cluster_info(cluster_name)
        target_table_settings = []
        for table_settings_dict in cluster_info["tableSettingsList"]:
            target_table_settings.append(TableSettings(**table_settings_dict))
        return target_table_settings
    
    def get_databases(self,cluster_name:str) -> List[str]:
        table_settings_list = self.get_tables(cluster_name)
        return [x.database for x in table_settings_list]
        
    
    def shutdown_cluster(self,cluster_name:str)->bool:
        if not self.launched:
            raise Exception("Please launch gateway first")
                
        v = ray.get(self.retrieval_gateway.shutdownCluster.remote(cluster_name))
        if cluster_name in self.clusters:
            del self.clusters[cluster_name]        
        return v
            

    def commit(self,cluster_name:str, database:str, table:str)-> bool:
        
        if not self.check_table_exists(cluster_name,database,table):
            raise Exception(f"Table {database}.{table} not exists in cluster {cluster_name}")
        
        cluster = self.cluster(cluster_name)
        return ray.get(cluster.commit.remote(database,table))
    
    def truncate(self,cluster_name:str, database:str, table:str)-> bool:

        if not self.check_table_exists(cluster_name,database,table):
            raise Exception(f"Table {database}.{table} not exists in cluster {cluster_name}")

        cluster = self.cluster(cluster_name)
        return ray.get(cluster.truncate.remote(database,table))
    
    def close(self,cluster_name:str, database:str, table:str)-> bool:

        if not self.check_table_exists(cluster_name,database,table):
            raise Exception(f"Table {database}.{table} not exists in cluster {cluster_name}")

        cluster = self.cluster(cluster_name)
        return ray.get(cluster.close.remote(database,table))
    
    def closeAndDeleteFile(self,cluster_name:str, database:str, table:str)-> bool:

        if not self.check_table_exists(cluster_name,database,table):
            raise Exception(f"Table {database}.{table} not exists in cluster {cluster_name}")

        cluster = self.cluster(cluster_name)
        return ray.get(cluster.closeAndDeleteFile.remote(database,table))
    
    def search_keyword(self,cluster_name:str, 
                       database:str, 
                       table:str, 
                       filters: Dict[str,Any],
                       keyword:str, 
                       fields:List[str], 
                       limit:int=10) -> List[Dict[str,Any]]:                

        search = SearchQuery(database=database,table=table,filters=filters,keyword=keyword,fields=fields,vector=[],vectorField=None,limit=limit)
        cluster = self.cluster(cluster_name)
        v = cluster.search.remote(f"[{search.json()}]")
        return json.loads(ray.get(v))
    
    def search_vector(self,cluster_name:str, 
                       database:str, 
                       table:str, 
                       filters: Dict[str,Any],
                       vector:List[float], 
                       vector_field:str,                        
                       limit:int=10) -> List[Dict[str,Any]]:
                
        search = SearchQuery(database=database,table=table,filters=filters,keyword=None,fields=[],vector=vector,vectorField=vector_field,limit=limit)
        cluster = self.cluster(cluster_name)
        v = cluster.search.remote(f"[{search.json()}]")
        return json.loads(ray.get(v))
    
    def search(self,cluster_name:str,search_query: Union[List[SearchQuery],SearchQuery]) -> List[Dict[str,Any]]:        
        cluster = self.cluster(cluster_name)
        if isinstance(search_query,SearchQuery):
            search_query = [search_query]

        if not search_query:
            raise Exception("search_query is empty")
            
        v = cluster.search.remote(f"[{','.join([x.json() for x in search_query])}]")
        return json.loads(ray.get(v))  
    
    def filter(self,cluster_name:str,search_query: Union[List[SearchQuery],SearchQuery]) -> List[Dict[str,Any]]:        
        cluster = self.cluster(cluster_name)
        if isinstance(search_query,SearchQuery):
            search_query = [search_query]
        v = cluster.filter.remote(f"[{','.join([x.json() for x in search_query])}]")
        return json.loads(ray.get(v))

    def delete_by_filter(self,cluster_name:str, database:str, table:str,filter:Dict[str,Any])-> bool:
        cluster = self.cluster(cluster_name)
        return ray.get(cluster.deleteByFilter.remote(database,table,json.dumps(filter,ensure_ascii=False)))    


    
    
    

        


      

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/sft/dataset.py
import json
from torch.utils.data import Dataset


class SFTDataset(Dataset):
    def __init__(self, file, tokenizer, max_seq_length,**kwargs):
        self.tokenizer = tokenizer
        self.bos_token_id = tokenizer.bos_token_id
        self.eos_token_id = tokenizer.eos_token_id

        if self.bos_token_id is None and self.eos_token_id is None:
            print("bos_token_id or eos_token_id are both unset",flush=True)

        self.eos_token = tokenizer.eos_token
        self.bos_token = tokenizer.bos_token
        self.max_seq_length = max_seq_length
        print('Loading data: {}'.format(file))
        with open(file, 'r', encoding='utf8') as f:
            data_list = f.readlines()
        print("there are {} data in dataset".format(len(data_list)))
        self.data_list = data_list
        self.system_msg = kwargs.get('system_msg', 'You are a helpful assistant. Think it over and answer the user question correctly.\n')
        self.user_role = kwargs.get('user_role', 'User')
        self.assistant_role = kwargs.get('assistant_role', 'Assistant')
        self.user_role_prefix = f"{self.user_role}:"
        self.assistant_role_prefix = f"{self.assistant_role}:"

        self.dataset_tokens_count = 0

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, index):
                
        data = self.data_list[index]
        data = json.loads(data)
        conversation = data['conversation']

        # 收集多轮对话
        utterances = [self.system_msg]
        for x in conversation:
            utterances.append(f"{self.user_role_prefix}{x['human']}\n")
            utterances.append(f"{self.assistant_role_prefix}:{x['assistant']}\n")
        utterances_ids = self.tokenizer(utterances).input_ids

        # 模型的输入格式为：<s>User:input1</s>target1</s>input2</s>target2</s>...
        if self.bos_token_id is None:
            input_ids = []
            target_mask = []  # 用于对input进行mask，只计算target部分的loss           
        else:
            input_ids = [self.bos_token_id]
            target_mask = [0]  # 用于对input进行mask，只计算target部分的loss

        if self.eos_token_id is None:
            input_ids_suffix   = []
            target_mask_suffix = 0
        else:
            input_ids_suffix   = [self.eos_token_id]
            target_mask_suffix = 1
        
        for i, utterances_id in enumerate(utterances_ids):
            input_ids += (utterances_id + input_ids_suffix)
            if i % 2 == 0:
                target_mask += [0] * (len(utterances_id) + target_mask_suffix)
            else:
                target_mask += [1] * (len(utterances_id) + target_mask_suffix)

        assert len(input_ids) == len(target_mask)
        # 对长度进行截断
        input_ids = input_ids[:self.max_seq_length]
        target_mask = target_mask[:self.max_seq_length]
        attention_mask = [1] * len(input_ids)
        assert len(input_ids) == len(target_mask) == len(attention_mask)

        self.dataset_tokens_count += len(input_ids)

        inputs = {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'target_mask': target_mask
        }
        return inputs


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/sft/loss.py
import torch
import torch.nn as nn


class Loss(object):
    """
    所有loss的类父类
    """
    def __call__(self, model, inputs, training_args, return_outputs=False):
        """
        todo label smoothing
        用于计算loss。
        看源码发现，return_outputs=True为train时调用，return_outputs=False为eval和predict调用
        :param model: 模型
        :param inputs: 模型输入，dict
        :param training_args: 训练配置参数
        :param return_outputs:是否返回模型的输出
        :return:
        """
        raise NotImplemented


class TargetLMLoss(Loss):

    def __init__(self, ignore_index):
        super().__init__()
        self.ignore_index = ignore_index
        self.loss_fn = nn.CrossEntropyLoss(ignore_index=ignore_index)

    def __call__(self, model, inputs, training_args, return_outputs=False):
        input_ids = inputs['input_ids']
        attention_mask = inputs['attention_mask']
        target_mask = inputs['target_mask']
        # 模型前馈预测
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits

        # 将labels中不属于target的部分，设为ignore_index，只计算target部分的loss
        labels = torch.where(target_mask == 1, input_ids, self.ignore_index)
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        # Flatten the tokens
        loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        return (loss, outputs) if return_outputs else loss


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/sft/collator.py
from typing import Any, Dict, List
import torch


class SFTDataCollator(object):
    def __init__(self, tokenizer, max_seq_length, **kwargs):
        self.tokenizer = tokenizer
        self.max_seq_length = max_seq_length
        self.pad_token_id = tokenizer.pad_token_id

    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        # 找出batch中的最大长度
        lengths = [len(x['input_ids']) for x in batch]
        # 取出batch中的最大长度，如果超过max_seq_length，则取max_seq_length
        batch_max_len = min(max(lengths), self.max_seq_length)
        # batch_max_len = self.max_seq_length

        input_ids_batch, attention_mask_batch, target_mask_batch = [], [], []
        # truncate and padding
        for x in batch:
            input_ids = x['input_ids']
            attention_mask = x['attention_mask']
            target_mask = x['target_mask']
            padding_len = batch_max_len - len(input_ids)
            # padding
            input_ids = input_ids + [self.pad_token_id] * padding_len
            attention_mask = attention_mask + [0] * padding_len
            target_mask = target_mask + [0] * padding_len
            # truncate
            input_ids = input_ids[:self.max_seq_length]
            attention_mask = attention_mask[:self.max_seq_length]
            target_mask = target_mask[:self.max_seq_length]

            input_ids_batch.append(input_ids)
            attention_mask_batch.append(attention_mask)
            target_mask_batch.append(target_mask)

        # 将list转换为tensor，得到最终的的模型输入
        input_ids_batch = torch.tensor(input_ids_batch, dtype=torch.long)
        attention_mask_batch = torch.tensor(attention_mask_batch, dtype=torch.long)
        target_mask_batch = torch.tensor(target_mask_batch, dtype=torch.long)
        inputs = {
            'input_ids': input_ids_batch,
            'attention_mask': attention_mask_batch,
            'target_mask': target_mask_batch
        }
        return inputs



##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/sft/trainer.py
import transformers
from transformers import (
    PreTrainedModel,
    TrainingArguments,
    DataCollator,
    PreTrainedTokenizerBase,
    EvalPrediction,
    TrainerCallback,
)
from typing import Callable, Dict, List, Optional, Tuple, Union, Any
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers.utils import (
    logging,
)
from typing import Optional
import os
import torch


logger = logging.get_logger(__name__)

# Name of the files used for checkpointing
TRAINING_ARGS_NAME = "training_args.bin"
TRAINER_STATE_NAME = "trainer_state.json"
OPTIMIZER_NAME = "optimizer.pt"
SCHEDULER_NAME = "scheduler.pt"
SCALER_NAME = "scaler.pt"


class Trainer(transformers.Trainer):
    """
    主要修改逻辑：通过传入compute_loss，支持自定义loss计算方式
    """
    def __init__(
            self,
            model: Union[PreTrainedModel, nn.Module] = None,
            args: TrainingArguments = None,
            data_collator: Optional[DataCollator] = None,
            train_dataset: Optional[Dataset] = None,
            eval_dataset: Optional[Dataset] = None,
            tokenizer: Optional[PreTrainedTokenizerBase] = None,
            model_init: Callable[[], PreTrainedModel] = None,
            compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
            callbacks: Optional[List[TrainerCallback]] = None,
            optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
            preprocess_logits_for_metrics: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = None,
            compute_loss=None,
    ):
        super(Trainer, self).__init__(
            model=model,
            args=args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            model_init=model_init,
            compute_metrics=compute_metrics,
            callbacks=callbacks,
            optimizers=optimizers,
            preprocess_logits_for_metrics=preprocess_logits_for_metrics,
        )
        self.loss_func = compute_loss

    def compute_loss(self, model, inputs, return_outputs=False):
        """
        重写loss的计算方式
        How the loss is computed by Trainer. By default, all models return the loss in the first element.

        Subclass and override for custom behavior.
        """
        return self.loss_func(model, inputs, self.args, return_outputs)


class LoRATrainer(Trainer):
    """
    修改checkkpoint的保存逻辑，只保存lora
    """
    def _save(self, output_dir: Optional[str] = None, state_dict=None):
        # If we are executing this function, we are the process zero, so we don't check for that.
        output_dir = output_dir if output_dir is not None else self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        logger.info(f"Saving model checkpoint to {output_dir}")
        # 保存lora权重和配置
        self.model.save_pretrained(
            output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors
        )

        if self.tokenizer is not None:
            self.tokenizer.save_pretrained(output_dir)

        # Good practice: save your training arguments together with the trained model
        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/sft/argument.py
from dataclasses import dataclass, field
from typing import Optional


@dataclass
class CustomizedArguments:
    """
    一些自定义参数
    """
    max_seq_length: int = field(metadata={"help": "输入最大长度"})
    train_file: str = field(metadata={"help": "训练集"})
    model_name_or_path: str = field(metadata={"help": "预训练权重路径"})
    eval_file: Optional[str] = field(default="", metadata={"help": "the file of training data"})


@dataclass
class QLoRAArguments:
    """
    一些自定义参数
    """
    max_seq_length: int = field(metadata={"help": "输入最大长度"})
    train_file: str = field(metadata={"help": "训练集"})
    model_name_or_path: str = field(metadata={"help": "预训练权重路径"})
    task_type: str = field(default="", metadata={"help": "预训练任务：[sft, pretrain]"})
    eval_file: Optional[str] = field(default="", metadata={"help": "the file of training data"})
    lora_rank: Optional[int] = field(default=64, metadata={"help": "lora rank"})
    lora_alpha: Optional[int] = field(default=16, metadata={"help": "lora alpha"})
    lora_dropout: Optional[float] = field(default=0.05, metadata={"help": "lora dropout"})    



##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/sft/model.py
import transformers
from typing import Tuple, Union
import torch
from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions, CausalLMOutputWithPast
from .loss import TargetLMLoss
from transformers.utils import logging


logger = logging.get_logger(__name__)


class BloomForCausalLM(transformers.BloomForCausalLM):
    """
    继承自BloomForCausalLM，区别在于只计算target部分的loss
    """
    def forward(
        self,
        input_ids=None,
        past_key_values=None,
        attention_mask=None,
        labels=None,
        target_mask=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        return_loss=False,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        transformer_outputs = self.transformer(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        hidden_states = transformer_outputs[0]

        lm_logits = self.lm_head(hidden_states)

        loss = None
        if return_loss:
            loss_fn = TargetLMLoss(ignore_index=self.config.pad_token_id)
            loss = loss_fn(lm_logits, input_ids, target_mask)

        if not return_dict:
            output = (lm_logits,) + transformer_outputs[1:]
            return ((loss,) + output) if loss is not None else output

        return CausalLMOutputWithCrossAttentions(
            loss=loss,
            logits=lm_logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
        )



##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/sft/__init__.py
import ray
import json
import os
import uuid
import shutil
from typing import Optional, Tuple, List, Dict, Any
from datetime import datetime
from typing import Dict, Any,List,Generator
from pyjava.storage import streaming_tar as STar
from pyjava import RayContext
from pyjava.api.mlsql import DataServer
from byzerllm import BlockRow
from ray.air.util.torch_dist import (
    ActorHandle,
    _get_node_and_gpu_ids,
    _init_torch_distributed,
    get_address_and_port,
)
from . import qlora as QLoraTrainer
from byzerllm import restore_model
from .. import print_flush

DEFAULT_QLORA_CONFIG = {
    'output_dir': '',
    'model_name_or_path': '',
    'train_file': '',
    'num_train_epochs': 1,
    'per_device_train_batch_size': 1,
    'gradient_accumulation_steps': 16,
    'learning_rate': 0.0002,
    'max_seq_length': 1024,
    'logging_steps': 300,
    'save_steps': 500,
    'save_total_limit': 1,
    'lr_scheduler_type': 'cosine',
    'warmup_steps': 3000,
    'lora_rank': 64,
    'lora_alpha': 16,
    'lora_dropout': 0.05,
    'gradient_checkpointing': False,
    'disable_tqdm': False,
    'optim': 'paged_adamw_32bit',
    'seed': 42,
    'fp16': True,
    'report_to': 'tensorboard',
    'dataloader_num_workers': 0,
    'save_strategy': 'steps',
    'weight_decay': 0,
    'max_grad_norm': 0.3,
    'remove_unused_columns': False
 }

@ray.remote
class SFT:
    def __init__(self,data_refs:List[DataServer],sft_config:Dict[str,Any],train_params:Dict[str,str],sys_conf: Dict[str, str]) -> None:
        if "runIn" in sys_conf and sys_conf["runIn"] == "driver":
            raise Exception('''
                SFT can not run in driver. 
                Try the one of the following instructions:

                1. !byzerllm setup sft; 
                2. !byzerllm setup "runIn=executor"
            ''')
             
        self.sft_config = sft_config
        self.data_refs = data_refs
        self.train_params = train_params
        self.sys_conf = sys_conf

    def setup_tensorboard(self)->Optional[Tuple[str,int]]:        
        logging_dir = self.sft_config["logging_dir"]
        import subprocess               
        ip, port = get_address_and_port()
        log_dir = logging_dir            
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)
        tb_process = subprocess.Popen(['tensorboard', '--logdir', log_dir,"--port",str(port),"--host",ip], stdout=subprocess.PIPE, stderr=subprocess.PIPE)                                    
        self.tensorboard_pid = tb_process.pid
        return (ip,port)

    def train(self,args:List[str]):
        
        sft_name = self.train_params["name"] if "name" in self.train_params else f"sft-{self.sys_conf['OWNER']}"
        print_flush(f"[{sft_name}] SFT Config: {self.sft_config}")

        if not os.path.exists(self.sft_config["output_dir"]):
            os.makedirs(self.sft_config["output_dir"])
        
        train_file = self.sft_config["train_file"]
        if not os.path.exists(train_file): 
            os.makedirs(os.path.dirname(train_file))

        if "localModelDir" not in self.train_params:
            restore_model(self.conf,self.sft_config["model_name_or_path"])                                  
        
        
        # prepare data
        if self.data_refs:
            with open(train_file,"w") as f: 
                count = 0
                for item in RayContext.collect_from(self.data_refs):                
                    if "conversation" in item:
                        item["conversation"] = item["conversation"].tolist()
                        s = json.dumps(item,ensure_ascii=False)               
                        f.write(s+"\n")                    
                    elif "instruction" in item and "output" in item :
                        # support alpaca format data
                        history = item.get("history",[]) 
                        
                        if hasattr(history,"tolist"):
                            history = history.tolist()

                        input = item.get("input","")
                        conversation = [sub.tolist() for sub in history]
                        conversation = [{"human":x[0],"assistant":x[1]} for x in conversation]
                        latest_conversation = [{"human":item["instruction"]+"\n"+input,"assistant":item["output"]}] if "instruction" in item and item["instruction"] else []
                        s = json.dumps({
                            "category":"",
                            "conversation":conversation + latest_conversation,
                            "conversation_id":count,
                            "dataset":"",                
                        },ensure_ascii=False)               
                        f.write(s+"\n") 
                    else:
                        raise Exception(f"Unknown data format: {item}")                                            
                    count += 1       
        
        ip,port = self.setup_tensorboard()
        print_flush(f"[{sft_name}] Tensorboard is running at: {ip}:{port}")

        final_path = QLoraTrainer.train(json.dumps(self.sft_config,ensure_ascii=False), args, {
            "model_type": self.train_params.get("model_type","casual_lm"),"sft_name":sft_name
        })
        # copy the pretrained model to output dir
        if self.train_params.get("skipCopyPretrainedModel","false") == "false":
            print_flush(f'[{sft_name}] Copy pretrained model: {self.sft_config["model_name_or_path"]} to {os.path.join(final_path,"pretrained_model")}')        
            shutil.copytree(self.sft_config["model_name_or_path"],os.path.join(final_path,"pretrained_model"))
        
        # if detached, do not transfer the model to delta lake
        detached = self.train_params.get("detached","false") == "true"
        if detached:
            print_flush(f'''
              [{sft_name}] Train Actor is already finished. You can check the model in: {final_path}              
              ''') 
            return ([],0)
        
        # push the model to ray object store
        result = []
        count = 0
        print_flush(f"[{sft_name}] Store model({final_path}) to Ray object store")
        for item in STar.build_rows_from_file(final_path):
            if count % 1000 == 0:
                print_flush(f"[{sft_name}] Progress: {count} processed")
            count += 1    
            result.append(ray.put(item))
        
        print_flush(f'''
              [{sft_name}] Train Actor already finished.
              [{sft_name}] It may take a while to transfer the model from Ray object store to delta lake. 
              [{sft_name}] Try to check the progress in Byzer console or Byzer Notebook. 
              ''')    
        return (result,count) 

def sft_train(data_refs:List[DataServer],train_params:Dict[str,str],sys_conf: Dict[str, str])->Generator[BlockRow,Any,Any]:
    
    localPathPrefix = train_params.get("localPathPrefix","/tmp/byzerllm")
    
    current_time = datetime.now()
    formatted_time = current_time.strftime("%Y%m%d-%H-%M-%S")
    sft_name = train_params["name"] if "name" in train_params else f"sft-{sys_conf['OWNER']}-{formatted_time}"        
    
    rd = f"{sft_name}-{str(uuid.uuid4())}"    
    
    model_dir = os.path.join(localPathPrefix,rd,"pretrained_model")

    if "localModelDir" in train_params:
        model_dir = train_params["localModelDir"]

    output_dir = os.path.join(localPathPrefix,rd,"finetune_model")
    logging_dir = os.path.join(localPathPrefix,rd,"logging")
    data_dir = os.path.join(localPathPrefix,rd,"finetune_data")
    
    if "data_dir" in train_params:
        data_dir = train_params["data_dir"]

    data_file = os.path.join(data_dir,"data.jsonl")

    train_worker_conf = {}
    if "num_cpus" in sys_conf:
        train_worker_conf["num_cpus"] = float(sys_conf["num_cpus"])

    if "num_gpus" in sys_conf:
        train_worker_conf["num_gpus"] = float(sys_conf["num_gpus"])

    custom_resources = [(key.split("resource.")[1], float(sys_conf[key])) for key in
                        sys_conf.keys() if
                        key.startswith("resource.")]

    if len(custom_resources) > 0:
        train_worker_conf["resources"] = dict(custom_resources)   
    
    train_params_sft = {}
    
    for k,v in train_params.items():
        if k.startswith("sft."):
            # sft.float.num_train_epochs
            tpe = k.split(".")[1]
            new_k = k.split(".")[2]
            new_v = v
            if tpe == "float":
              new_v = float(v)
            elif tpe == "int":
                new_v = int(v)
            elif tpe == "bool":
                new_v = v == "true"
            elif tpe == "str":
                new_v = v
            elif tpe == "list":
                new_v = json.loads(v)
            elif tpe == "dict":
                new_v = json.loads(v)            
            train_params_sft[new_k] = new_v

    if "config" in train_params:
        train_params_sft = {**json.loads(train_params["config"]),**train_params_sft}

    sft_config = {
       **DEFAULT_QLORA_CONFIG,
       **train_params_sft,
       **{
           "output_dir":output_dir,
           "logging_dir": logging_dir,
           "model_name_or_path":model_dir,
           "train_file":data_file,
       }
    }         
    
    detached = train_params.get("detached","true") == "true"
    
    if detached:
        print_flush(f"[{sft_name}] Detached mode is enabled. ")
        train_actor = SFT.options(name=sft_name,lifetime="detached", **train_worker_conf).remote(data_refs,sft_config,train_params,sys_conf)
        train_actor.train.remote([])
        return [] 

    train_actor = SFT.options(name=sft_name,**train_worker_conf).remote(data_refs,sft_config,train_params,sys_conf)
    try:        
        items,obj_count = ray.get(train_actor.train.remote([]))
    except Exception as e:
        ray.kill(train_actor)
        raise e  
            
    print_flush(f"[{sft_name}] Transform Model from Ray object store to new storage(delta lake), total refs: {obj_count}. ")
    count = 0
    for item in items:
        if count % 1000 == 0:
            print_flush(f"[{sft_name}] Process: {float(count)/obj_count*100}%")
        count += 1
        yield ray.get(item)
        

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/sft/merge_lora.py
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
from pyjava.api.mlsql import DataServer
from pyjava.storage import streaming_tar as STar
import torch
from typing import Any,Any,Dict, List,Tuple,Generator
from byzerllm import BlockRow
import os

class MergeLoraActor(object):
    
    def merge_lora_to_base_model(self,data_refs:List[DataServer],
                train_params:Dict[str,str],
                conf: Dict[str, str]):
        
        model_name_or_path = train_params.get("modelNameOrPath",train_params.get("model_name_or_path",""))
        adapter_name_or_path = train_params.get("adapterNameOrPath",train_params.get("adapter_name_or_path",""))
        save_path = train_params.get("savePath",train_params.get("save_path",""))        
        
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path,
            trust_remote_code=True
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_name_or_path,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            torch_dtype=torch.float16,
            device_map='auto'
        )
        model = PeftModel.from_pretrained(model, adapter_name_or_path)
        model = model.merge_and_unload()

        if not os.path.exists(save_path):
            os.makedirs(save_path)

        tokenizer.save_pretrained(save_path)
        model.save_pretrained(save_path)
        # STar.build_rows_from_file(save_path)          

def merge_lora_to_base_model(data_refs:List[DataServer],
              train_params:Dict[str,str],
              conf: Dict[str, str])->Generator[BlockRow,Any,Any]:
    import ray
    custom_resources = [(key.split("resource.")[1], float(conf[key])) for key in
                            conf.keys() if
                            key.startswith("resource.")]
    worker_conf = {}

    if len(custom_resources) > 0:
        worker_conf["resources"] = dict(custom_resources)
    merge_job_name = train_params["name"] if "name" in train_params else f"merge-lora-{conf['OWNER']}"
    worker = ray.remote(name=merge_job_name, **worker_conf)(MergeLoraActor).remote()
    ray.get(worker.merge_lora_to_base_model.remote(
        data_refs,
        train_params,
        conf))
    return []

  


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/sft/qlora.py
from typing import List,Dict
import json
from transformers import AutoTokenizer, BitsAndBytesConfig
from byzerllm.utils.metrics import Metric
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import (
    set_seed,
    HfArgumentParser,
    TrainingArguments,
    AutoModelForCausalLM,
    AutoModel
)
import argparse
import os
from os.path import join
import torch
import bitsandbytes as bnb
from collections import defaultdict

from .collator import SFTDataCollator
from .dataset import SFTDataset
from .argument import QLoRAArguments
from .trainer import LoRATrainer
from .loss import TargetLMLoss


def verify_model_dtype(model):
    """
    查看模型种各种类型的参数的情况
    """
    dtype2param_num = defaultdict(int)  # 每种数据类型的参数量
    dtype2param_name = defaultdict(list)  # 每种数据类型的参数名称
    dtype2trainable_param_num = defaultdict(int)  # 每种数据类型参与训练的参数量
    dtype2trainable_param_name = defaultdict(list)  # 每种数据类型参与训练的参数名称
    for name, p in model.named_parameters():
        dtype = p.dtype
        dtype2param_num[dtype] += p.numel()
        dtype2param_name[dtype].append(name)
        if p.requires_grad:
            dtype2trainable_param_num[dtype] += p.numel()
            dtype2trainable_param_name[dtype].append(name)
    # 统计全部参数中，各种类型参数分布
    total = 0
    print('verify all params of the model')
    for k, v in dtype2param_num.items():
        total += v
    for k, v in dtype2param_num.items():
        print(k, v, v / total)
    for k, v in dtype2trainable_param_name.items():
        print(k, v)

    print()
    # 统计可训练参数中，各种类型参数分布
    print('verify trainable params the model')
    total_trainable = 0
    for k, v in dtype2trainable_param_num.items():
        total_trainable += v
    for k, v in dtype2trainable_param_num.items():
        print(k, v, v / total_trainable)
    for k, v in dtype2trainable_param_num.items():
        print(k, v)


def find_all_linear_names(model):
    """
    找出所有全连接层，为所有全连接添加adapter
    """
    cls = bnb.nn.Linear4bit
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])

    if 'lm_head' in lora_module_names:  # needed for 16-bit
        lora_module_names.remove('lm_head')
    return list(lora_module_names)


def setup_everything(lora_config:str, args:List[str]):
    parser = argparse.ArgumentParser()    
    args = parser.parse_args(args=args)
    
    # 读取训练的参数配置
    parser = HfArgumentParser((QLoRAArguments, TrainingArguments))
    # 解析得到自定义参数，以及自带参数
    args, training_args = parser.parse_dict(json.loads(lora_config),allow_extra_keys=True)
    # 设置随机种子
    set_seed(training_args.seed)
    return args, training_args


def init_components(args, training_args,extra_params):
    """
    初始化各个组件
    """
    print('Initializing components...')
    # 下面的设置至关重要，否则无法多卡训练
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    ddp = world_size != 1
    # training_args.ddp_find_unused_parameters = False if ddp else None
    device_map = "auto"
    # if we are in a distributed setting, we need to set the device map and max memory per device
    if os.environ.get('LOCAL_RANK') is not None:
        local_rank = int(os.environ.get('LOCAL_RANK', '0'))
        device_map = {'': local_rank}
    # 加载tokenzier
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_name_or_path,
        trust_remote_code=True,
    )
    # 部分tokenizer没有pad_token_id
    if tokenizer.pad_token_id is None:
        if tokenizer.unk_token_id is None:
            print(f"tokenizer has no pad_token_id and unk_token_id, setting it to 0")
            tokenizer.pad_token_id = 0
        else:                
            print(f"tokenizer has no pad_token_id({tokenizer.pad_token_id}), setting it to unk_token_id({tokenizer.unk_token_id})")
            tokenizer.pad_token_id = tokenizer.unk_token_id
    # 如果两者相同，模型训练时不会计算eos_token_id的loss
    if tokenizer.pad_token_id == tokenizer.eos_token_id:
        raise Exception('pad_token_id should not be equal to eos_token_id')       
    
    # 加载模型  
    model_type = extra_params.get("model_type","casual_lm")

    if model_type == "casual_lm":
        model = AutoModelForCausalLM.from_pretrained(
            args.model_name_or_path,
            device_map=device_map,
            load_in_4bit=True,
            torch_dtype=torch.float16,
            trust_remote_code=True,
            quantization_config=BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False,
            ),
        )    
        model.tie_weights()
    else:
        model = AutoModel.from_pretrained(
            args.model_name_or_path,
            device_map=device_map,
            load_in_4bit=True,
            torch_dtype=torch.float16,
            trust_remote_code=True,
            quantization_config=BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False,
            ),
        )     
    # casts all the non int8 modules to full precision (fp32) for stability
    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=training_args.gradient_checkpointing)    
    print(f'memory footprint of model: {model.get_memory_footprint()/(1024*1024*1024)} GB')
    # 找到所有需要插入adapter的全连接层
    target_modules = find_all_linear_names(model)
    # 初始化lora配置
    config = LoraConfig(
        r=args.lora_rank,
        lora_alpha=args.lora_alpha,
        target_modules=target_modules,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, config)
    model.print_trainable_parameters()
    model.config.torch_dtype = torch.float32

    # 查看模型种各种类型的参数的情况
    verify_model_dtype(model)

    # 初始化损失函数
    print(f'Initializing loss function (ignore_index={tokenizer.pad_token_id})...',flush=True)
    loss_func = TargetLMLoss(ignore_index=tokenizer.pad_token_id)
    
    train_dataset = SFTDataset(args.train_file, tokenizer, args.max_seq_length, **extra_params)
    data_collator = SFTDataCollator(tokenizer, args.max_seq_length, **extra_params)

    # 初始化Trainer
    trainer = LoRATrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_loss=loss_func
    )
    return trainer


def train(lora_config:str, args:List[str],extra_params={})->str:

    sft_name = extra_params["sft_name"]   
    # 进行一些配置和检查
    parsed_args, training_args = setup_everything(lora_config, args)
    # 加载各种组件
    trainer = init_components(parsed_args, training_args,extra_params)
    # 开始训练
    print(f"*** [{sft_name}] starting training ***")
    train_result = trainer.train()

    # 打印 token 总数
    print(f"[{sft_name}] total tokens: {trainer.train_dataset.dataset_tokens_count}",flush=True)
    token_metrics = Metric()
    token_metrics.inc(f"sft_{sft_name}_tokens_num",trainer.train_dataset.dataset_tokens_count)
    token_metrics.push()

    # 保存最好的checkpoint
    final_save_path = join(training_args.output_dir, 'final')
    trainer.save_model(final_save_path)  # Saves the tokenizer too
    # 保存训练指标
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()  
    return final_save_path  




##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/metrics/__init__.py
from prometheus_client import CollectorRegistry, Gauge,Counter, pushadd_to_gateway
from byzerllm.utils.config import get_mlsql_config_pushgateway_address,get_mlsql_config
from typing import Union,Dict
import ray

class Metric:

    def __init__(self):
        self.registry = CollectorRegistry()
        config = get_mlsql_config()
        self.metric_enabled = False
        self.pushgateway_address = None
        if config is not None:
            self.pushgateway_address = ray.get(config.getitem.remote("spark.mlsql.pushgateway.address",None))
            self.metric_enabled = True

        self.gauges = {}
        self.counters = {}        

    def inc(self, name:str,value: Union[int, float] = 1.0, tags: Dict[str, str] = None):
        if not self.metric_enabled:
            return
        if name not in self.counters:
            self.counters[name] = Counter(name, '', registry=self.registry)                    
        self.counters[name].inc(value)   

    def push(self):
        if not self.metric_enabled:
            return
        if self.pushgateway_address is not None:
            pushadd_to_gateway(self.pushgateway_address, job='pushgateway', registry=self.registry)      
        
    



##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/client/parallel_utils.py
import concurrent.futures

def chat_oai(llm,workers: int=3, **kwargs):
    """
    Invoke llm.chat_oai in multi-threading with specified size
    and return the combined result.
    """
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        # Submit tasks to the executor
        futures = [executor.submit(llm.chat_oai, **kwargs) for _ in range(workers)]

        # Collect results as they are completed
        results = [future.result() for future in concurrent.futures.as_completed(futures)]
    
    return results

def get_single_result(ts):    
    if not hasattr(ts[0][0],"values")  and not hasattr(ts[0][0],"value"):
        for t in ts:
            if t[0].output:
                return t       
        

    if hasattr(ts[0][0],"values"):        
        for t in ts:
            if t[0].values:
                return t 
    
    if hasattr(ts[0][0],"value"):        
        for t in ts:
            if t[0].value:
                return t        
    
    return None

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/client/message_utils.py
from typing import List, Dict,Any
import copy

def termindate_message(message:Dict[str,Any]):
    if "metadata" not in message:
        message["metadata"] = {}
    message["metadata"]["TERMINATE"] = True
    return message

def un_termindate_message(message:Dict[str,Any]):
    if "metadata" not in message:
        message["metadata"] = {}
    message["metadata"]["TERMINATE"] = False
    return message

def success_message(message:Dict[str,Any]):
    if "metadata" not in message:
        message["metadata"] = {}
    message["metadata"]["code"] = 0
    return message

def fail_message(message:Dict[str,Any]):
    if "metadata" not in message:
        message["metadata"] = {}
    message["metadata"]["code"] = 1
    return message

def is_success(message:Dict[str,Any]):
    if "metadata" not in message or "code" not in message["metadata"]:
        return False
    return message["metadata"]["code"] == 0

def copy_error_count(message:Dict[str,Any],new_message:Dict[str,Any]):
    if "metadata" not in message:
        message["metadata"] = {}
    if "metadata" not in new_message:
        new_message["metadata"] = {}
    new_message["metadata"]["error_count"] = message["metadata"].get("error_count",0)
    return new_message

def get_error_count(message:Dict[str,Any]):
    if "metadata" not in message:
        message["metadata"] = {}   
    return message["metadata"].get("error_count",0)

def inc_error_count(message:Dict[str,Any]):
    if "metadata" not in message:
        message["metadata"] = {}
    message["metadata"]["error_count"] = message["metadata"].get("error_count",0) + 1
    return message

def check_error_count(message:Dict[str,Any],max_error_count:int=3):
    if "metadata" not in message:
        message["metadata"] = {}
    return message["metadata"].get("error_count",0) >= max_error_count

def padding_messages_merge(data:List[Dict[str,Any]]):
    '''
    merge the neighbor messages with the same role
    '''
    temp_data = copy.deepcopy(data)
    padded_data = []
    last_role = None    
    for message in temp_data:
        if message["role"] == "system":
            padded_data.append(message)
            continue
        if last_role is None:
            if message["role"] == "assistant":
                padded_data.append({'content': 'continue', 'role': 'user'})                            
            padded_data.append(message)            
            last_role = message['role']
        elif last_role == message['role']:
            padded_data[-1]["content"] += f"\n{message['content']}"
        else:
            padded_data.append(message)            
            last_role = message['role']        
    if padded_data[-1]["role"] == "assistant":
        padded_data.append({'content': 'continue', 'role': 'user'})    
    return padded_data

def padding_messages_expand(data:Dict[str,Any]):
    '''
    padding the message between the neighbor messages with the same role
    '''
    temp_data = copy.deepcopy(data)
    padded_data = []        
    last_role = None                
    for message in temp_data:   
        if message["role"] == "system":
            padded_data.append(message)
            continue         
        if (last_role is None) and (message['role'] == 'assistant'):
            padded_data.append({'content': 'continue', 'role': 'user'})
            padded_data.append(message)

        elif (last_role is None) and (message['role'] == 'user'):                
            padded_data.append(message)    

        elif (last_role == message['role']) and (message['role'] == 'assistant'):
            padded_data.append({'content': 'continue', 'role': 'user'})
            padded_data.append(message)

        elif (last_role == message['role']) and (message['role'] == 'user'):
            padded_data.append({'content': 'continue', 'role': 'assistant'})
            padded_data.append(message)

        elif (last_role == message['role']) and (message['role'] == 'user'):                                        
            padded_data.append(message)

        else:
            padded_data.append(message)    
        
        last_role = message['role']
    
    if last_role == 'assistant':
        padded_data.append({'content': 'continue', 'role': 'user'})

    return padded_data

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/client/img_utils.py
import base64
import mimetypes
import re
from io import BytesIO
from typing import Any, Dict, List, Optional, Tuple, Union

import requests
from PIL import Image


def get_image_data(image_file: str, use_b64=True) -> bytes:
    if image_file.startswith("http://") or image_file.startswith("https://"):
        response = requests.get(image_file)
        content = response.content
    elif re.match(r"data:image/(?:png|jpeg);base64,", image_file):
        return re.sub(r"data:image/(?:png|jpeg);base64,", "", image_file)
    else:
        image = Image.open(image_file).convert("RGB")
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        content = buffered.getvalue()

    if use_b64:
        return base64.b64encode(content).decode("utf-8")
    else:
        return content


def llava_formater(prompt: str, order_image_tokens: bool = False) -> Tuple[str, List[str]]:
    """
    Formats the input prompt by replacing image tags and returns the new prompt along with image locations.

    Parameters:
        - prompt (str): The input string that may contain image tags like <img ...>.
        - order_image_tokens (bool, optional): Whether to order the image tokens with numbers.
            It will be useful for GPT-4V. Defaults to False.

    Returns:
        - Tuple[str, List[str]]: A tuple containing the formatted string and a list of images (loaded in b64 format).
    """

    # Initialize variables
    new_prompt = prompt
    image_locations = []
    images = []
    image_count = 0

    # Regular expression pattern for matching <img ...> tags
    img_tag_pattern = re.compile(r"<img ([^>]+)>")

    # Find all image tags
    for match in img_tag_pattern.finditer(prompt):
        image_location = match.group(1)

        try:
            img_data = get_image_data(image_location)
        except Exception as e:
            # Remove the token
            print(f"Warning! Unable to load image from {image_location}, because of {e}")
            new_prompt = new_prompt.replace(match.group(0), "", 1)
            continue

        image_locations.append(image_location)
        images.append(img_data)

        # Increment the image count and replace the tag in the prompt
        new_token = f"<image {image_count}>" if order_image_tokens else "<image>"

        new_prompt = new_prompt.replace(match.group(0), new_token, 1)
        image_count += 1

    return new_prompt, images


def convert_base64_to_data_uri(base64_image):
    def _get_mime_type_from_data_uri(base64_image):
        # Decode the base64 string
        image_data = base64.b64decode(base64_image)
        # Check the first few bytes for known signatures
        if image_data.startswith(b"\xff\xd8\xff"):
            return "image/jpeg"
        elif image_data.startswith(b"\x89PNG\r\n\x1a\n"):
            return "image/png"
        elif image_data.startswith(b"GIF87a") or image_data.startswith(b"GIF89a"):
            return "image/gif"
        elif image_data.startswith(b"RIFF") and image_data[8:12] == b"WEBP":
            return "image/webp"
        return "image/jpeg"  # use jpeg for unknown formats, best guess.

    mime_type = _get_mime_type_from_data_uri(base64_image)
    data_uri = f"data:{mime_type};base64,{base64_image}"
    return data_uri


def gpt4v_formatter(prompt: str) -> List[Union[str, dict]]:
    """
    Formats the input prompt by replacing image tags and returns a list of text and images.

    Parameters:
        - prompt (str): The input string that may contain image tags like <img ...>.

    Returns:
        - List[Union[str, dict]]: A list of alternating text and image dictionary items.
    """
    output = []
    last_index = 0
    image_count = 0

    # Regular expression pattern for matching <img ...> tags
    img_tag_pattern = re.compile(r"<img ([^>]+)>")

    # Find all image tags
    for match in img_tag_pattern.finditer(prompt):
        image_location = match.group(1)

        try:
            img_data = get_image_data(image_location)
        except Exception as e:
            # Warning and skip this token
            print(f"Warning! Unable to load image from {image_location}, because {e}")
            continue

        # Add text before this image tag to output list
        output.append({"type": "text", "text": prompt[last_index : match.start()]})

        # Add image data to output list
        output.append({"type": "image_url", "image_url": {"url": convert_base64_to_data_uri(img_data)}})

        last_index = match.end()
        image_count += 1

    # Add remaining text to output list
    output.append({"type": "text", "text": prompt[last_index:]})
    return output


def extract_img_paths(paragraph: str) -> list:
    """
    Extract image paths (URLs or local paths) from a text paragraph.

    Parameters:
        paragraph (str): The input text paragraph.

    Returns:
        list: A list of extracted image paths.
    """
    # Regular expression to match image URLs and file paths
    img_path_pattern = re.compile(
        r"\b(?:http[s]?://\S+\.(?:jpg|jpeg|png|gif|bmp)|\S+\.(?:jpg|jpeg|png|gif|bmp))\b", re.IGNORECASE
    )

    # Find all matches in the paragraph
    img_paths = re.findall(img_path_pattern, paragraph)
    return img_paths


def _to_pil(data: str) -> Image.Image:
    """
    Converts a base64 encoded image data string to a PIL Image object.

    This function first decodes the base64 encoded string to bytes, then creates a BytesIO object from the bytes,
    and finally creates and returns a PIL Image object from the BytesIO object.

    Parameters:
        data (str): The base64 encoded image data string.

    Returns:
        Image.Image: The PIL Image object created from the input data.
    """
    return Image.open(BytesIO(base64.b64decode(data)))


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/client/math_utils.py
from typing import Optional

_MATH_PROMPT = "{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}."
_MATH_CONFIG = {
    "model": DEFAULT_MODEL,
    "prompt": _MATH_PROMPT,
}


def solve_problem(problem: str, **config) -> str:
    """(openai<1) Solve the math problem.

    Args:
        problem (str): The problem statement.
        config (Optional, dict): The configuration for the API call.

    Returns:
        str: The solution to the problem.
    """
    params = {**_MATH_CONFIG, **config}
    response = oai.Completion.create({"problem": problem}, **params)
    results = eval_math_responses(oai.Completion.extract_text(response))
    return results.get("voted_answer"), response["cost"]


def remove_boxed(string: str) -> Optional[str]:
    """Source: https://github.com/hendrycks/math
    Extract the text within a \\boxed{...} environment.
    Example:

    > remove_boxed("\\boxed{\\frac{2}{3}}")

    \\frac{2}{3}
    """
    left = "\\boxed{"
    try:
        if not all((string[: len(left)] == left, string[-1] == "}")):
            raise AssertionError

        return string[len(left) : -1]
    except Exception:
        return None


def last_boxed_only_string(string: str) -> Optional[str]:
    """Source: https://github.com/hendrycks/math
    Extract the last \\boxed{...} or \\fbox{...} element from a string.
    """
    idx = string.rfind("\\boxed")
    if idx < 0:
        idx = string.rfind("\\fbox")
        if idx < 0:
            return None

    i = idx
    right_brace_idx = None
    num_left_braces_open = 0
    while i < len(string):
        if string[i] == "{":
            num_left_braces_open += 1
        if string[i] == "}":
            num_left_braces_open -= 1
            if num_left_braces_open == 0:
                right_brace_idx = i
                break
        i += 1

    if right_brace_idx is None:
        retval = None
    else:
        retval = string[idx : right_brace_idx + 1]

    return retval


def _fix_fracs(string: str) -> str:
    """Source: https://github.com/hendrycks/math
    Reformat fractions.
    Examples:
    >>> _fix_fracs("\\frac1b")
    \frac{1}{b}
    >>> _fix_fracs("\\frac12")
    \frac{1}{2}
    >>> _fix_fracs("\\frac1{72}")
    \frac{1}{72}
    """
    substrs = string.split("\\frac")
    new_str = substrs[0]
    if len(substrs) > 1:
        substrs = substrs[1:]
        for substr in substrs:
            new_str += "\\frac"
            if substr[0] == "{":
                new_str += substr
            else:
                try:
                    if not len(substr) >= 2:
                        raise AssertionError
                except Exception:
                    return string
                a = substr[0]
                b = substr[1]
                if b != "{":
                    if len(substr) > 2:
                        post_substr = substr[2:]
                        new_str += "{" + a + "}{" + b + "}" + post_substr
                    else:
                        new_str += "{" + a + "}{" + b + "}"
                else:
                    if len(substr) > 2:
                        post_substr = substr[2:]
                        new_str += "{" + a + "}" + b + post_substr
                    else:
                        new_str += "{" + a + "}" + b
    string = new_str
    return string


def _fix_a_slash_b(string: str) -> str:
    """Source: https://github.com/hendrycks/math
    Reformat fractions formatted as a/b to \\frac{a}{b}.
    Example:
    >>> _fix_a_slash_b("2/3")
    \frac{2}{3}
    """
    if len(string.split("/")) != 2:
        return string
    a_str = string.split("/")[0]
    b_str = string.split("/")[1]
    try:
        a = int(a_str)
        b = int(b_str)
        if not string == "{}/{}".format(a, b):
            raise AssertionError
        new_string = "\\frac{" + str(a) + "}{" + str(b) + "}"
        return new_string
    except Exception:
        return string


def _remove_right_units(string: str) -> str:
    """Source: https://github.com/hendrycks/math
    Remove units (on the right).
    "\\text{ " only ever occurs (at least in the val set) when describing units.
    """
    if "\\text{ " in string:
        splits = string.split("\\text{ ")
        if not len(splits) == 2:
            raise AssertionError
        return splits[0]
    else:
        return string


def _fix_sqrt(string: str) -> str:
    """Source: https://github.com/hendrycks/math
    Reformat square roots.
    Example:
    >>> _fix_sqrt("\\sqrt3")
    \\sqrt{3}
    """
    if "\\sqrt" not in string:
        return string
    splits = string.split("\\sqrt")
    new_string = splits[0]
    for split in splits[1:]:
        if split[0] != "{":
            a = split[0]
            new_substr = "\\sqrt{" + a + "}" + split[1:]
        else:
            new_substr = "\\sqrt" + split
        new_string += new_substr
    return new_string


def _strip_string(string: str) -> str:
    """Source: https://github.com/hendrycks/math
    Apply the reformatting helper functions above.
    """
    # linebreaks
    string = string.replace("\n", "")
    # print(string)

    # remove inverse spaces
    string = string.replace("\\!", "")
    # print(string)

    # replace \\ with \
    string = string.replace("\\\\", "\\")
    # print(string)

    # replace tfrac and dfrac with frac
    string = string.replace("tfrac", "frac")
    string = string.replace("dfrac", "frac")
    # print(string)

    # remove \left and \right
    string = string.replace("\\left", "")
    string = string.replace("\\right", "")
    # print(string)

    # Remove circ (degrees)
    string = string.replace("^{\\circ}", "")
    string = string.replace("^\\circ", "")

    # remove dollar signs
    string = string.replace("\\$", "")

    # remove units (on the right)
    string = _remove_right_units(string)

    # remove percentage
    string = string.replace("\\%", "")
    string = string.replace("%", "")

    # " 0." equivalent to " ." and "{0." equivalent to "{." Alternatively, add "0" if "." is the start of the string
    string = string.replace(" .", " 0.")
    string = string.replace("{.", "{0.")
    # if empty, return empty string
    if len(string) == 0:
        return string
    if string[0] == ".":
        string = "0" + string

    # to consider: get rid of e.g. "k = " or "q = " at beginning
    if len(string.split("=")) == 2:
        if len(string.split("=")[0]) <= 2:
            string = string.split("=")[1]

    # fix sqrt3 --> sqrt{3}
    string = _fix_sqrt(string)

    # remove spaces
    string = string.replace(" ", "")

    # \frac1b or \frac12 --> \frac{1}{b} and \frac{1}{2}, etc.
    # Even works with \frac1{72} (but not \frac{72}1).
    # Also does a/b --> \\frac{a}{b}
    string = _fix_fracs(string)

    # manually change 0.5 --> \frac{1}{2}
    if string == "0.5":
        string = "\\frac{1}{2}"

    # NOTE: X/Y changed to \frac{X}{Y} in dataset, but in simple cases fix in case the model output is X/Y
    string = _fix_a_slash_b(string)

    return string


def get_answer(solution: Optional[str]) -> Optional[str]:
    if solution is None:
        return None
    last_boxed = last_boxed_only_string(solution)
    if last_boxed is None:
        return None
    answer = remove_boxed(last_boxed)
    if answer is None:
        return None
    return answer


def is_equiv(str1: Optional[str], str2: Optional[str]) -> float:
    """Returns (as a float) whether two strings containing math are equivalent up to differences of formatting in
    - units
    - fractions
    - square roots
    - superfluous LaTeX.
    Source: https://github.com/hendrycks/math
    """
    if str1 is None and str2 is None:
        print("WARNING: Both None")
        return 1.0
    if str1 is None or str2 is None:
        return 0.0

    try:
        ss1 = _strip_string(str1)
        ss2 = _strip_string(str2)
        return float(ss1 == ss2)
    except Exception:
        return float(str1 == str2)


def is_equiv_chain_of_thought(str1: str, str2: str) -> float:
    """Strips the solution first before calling `is_equiv`."""
    ans1 = get_answer(str1)
    ans2 = get_answer(str2)

    return is_equiv(ans1, ans2)


def voting_counts(responses):
    answers = {}
    for i in range(len(responses)):
        equiv = i
        if get_answer(responses[i]) is None:
            # ignore None answers
            continue
        for j in answers:
            if is_equiv_chain_of_thought(responses[i], responses[j]):
                equiv = j
                break
        if equiv in answers:
            answers[equiv] += 1
        else:
            answers[equiv] = 1
    return answers


def eval_math_responses(responses, solution=None, **args):
    """Select a response for a math problem using voting, and check if the response is correct if the solution is provided.

    Args:
        responses (list): The list of responses.
        solution (str): The canonical solution.

    Returns:
        dict: The success metrics.
    """
    n = len(responses)
    if not n:
        return {
            "expected_success": 0,
            "success": False,
            "success_vote": 0,
            "voted_answer": None,
            "votes": 0,
        }
    success_list = []
    if solution is not None:
        for i in range(n):
            response = responses[i]
            succeed = is_equiv_chain_of_thought(response, solution)
            success_list.append(succeed)
    # voting
    answers = voting_counts(responses)
    # find the answer with highest votes in answers
    answer, votes = max(answers.items(), key=lambda x: x[1], default=(0, 0))
    # check if the answer is correct
    success_vote = is_equiv_chain_of_thought(responses[answer], solution)
    return {
        "expected_success": 1 - pow(1 - sum(success_list) / n, n),
        "success": any(s for s in success_list),
        "success_vote": success_vote,
        "voted_answer": responses[answer],
        "votes": votes,
    }


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/client/__init__.py
from pyjava import PythonContext,RayContext
from typing import Dict,Any,List,Optional,Union,Tuple,Callable,Annotated
from pyjava.udf import UDFBuilder
import ray
from ray.util.client.common import ClientActorHandle, ClientObjectRef
from byzerllm.utils.client import code_utils 
from byzerllm.utils import (function_calling_format,
                            response_class_format,
                            response_class_format_after_chat,
                            FunctionCallList,
                            function_impl_format,
                            base_ability_format,
                            sys_response_class_format,
                            sys_function_calling_format,
                            sys_function_impl_format,
                            exec_capture_output,
                            format_prompt,
                            format_prompt_jinja2
                            )
from byzerllm.utils.ray_utils import cancel_placement_group,get_actor_info
from langchain.prompts import PromptTemplate
import json
import dataclasses
import importlib  
import logging
import time
import asyncio
import functools
import inspect
import pydantic
import copy
import traceback
from enum import Enum


logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# create a enum for the role
class Role:
    User = "user"
    Assistant = "assistant"
    System = "system"

@dataclasses.dataclass
class LLMHistoryItem:
      role: str
      content: str

@dataclasses.dataclass
class LLMResponse:
    output: Union[str,List[float]]
    input: Union[str,Dict[str,Any]]
    metadata: Dict[str,Any] = dataclasses.field(default_factory=dict)


class LLMFunctionCallResponse(pydantic.BaseModel):
    response:LLMResponse
    values:List[Any]
    metadata:Dict[str,Any]


class LLMClassResponse(pydantic.BaseModel):
    response:LLMResponse
    value:Optional[Any]
    metadata:Dict[str,Any]

@dataclasses.dataclass
class LLMRequest:
    instruction: Union[str,List[str]]
    embedding: bool = False
    max_length: int = 4096
    top_p: float = 0.95
    temperature: float = 0.1    
        

@dataclasses.dataclass
class FintuneRequestExtra:
    max_seq_length: int = 1024
    num_train_epochs: int = 1
    logging_steps: int = 100
    save_steps: int = 100
    extra_params: Dict[str,Any] = dataclasses.field(default_factory=dict)

@dataclasses.dataclass
class  FintuneRequest:
    model_path: str
    pretrained_model_type: str
    input_data_path: str
    extra_params: FintuneRequestExtra = FintuneRequestExtra()


class InferBackend:
    Transformers = "transformers"
    VLLM = "ray/vllm"
    DeepSpeed = "ray/deepspeed"

@dataclasses.dataclass
class ExecuteCodeResponse:
      status: int
      output: str      
      code: str
      prompt: str
      variables: Dict[str,Any]=dataclasses.field(default_factory=dict)

class EventName(Enum):
    BEFORE_CALL_MODEL = "before_call_model"
    AFTER_CALL_MODEL = "after_call_model"

EventCallbackResult = Tuple[bool, Optional[Any]]
EventCallback = Callable[..., EventCallbackResult]    

class Template:
    def __init__(self,
                 role_mapping:Dict[str,str],
                 generation_config:Dict[str,Any],
                 clean_func:Callable[[str],str]=lambda s: s,
                 function_calling_format_func=function_calling_format,
                 response_class_format_func=response_class_format,
                 response_class_format_after_chat_func=response_class_format_after_chat
                 ) -> None:
        self.role_mapping = role_mapping
        self.generation_config = generation_config
        self.clean_func = clean_func        
        self.function_calling_format_func = function_calling_format_func
        self.response_class_format_func = response_class_format_func
        self.response_class_format_after_chat_func = response_class_format_after_chat_func


class Templates:

    def default_format(t,v):
        return f"{t}{v}"


    @staticmethod
    def qwen():
        def clean_func(v):            
            if "<|im_end|>" in v:
                v = v.split("<|im_end|>")[0]
            if "<|endoftext|>" in v:
                v = v.split("<|endoftext|>")[0] 
            if "<|im_start|>" in v:             
                v = v.split("<|im_start|>")[0]   
            return v   

        def sys_format(t,v):
            m = PromptTemplate.from_template(t)
            return m.format(system_msg=v)


        return Template(role_mapping={
                        "user_role":"<|im_start|>user\n",
                        "assistant_role": "<|im_end|>\n<|im_start|>assistant\n",
                        "system_msg":"<|im_start|>system\n{system_msg}<|im_end|>",
                        "system_msg_func":sys_format
                        },
                        generation_config={                            
                            "generation.repetition_penalty":1.1,
                            "generation.stop_token_ids":[151643,151645]},                  
                        clean_func=clean_func) 
    
    @staticmethod
    def llama():
        def sys_format(t,v):
            m = PromptTemplate.from_template(t)
            return m.format(system_msg=v)
        
        def user_format(t,v):
            return f"<s>[INST] {v} [/INST]"
        
        def assistant_format(t,v):
            return f" {v} </s>"
        
        return Template(
            role_mapping={
               "user_role":"",
               "assistant_role": "",
               "system_msg":"<s>[INST] <<SYS>>\n{system_msg}\n<</SYS>>\n[/INST]</s>",
               "system_msg_func":sys_format,
               "user_role_func": user_format,
               "assistant_role_func": assistant_format
            },            
            generation_config={},
            clean_func=lambda s: s
        )
    
    @staticmethod
    def deepseek_code_chat():
        '''
        DeepSeek Coder Chat mode template:

        ### Instruction:
        ['content']
        ### Response:
        ['content']
        <|EOT|>
        ### Instruction:
        ['content']
        ### Response:
        '''
        

        def sys_format(t:Annotated[str,"the field system_msg in role_mapping "],
                       v:Annotated[str,"the system message in chat"]):
            m = PromptTemplate.from_template(t)
            return m.format(system_msg=v)
        
        def user_format(t:Annotated[str,"the field user_role in role_mapping"],
                        v:Annotated[str,"the user message in chat"]):
            '''
            format single user message
            '''
            return f"### Instruction:\n{v}"
        
        def assistant_format(t:Annotated[str,"the field assistant_role in role_mapping"],
                             v:Annotated[str,"the assistant message in chat"]):
            '''
            format single assitant message.
            
            Notice that here we do not use `t` , because we will
            use the `t` as the final suffix.
            '''
            return f"### Response:\n{v}\n<|EOT|>"
        
        return Template(
            role_mapping={
               "user_role":"",
               "assistant_role": "### Response:\n",
               "system_msg":"{system_msg}",
               "system_msg_func":sys_format,
               "user_role_func": user_format,
               "assistant_role_func": assistant_format
            },            
            generation_config={"generation.stop_token_ids":[32021]},
            clean_func=lambda s: s
        )
    @staticmethod
    def deepseek_code_insertion():        
        def sys_format(t,v):
            if "<｜fim▁hole｜>" not in v:
                raise Exception("the system message should contains <｜fim▁hole｜>")
            m = PromptTemplate.from_template(t)
            return m.format(system_msg=v)
        
        def user_format(t,v):            
            return ""
        
        def assistant_format(t,v):            
            return ""
        
        return Template(
            role_mapping={
               "user_role":"",
               "assistant_role": "",
               "system_msg":"<｜fim▁begin｜>{system_msg}<｜fim▁end｜>",
               "system_msg_func":sys_format,
               "user_role_func": user_format,
               "assistant_role_func": assistant_format
            },            
            generation_config={},
            clean_func=lambda s: s
        )
    
    @staticmethod
    def deepseek_code_completion():        
        def sys_format(t,v):            
            m = PromptTemplate.from_template(t)
            return m.format(system_msg=v)
        
        def user_format(t,v):            
            return ""
        
        def assistant_format(t,v):            
            return ""
        
        return Template(
            role_mapping={
               "user_role":"",
               "assistant_role": "",
               "system_msg":"{system_msg}",
               "system_msg_func":sys_format,
               "user_role_func": user_format,
               "assistant_role_func": assistant_format
            },            
            generation_config={},
            clean_func=lambda s: s
        )
    @staticmethod
    def yi():
        def clean_func(v):                    
            return v   

        def sys_format(t,v):
            m = PromptTemplate.from_template(t)
            return m.format(system_msg=v)


        return Template(role_mapping={
                        "user_role":"<|im_start|>user\n",
                        "assistant_role": "<|im_end|>\n<|im_start|>assistant\n",
                        "system_msg":"<|im_start|>system\n{system_msg}<|im_end|>",
                        "system_msg_func":sys_format
                        },
                        generation_config={"generation.stop_token_ids":[7]},                  
                        clean_func=clean_func) 

    @staticmethod
    def default():
        def clean_func(v):                    
            return v   

        def sys_format(t,v):
            return v

        return Template(role_mapping={
                        "user_role":"User:",
                        "assistant_role": "Assistant:",
                        "system_msg":"You are a helpful assistant. Think it over and answer the user question correctly.",
                        "system_msg_func":sys_format
                        },
                        generation_configexecute_response_format={},                  
                        clean_func=clean_func)   

    @staticmethod
    def empty():
        def clean_func(v):                    
            return v   

        def sys_format(t,v):
            return v

        return Template(role_mapping={
                        "user_role":"",
                        "assistant_role": "",
                        "system_msg":"",
                        "system_msg_func":sys_format
                        },
                        generation_config={},                  
                        clean_func=clean_func)   

class ByzerLLM:
   
    def __init__(self,url:Optional[str]=None,**kwargs):
        self.url = url               
        self.default_sys_conf = {"pythonMode":"ray",
                         "maxConcurrency":1,
                         "num_gpus":1,
                         "masterMaxConcurrency":1000,
                         "workerMaxConcurrency":1,
                         "infer_backend":"transformers"
                         }
        self.sys_conf = self.default_sys_conf.copy()
        self.sql_model = "context" in globals()
        
        self.verbose = kwargs.get("verbose",False)
        
        self.force_skip_context_length_check = False
        if "force_skip_context_length_check" in kwargs:
            self.force_skip_context_length_check = kwargs["force_skip_context_length_check"]

        self.mapping_auto_use_apply_chat_template = {}
        
        self.mapping_max_input_length = {}
        self.mapping_max_output_length = {}
        self.mapping_max_model_length = {}        
        self.mapping_role_mapping = {}
        self.mapping_extra_generation_params = {}
        self.mapping_clean_func = {}
   
        self.mapping_function_calling_format_func = {}
        self.mapping_response_class_format_func = {}
        self.mapping_response_class_format_after_chat_func = {}
        self.mapping_impl_func_format_func = {}

        self.mapping_base_system_message = {}
        self.mapping_sys_response_class_format_func = {}
        self.mapping_sys_function_calling_format_func = {}
        self.mapping_sys_response_class_format_after_chat_func = {}
        self.mapping_sys_impl_func_format_func = {}

        
        self.func_impl_cache = {}
        self.meta_cache = {}

        self.byzer_engine_url = None
        if "byzer_engine_url" in kwargs:
            self.byzer_engine_url = kwargs["byzer_engine_url"]  

        self.default_max_output_length = 1024
        if "default_max_output_length" in kwargs:
            self.default_max_output_length = kwargs["default_max_output_length"]   
        

        self.default_model_name = None
        self.default_emb_model_name = None
        self.default_rerank_model_name = None
        self.default_role_mapping = {
                    "user_role":"User:",
                    "assistant_role": "Assistant:",
                    "system_msg":"You are a helpful assistant. Think it over and answer the user question correctly."
                    }
        
        self.pin_model_worker_mapping = None

        if url is not None and self.sql_model:            
            v = globals()
            self.context = v["context"]
            self.ray_context = RayContext.connect(v, self.url, **kwargs)
        else:
            self.context = PythonContext(
                0,[],self.sys_conf
            ) 
            self.context.have_fetched = True
            self.ray_context = self.context.rayContext 

        self.event_callbacks: Dict[EventName, List[EventCallback]] = {}  

    def add_event_callback(self, event_name: EventName, callback: EventCallback) -> None:
        self.event_callbacks.setdefault(event_name, []).append(callback)

    def _trigger_event(self, event_name: EventName, *args, **kwargs) -> Optional[Any]:
        if event_name in self.event_callbacks:
            for callback in self.event_callbacks[event_name]:
                continue_flag, value = callback(*args, **kwargs)
                if not continue_flag:
                    return value
        return None                         
        
    def setup_reset(self):
        self.sys_conf = self.default_sys_conf.copy()
        self.context.conf = self.sys_conf

    def setup_pin_model_worker_mapping(self,pin_model_worker_mapping:Dict[Any,int])->'ByzerLLM':
        self.pin_model_worker_mapping = pin_model_worker_mapping
        return self   

    def setup_load_balance_way(self,load_balance_way:str)->'ByzerLLM':
        self.sys_conf["load_balance"] = load_balance_way
        return self 

    def setup_default_model_name(self,model_name:str)->'ByzerLLM':
        self.default_model_name = model_name
        return self 

    def setup_default_emb_model_name(self,model_name:str)->'ByzerLLM':
        self.default_emb_model_name = model_name
        return self  

    def setup_default_re_rank_model_name(self,model_name:str)->'ByzerLLM':
        self.default_rerank_model_name = model_name
        return self  

    def setup(self,name:str, value:Any)->'ByzerLLM':
        self.sys_conf[name]=value
        # update the context conf
        self.context.conf = self.sys_conf
        return self

    def setup_function_calling_format_func(self,model:str,func)->'ByzerLLM':
        self.mapping_function_calling_format_func[model] = func
        return self

    def setup_response_class_format_func(self,model:str,func)->'ByzerLLM':
        self.mapping_response_class_format_func[model] = func
        return self
    
    def setup_impl_func_format_func(self,model:str,func)->'ByzerLLM':
        self.mapping_impl_func_format_func[model] = func
        return self

    def setup_response_class_format_after_chat_func(self,model:str,func)->'ByzerLLM':
        self.mapping_response_class_format_after_chat_func[model] = func
        return self  

    def setup_base_system_messages(self,model:str,base_system_message:str)->'ByzerLLM':
        self.mapping_base_system_message[model] = base_system_message
        return self 

    def setup_sys_response_class_format_func(self,model:str,func)->'ByzerLLM':
        self.mapping_sys_response_class_format_func[model] = func
        return self  

    def setup_sys_function_calling_format_func(self,model:str,func)->'ByzerLLM':
        self.mapping_sys_function_calling_format_func[model] = func
        return self

    def setup_sys_response_class_format_after_chat_func(self,model:str,func)->'ByzerLLM':
        self.mapping_sys_response_class_format_after_chat_func[model] = func
        return self

    def setup_sys_impl_func_format_func(self,model:str,func)->'ByzerLLM':
        self.mapping_sys_impl_func_format_func[model] = func
        return self   
    
    
    def setup_infer_backend(self,backend:str)->'ByzerLLM':
        self.sys_conf["infer_backend"] = backend
        
        if backend == InferBackend.VLLM or backend == InferBackend.DeepSpeed:            
            self.sys_conf["masterMaxConcurrency"] = 1000
            self.sys_conf["workerMaxConcurrency"] = 100
        
        if backend == InferBackend.Transformers:
            self.sys_conf["masterMaxConcurrency"] = 1000
            self.sys_conf["workerMaxConcurrency"] = 1

        return self
    
    def setup_gpus_per_worker(self,num_gpus:int)->'ByzerLLM':
        self.sys_conf["num_gpus"] = num_gpus
        return self
    
    def setup_cpus_per_worker(self,num_cpus:int)->'ByzerLLM':
        self.sys_conf["num_cpus"] = num_cpus
        return self
    
    def setup_worker_concurrency(self,num:int)->'ByzerLLM':        
        self.sys_conf["workerMaxConcurrency"] = num
        return self        

    def setup_num_workers(self,num_workers:int)->'ByzerLLM':
        self.sys_conf["maxConcurrency"] = num_workers
        return self
    
    def setup_max_model_length(self,model:str,max_model_length:int)->'ByzerLLM':
        self.mapping_max_model_length[model] = max_model_length
        return self
    
    def setup_max_input_length(self,model:str,max_input_length:int)->'ByzerLLM':
        self.mapping_max_input_length[model] = max_input_length
        return self
    
    def setup_max_output_length(self,model:str, max_output_length:int)->'ByzerLLM':
        self.mapping_max_output_length[model] = max_output_length
        return self
    
    def setup_role_mapping(self,model:str,role_mapping:Dict[str,str])->'ByzerLLM':
        self.mapping_role_mapping[model] = role_mapping
        return self
    
    def setup_extra_generation_params(self,model:str,extra_generation_params:Dict[str,Any])->'ByzerLLM':
        v = self.mapping_extra_generation_params.get(model,{}) 
        self.mapping_extra_generation_params[model] = {**v,**extra_generation_params}
        return self       
    
    def setup_template(self,model:str,template:Union[Template,str])->'ByzerLLM':
        if template == "auto":
            meta = self.get_meta(model=model)
            
            is_saas_model =  meta.get("model_deploy_type",None) == "saas"
            
            if is_saas_model:
                return self
            
            is_message_format = meta.get("message_format",False)
            
            if is_message_format:                
                return self
                        
            if "QWenLMHeadModel" in meta.get("architectures",[]):
                self.setup_template(model,Templates.qwen())
                return self

            if not meta.get("support_chat_template",False):
                raise Exception(f"The model({model}) is not support auto(apply chat template) for now.")
            
            self.mapping_auto_use_apply_chat_template[model] = True
            return self

        self.mapping_role_mapping[model] = template.role_mapping
        
        v = self.mapping_extra_generation_params.get(model,{}) 
        self.mapping_extra_generation_params[model] = {**v,**template.generation_config}

        self.mapping_clean_func[model] = template.clean_func
        self.mapping_function_calling_format_func[model] = template.function_calling_format_func
        self.mapping_response_class_format_after_chat_func[model] = template.response_class_format_after_chat_func
        self.mapping_response_class_format_func[model] = template.response_class_format_func
        return self
           

    def sft(self,sft_name:str,
            local_data_dir_path:str,
            local_model_path:str,
            local_stage_path:str,
            pretrained_model_type:str,            
            num_cpus:int,
            num_gpus:int,
            detached:bool=True,
            json_config:str="{}",
            model_params:Dict[str,Any]={},
            **kwargs
            ):
        '''
        finetune a pretrained model

        Args:
            sft_name (str): the uniq name of this finetune task
            local_data_dir_path (str): the local data dir path, which should contains `data.jsonl` file
            local_model_path (str): the local model path, which should contains `config.json` file
            local_stage_path (str): the local stage path which store the temp data and model
            pretrained_model_type (str): the pretrained model type, e.g. "sft/llama2","sft/baichuan"
            num_cpus (int): the number of cpus
            num_gpus (int): the number of gpus
            detached (bool, optional): whether to run this task in detached mode. Defaults to True.
            json_config (str, optional): the json config string. Defaults to "{}".
            model_params (Dict[str,Any], optional): the model params. Defaults to {}. The key should like this style `sft.int.logging_steps`, `sft.int.max_seq_length`
                                                    which contains the `sft` prefix and the type of the value.
        '''
        train_params = {}
        train_params["name"] = sft_name
        train_params["data_dir"] = local_data_dir_path
        train_params["localModelDir"] = local_model_path
        train_params["pretrainedModelType"] = pretrained_model_type
        train_params["config"] = json_config
        train_params["detached"] = "true" if detached else "false"
        train_params["localPathPrefix"] = local_stage_path
        
        for k,v in model_params.items():
            train_params[k] = v

        sys_conf = {}
        sys_conf["num_gpus"] = num_gpus
        sys_conf["num_cpus"] = num_cpus    

        r = self.raw_sft(train_params=train_params,sys_conf=sys_conf)
        if detached:
           return [i for i in r]
        return r
    
    def merge_lora(self,name:str,
                   local_model_path:str,
                   local_adpator_model_path:str,
                   local_target_path:str
                   ):
        train_params = {}
        train_params["name"] = name
        train_params["modelNameOrPath"] = local_model_path
        train_params["adapterNameOrPath"] = local_adpator_model_path
        train_params["savePath"] = local_target_path
        self.raw_merge_lora(train_params=train_params,sys_conf={})
        return local_target_path
    
    def pretrain(self,name:str,
            local_data_dir_path:str,
            local_model_path:str,
            local_stage_path:str,
            pretrained_model_type:str,            
            num_cpus:int,
            num_gpus:int,
            detached:bool=True,
            json_config:str="{}",
            model_params:Dict[str,Any]={},
            **kwargs):
        train_params = {}
        train_params["name"] = name
        train_params["localDataDir"] = local_data_dir_path
        train_params["localModelDir"] = local_model_path
        train_params["pretrainedModelType"] = pretrained_model_type
        train_params["deepspeedConfig"] = json_config
        train_params["detached"] = "true" if detached else "false"
        train_params["localPathPrefix"] = local_stage_path
        
        for k,v in model_params.items():
            train_params[k] = v

        sys_conf = {}
        sys_conf["num_gpus"] = num_gpus
        sys_conf["num_cpus"] = num_cpus    

        r = self.raw_pretrain(train_params=train_params,sys_conf=sys_conf)
        if detached:
           return [i for i in r]
        return r
    
    
    
    def raw_sft(self,train_params:Dict[str,Any],sys_conf:Dict[str,Any]={}):                   
        model_type = train_params["pretrainedModelType"] .split("/")[-1]              
        train_module =  importlib.import_module(f'byzerllm.{model_type}')
        return train_module.sft_train([],train_params,sys_conf)                
            

    def raw_pretrain(self,train_params:Dict[str,Any],sys_conf:Dict[str,Any]={}):                  
        model_type = train_params["pretrainedModelType"][-1]      
        train_module = importlib.import_module(f'byzerllm.{model_type}')        
        return train_module.sfft_train([],train_params,sys_conf)

    def raw_merge_lora(self,train_params:Dict[str,Any],sys_conf:Dict[str,Any]):                
        from byzerllm.utils.sft.merge_lora import merge_lora_to_base_model    
        merge_lora_to_base_model([],train_params,sys_conf) 

    def raw_deepspeed_to_huggingface(self,train_params:Dict[str,Any]):
        from byzerllm.utils.fulltune.pretrain.convert_to_transformers import convert
        convert(train_params,self.conf()) 

    def undeploy(self,udf_name:str):  
        import time                        
        try:
            model = ray.get_actor(udf_name)
            try:
                meta = self.get_meta(model=udf_name)
                if meta.get("backend","") == "ray/vllm":
                    if "engine_placement_group_id" in meta:
                        cancel_placement_group(meta["engine_placement_group_id"])
            except Exception as inst:
                pass
            ray.kill(model)  
            if udf_name in self.meta_cache:
                del self.meta_cache[udf_name]                          
        except ValueError:
            pass
        time.sleep(3)

    def generate_instruction_from_history(self,model:str,conversations:List[Dict[str,str]],role_mapping:Dict[str,str]={        
        "user_role":"User:",        
        "assistant_role":"Assistant:",
    }):                
        meta = self.get_meta(model=model)
        if self.mapping_auto_use_apply_chat_template.get(model,False) and meta.get("support_chat_template",False) :
            return self.apply_chat_template(model,json.dumps(conversations,ensure_ascii=False))

        new_his = []    
        for item in conversations:
            if item["role"] == "system":
                value = item["content"]
                if "system_msg_func" in role_mapping:
                    value = role_mapping["system_msg_func"](t=role_mapping["system_msg"],v=item["content"])
                new_his.append(value)
                continue
            
            if item["role"] == "user":
                value =  f"{role_mapping['user_role']}{item['content']}"
                if "user_role_func" in role_mapping:
                        value = role_mapping["user_role_func"](t=role_mapping["user_role"],v=item["content"])         
                new_his.append(value)  
            
            if item["role"] == "assistant":
                value =  f"{role_mapping['assistant_role']}{item['content']}"
                if "user_role_func" in role_mapping:
                        value = role_mapping["assistant_role_func"](t=role_mapping["assistant_role"],v=item["content"])         
                new_his.append(value)              
        
        if conversations[-1]["role"] == "user":            
            new_his.append(f"{role_mapping['assistant_role']}")

        fin_ins = "\n".join(new_his)
        return fin_ins     

    def is_model_exist(self,udf_name:str)->bool:
        try:
            ray.get_actor(udf_name)
            return True
        except Exception as inst:
            return False                           

    def deploy(self,model_path:str,
               pretrained_model_type:str,
               udf_name:str,
               infer_params:Dict[str,Any]):        
        from byzerllm import common_init_model
        self.setup("UDF_CLIENT",udf_name)

        infer_backend = self.sys_conf["infer_backend"]
        
        if infer_backend == InferBackend.VLLM or infer_backend == InferBackend.DeepSpeed:
            if pretrained_model_type != "custom/auto":
                raise ValueError(f"Backend({infer_backend}) is set. the pretrained_model_type should be `custom/auto`")

        model_type = pretrained_model_type
        
        if pretrained_model_type.startswith("saas/"):
            model_type = pretrained_model_type.split("/")[-1]                       
            
            infer_module = importlib.import_module(f'byzerllm.saas.{model_type}')
            from byzerllm.utils.text_generator import simple_predict_func
            
            def init_model(model_refs: List[ClientObjectRef], conf: Dict[str, str]) -> Any:
                from byzerllm import consume_model
                consume_model(conf)                
                infer = infer_module.CustomSaasAPI(infer_params)
                return (infer,None)
            
            UDFBuilder.build(self.ray_context,init_model,simple_predict_func)
            return self.get_meta(model=udf_name) 

        
        if pretrained_model_type == "bark":
            from byzerllm.bark.bark_voice import build_void_infer, ZH_SPEAKER, EN_SPEAKER            
            def init_model(model_refs: List[ClientObjectRef], conf: Dict[str, str]) -> Any:
                infer = build_void_infer(
                model_dir=model_path,
                tokenizer_dir=f"{model_path}/pretrained_tokenizer")
                return infer
            def predict_func(model,v):
                data = [json.loads(item) for item in v]
                results=[{"predict":model.text_to_voice(item["instruction"]).tolist(),"labels":""} for item in data]
                return {"value":[json.dumps(results,ensure_ascii=False,indent=4)]}
            UDFBuilder.build(self.ray_context,init_model,predict_func)
            return self.get_meta(model=udf_name)               
        
        # we put in this place so it only take effect for private model
        self.mapping_max_output_length[udf_name]=1024

        if pretrained_model_type.startswith("custom/"):
            model_type = pretrained_model_type.split("/")[-1]

        predict_func = "simple_predict_func"
        if model_type == "chatglm2":
            predict_func = "chatglm_predict_func"

        infer_module = importlib.import_module(f'byzerllm.{model_type}')
        predict_module = importlib.import_module(f"byzerllm.utils.text_generator")
        
        def init_model(model_refs: List[ClientObjectRef], conf: Dict[str, str]) -> Any:
            common_init_model(model_refs,conf,model_path, is_load_from_local=True)
            model = infer_module.init_model(model_path,infer_params,conf)
            return model
        
        UDFBuilder.build(self.ray_context,init_model,getattr(predict_module,predict_func))
        return self.get_meta(model=udf_name)
  
    def get_meta(self,model:str,llm_config:Dict[str,Any]={}):        
        if not model and not self.default_model_name:
            raise Exception("model name is required")
        
        if not model:
            model = self.default_model_name

        if model in self.meta_cache:
            return self.meta_cache[model]    

        default_config = self.mapping_extra_generation_params.get(model,{})

        v = [{"instruction":"","meta":True, **{**default_config,**llm_config} }]        
        res = self._query(model,v) 
        
        t = [LLMResponse(output=item["predict"],metadata=item.get("metadata",{}),input=item["input"]) for item in res]        
        
        res = {}
        if len(t) != 0 and len(t[0].output) != 0 :
            res = t[0].output[0]

        self.meta_cache[model] = res            
        return self.meta_cache[model]
        
    def tokenize(self,model:str,s:str,llm_config:Dict[str,Any]={})->List[str]:
        
        if not model and not self.default_model_name:
            raise Exception("model name is required")
        
        if not model:
            model = self.default_model_name

        default_config = self.mapping_extra_generation_params.get(model,{})

        v = [{"instruction":s,"tokenizer":True, **{**default_config,**llm_config} }]        
        res = self._query(model,v) 
        return [LLMResponse(output=item["predict"],metadata=item.get("metadata",{}),input=item["input"]) for item in res]
    
    def apply_chat_template(self,model:str,s:str,llm_config:Dict[str,Any]={}):
        if not model and not self.default_model_name:
            raise Exception("model name is required")
        
        if not model:
            model = self.default_model_name
        
        default_config = self.mapping_extra_generation_params.get(model,{})
        v = [{"instruction":s,"apply_chat_template":True, **{**default_config,**llm_config} }]        
        res = self._query(model,v) 
        
        t = [LLMResponse(output=item["predict"],metadata=item.get("metadata",{}),input=item["input"]) for item in res]  
        return t[0].output      

    def emb_query(self,v:str,model:str=None):
        return self.emb(model=model,request=LLMRequest(instruction=v))


    def emb(self, model, request:LLMRequest ,extract_params:Dict[str,Any]={}):
        
        if not model and not self.default_emb_model_name:
            raise Exception("model name is required")
        
        if not model:
            model = self.default_emb_model_name

        default_config = self.mapping_extra_generation_params.get(model,{})            

        if isinstance(request,list):
            request = LLMRequest(instruction=request)

        if isinstance(request.instruction,str):
            v = [{
            "instruction":request.instruction,
            "embedding":True,
            "max_length":request.max_length,
            "top_p":request.top_p,
            "temperature":request.temperature,                                    
            ** default_config,           
            ** extract_params}] 
        else: 
            v = [{
            "instruction":x,
            "embedding":True,
            "max_length":request.max_length,
            "top_p":request.top_p,
            "temperature":request.temperature,            
            ** default_config, 
            ** extract_params} for x in request.instruction]    
        res = self._query(model,v) 
      
        return [LLMResponse(output=item["predict"],metadata=item.get("metadata",{}),input=item["input"]) for item in res]

    def emb_rerank(self, model: str = None, sentence_pairs: Union[List[Tuple[str, str]], Tuple[str, str]] = [],
                   extract_params: Dict[str, Any] = {}) -> Union[Tuple[Tuple[str, str], float], List[Tuple[Tuple[str, str], float]]]:

        if not model and not self.default_rerank_model_name:
            raise Exception("rerank model name is required")

        if not sentence_pairs or len(sentence_pairs) == 0:
            raise Exception("rerank rerank param sentence_pairs is required")

        if not model:
            model = self.default_rerank_model_name

        default_config = self.mapping_extra_generation_params.get(model, {})

        v = [{
            "instruction": sentence_pairs,
            "embedding": True,
            "embed_rerank": True,
            **default_config,
            **extract_params}]
        res = self._query(model, v)

        return [LLMResponse(output=item["predict"], metadata=item.get("metadata", {}), input=item["input"]) for item in
                res]

    def _generate_ins(self,model:str,request:LLMRequest,role_mapping:Dict[str,str]):
         if not role_mapping["user_role"]:
             return request.instruction
         
         sys_msg = role_mapping["system_msg"]
         if "system_msg_func" in role_mapping:
             sys_msg = "You are a helpful assistant. Think it over and answer the user question correctly."
         
         conversations = [{"role":"system","content":sys_msg}]
         # conversations += [{"role":item.role,"content":item.content} for item in request.extra_params.history]
         
         conversations += self._to_openai_format(request=request)
         
         final_ins = self.generate_instruction_from_history(model,conversations,role_mapping)                      
             
         return final_ins
    
    def _to_openai_format(self,request:LLMRequest): 
        conversations = []
        if isinstance(request.instruction,str):       
            conversations += [{
                        "role":"user",
                        "content":request.instruction
                    }]
        else:
            conversations += [{
                        "role":"user",
                        "content":x
                    } for x in request.instruction]    
        return conversations

    def execute_function_calling(self,response:LLMResponse,tools:List[Callable],func_params:Dict[str,Any])-> LLMFunctionCallResponse:            
        
        r = LLMFunctionCallResponse(response=response,values=[],metadata={"reason":""})
        
        is_json = False
        try:
            json.loads(response.output)
            is_json = True
        except Exception as inst:
            pass

        code = response.output
            
        if not is_json:
            if code.strip().startswith("```json"): 
                index = code.rfind("```")
                if index != -1:
                    code = code.strip()[7:index-1]                    
                    try:
                        json.loads(code)
                        is_json = True
                    except Exception as inst:
                        pass
                
        if not is_json:
            codes = code_utils.extract_code(response.output)         
            if len(codes) == 0:            
                r.metadata["reason"] = "No json block found"
                return r 
            
            lang,code = codes[-1]

            if lang != "json":
                r.metadata["reason"] = "No json block found"
                return r
        
        try:
            temp = json.loads(code)
            if isinstance(temp,list):
                temp = temp[-1]
            ms = FunctionCallList.parse_obj(temp)
        except Exception as inst:
            r.metadata["reason"] = str(inst) + "\n" + traceback.format_exc()
            return r
                    
        _func_maps = dict([(t.__name__,t) for t in tools])

        if func_params is None:
            func_params = {}
        
        try:
            r.metadata["selected_functions"] = []
            for m in ms.tool_calls:        
                if m.function.name in _func_maps:
                    r.metadata["selected_functions"].append(m.function.name)
                    r.values.append(_func_maps[m.function.name](**m.function.arguments,**func_params))
        except Exception as inst:
            r.metadata["reason"] = str(inst) + "\n" + traceback.format_exc()            

        return r
    
    def execute_generate_func(self,                              
                              func_name:str,
                              impl_func_params:Optional[Dict[str,Any]],
                              response:LLMResponse,
                              response_class:pydantic.BaseModel)-> LLMClassResponse:
        
        
        r = LLMClassResponse(response=response,value=None,metadata={"reason":""})

        is_python_code = False
        if code_utils.infer_lang(response.output) == "python":
            is_python_code = True
        
        code = response.output

        if not is_python_code:
            codes = code_utils.extract_code(response.output)
            
            if len(codes) == 0:
                r.metadata["reason"] = "No Python block found"
                return r 
            
            lang,code = codes[-1]

            if lang != "python":
                r.metadata["reason"] = "No Python block found"
                return r
                
        (status,output,variables) = exec_capture_output(code,{func_name:True})
        if status != 0:
            r.metadata["reason"] = output
            return r
        
        try:
            if impl_func_params is None:
                impl_func_params = {}
            res_json = variables[func_name](**impl_func_params)
            r.metadata["raw_func"] = code
            r.metadata["func"]  = variables[func_name]            
            if isinstance(res_json,str):
                res_json = json.loads(res_json)
            r.value=response_class.parse_obj(res_json)
        except Exception as inst:
            r.metadata["reason"] = str(inst) + "\n" + traceback.format_exc()
            return r                                                       

        return r
    
    def execute_response_format(self,response:LLMResponse,response_class:pydantic.BaseModel):
        
        
        r = LLMClassResponse(response=response,value=None,metadata={"reason":""})
        is_json = False
        try:
            json.loads(response.output)
            is_json = True
        except Exception as inst:
            pass
                
        code = response.output

        if not is_json:
            if code.strip().startswith("```json"): 
                index = code.rfind("```")
                if index != -1:
                    code = code.strip()[7:index-1]                    
                    try:
                        json.loads(code)
                        is_json = True
                    except Exception as inst:
                        pass

        if not is_json:
            codes = code_utils.extract_code(response.output)
            if len(codes) == 0:
                r.metadata["reason"] = "No json block found"
                return r 
            
            lang,code = codes[-1]

            if lang != "json":
                r.metadata["reason"] = "No json block found"
                return r
        
        try:
            ms = response_class.parse_obj(json.loads(code))            
        except Exception as inst:
            r.metadata["reason"] = str(inst) + "\n" + traceback.format_exc()
            return r                                       
        
        r.value=ms

        return r

    def abort(self,request_id:str,model:Optional[str]=None):
        if not model and not self.default_model_name:
            raise Exception("model name is required")
        if not model:
            model = self.default_model_name
        
        meta = self.get_meta(model=model)
        if meta.get("backend",None) != "ray/vllm":
            raise Exception("abort only support ray/vllm backend")
        
        self.chat_oai(conversations=[
            {
                "role":"user",
                "content":f"{request_id}"
            }
        ],llm_config={"gen.request_id":request_id,"gen.abort":True})    

    def chat_oai(self,
                 conversations,
                 tools:List[Union[Callable,str]]=[], 
                 tool_choice:Optional[Union[Callable,str]]=None,
                 execute_tool:bool=False,  
                 impl_func:Optional[Callable]=None,
                 execute_impl_func:bool=False,
                 impl_func_params:Optional[Dict[str,Any]]=None,
                 func_params:Optional[Dict[str,Any]]=None,
                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, 
                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,
                 enable_default_sys_message:bool=False,                 
                 model:Optional[str] = None,
                 role_mapping=None,llm_config:Dict[str,Any]={}
                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]:        
        
        if not self.default_model_name and not model:
            raise Exception("Use llm.setup_default_model_name to setup default model name or setup the model parameter")
        
        if not model:
            model = self.default_model_name
            
        if role_mapping is None:
            role_mapping = self.mapping_role_mapping.get(model, self.default_role_mapping)
        
        if response_class and (tools or tool_choice):
            raise Exception("function calling is enabled,response_class should not be set.")
        
        if impl_func and not response_class:
            raise Exception("impl_func is enabled,response_class should be set.")
        

        if enable_default_sys_message:
            first_message = conversations[0]
            if first_message["role"] == "user":
                conversations.insert(0,{
                    "role":"system",
                    "content": self.mapping_base_system_message.get(model,base_ability_format())
                })
            if first_message["role"] == "system":
                first_message["content"] = f'''{self.mapping_base_system_message.get(model,base_ability_format())}
{first_message["content"]}'''
                
        meta = self.get_meta(model=model)        
        is_saas_model =  meta.get("model_deploy_type",None) == "saas"
        is_message_format = meta.get("message_format",False)

        temp_conversations = copy.deepcopy(conversations)
        last_message = temp_conversations[-1]
        
        # function calling
        if tools or tool_choice:
            f = self.mapping_function_calling_format_func.get(model,function_calling_format) if not enable_default_sys_message else self.mapping_sys_function_calling_format_func.get(model,sys_function_calling_format)
            last_message["content"] = f(last_message["content"],tools,tool_choice)

        # implement function and the function should return a response class
        elif impl_func and response_class:
            f = self.mapping_impl_func_format_func.get(model,function_impl_format) if not enable_default_sys_message else self.mapping_sys_impl_func_format_func.get(model,sys_function_impl_format)    
            last_message["content"] = f(last_message["content"],impl_func,cls = response_class) 

        # generate response class 
        elif response_class and not response_after_chat:
            f = self.mapping_response_class_format_func.get(model,response_class_format) if not enable_default_sys_message else self.mapping_sys_response_class_format_func.get(model,sys_response_class_format) 
            last_message["content"] = f(last_message["content"],cls = response_class)
                           
        
        if is_saas_model or is_message_format:
            final_ins = last_message["content"]
            history = []
            for item in temp_conversations[:-1]:
                # clean metadata field in conversation 
                # which may used by agent.
                if "metadata" in item:
                    del item["metadata"]
                history.append(item)
            
        else:
            final_ins = self.generate_instruction_from_history(model,temp_conversations, role_mapping)         
            history = []

        default_config = self.mapping_extra_generation_params.get(model,{})
        v = [{"instruction":final_ins,"history":history,**default_config,**llm_config }]         
        res = self._query(model,v) 
        clean_func = self.mapping_clean_func.get(model,lambda s: s)        
        responses = [LLMResponse(output=clean_func(item["predict"]),metadata=item.get("metadata",{}),input=item["input"]) for item in res]        
        
        ## handle impl_func response
        if impl_func and response_class and execute_impl_func:
            final_result = []
            for response in responses:
                final_result.append(self.execute_generate_func(
                    func_name=impl_func.__name__,
                    impl_func_params=impl_func_params or func_params,
                    response=response,
                    response_class=response_class))
            return final_result
        
        if impl_func and response_class:
            return responses

        ## handle response_class response 
        temp_result = responses    
        if response_class and response_after_chat: 
            temp_result = []
            f = self.mapping_response_class_format_after_chat_func.get(model,response_class_format_after_chat)
            for response in responses:
                new_conversations = temp_conversations + [{
                                        "content":response.output,
                                        "role":"assistant"
                                    },{
                                        "content":f(response_class),
                                        "role":"user"
                                    }]
                temp_result.append(self.chat_oai(new_conversations,role_mapping=role_mapping,llm_config=llm_config)[0])            

        if response_class:
            final_result = []
            for response in temp_result:
                final_result.append(self.execute_response_format(response=response,response_class=response_class))
            return final_result    
                             
        ## handle function calling response
        if execute_tool:
            final_result = []
            for response in responses:
                final_result.append(self.execute_function_calling(response=response,tools=tools,func_params=func_params))

            return final_result
        
        return responses    
        
    def stream_chat_oai(self,conversations, model:Optional[str]=None, role_mapping=None,llm_config:Dict[str,Any]={}): 
        
        if not model:
            model = self.default_model_name

        meta = self.get_meta(model=model)
        if not meta.get("support_stream",False):
            raise Exception(f"The model({model}) is not support stream chat for now.")

        v = self.chat_oai(conversations,model=model,role_mapping = role_mapping,llm_config={**llm_config,**{"generation.stream":True}})       
        request_id = v[0].metadata["request_id"]
        stream_server = v[0].metadata.get("stream_server","VLLM_STREAM_SERVER")
        server = ray.get_actor(stream_server)                        

        pre_generated_text = None
        while True:                 
            final_output = ray.get(server.get_item.remote(request_id))
            if isinstance(final_output,str):
                time.sleep(0.01)
                continue
            
            if final_output is None:
                break
            
            text_outputs = final_output.outputs
            clean_func = self.mapping_clean_func.get(model,lambda s: s)
            generated_text = text_outputs[0].text
            if pre_generated_text is not None and generated_text == pre_generated_text:
                continue
            pre_generated_text=generated_text
            yield (clean_func(generated_text),text_outputs[0].metadata)

    async def async_stream_chat_oai(self,conversations,role_mapping=None,model:Optional[str]=None,llm_config:Dict[str,Any]={}): 
        
        if not model:
            model = self.default_model_name
        
        meta = self.get_meta(model=model)
        if not meta.get("support_stream",False):
            raise Exception(f"The model({model}) is not support stream chat for now.")    

        v = self.chat_oai(conversations,model=model,role_mapping=role_mapping,llm_config={**llm_config,**{"generation.stream":True}})       
        request_id = v[0].metadata["request_id"]
        stream_server = v[0].metadata.get("stream_server","VLLM_STREAM_SERVER")
        server = ray.get_actor(stream_server)

        pre_generated_text = None
        while True:                 
            final_output = await server.get_item.remote(request_id)
            if isinstance(final_output,str):
                time.sleep(0.01)
                continue
            
            if final_output is None:
                break
            
            text_outputs = [output for output in final_output.outputs]
            clean_func = self.mapping_clean_func.get(model,lambda s: s)
            generated_text = text_outputs[0].text
            if pre_generated_text is not None and generated_text == pre_generated_text:
                continue
            pre_generated_text=generated_text
            yield (clean_func(generated_text),text_outputs[0].metadata)   

    def clear_impl_cache(self,model:Optional[str]=None,
                         full_func_name:Optional[str]=None,
                         instruction:Optional[str]=None):
        if model is None and full_func_name is None and instruction is None:
            self.func_impl_cache = {}          
        
        if model is not None and full_func_name is not None and instruction is None:
            raise Exception("instruction is required")
        

        if model is not None:
            instruction = "" if not instruction else instruction
            full_func_name = "" if not full_func_name else full_func_name

            key = f"{model}_{instruction}_{full_func_name}"
            for k in list(self.func_impl_cache.keys()):
                if k.startswith(key):
                    del self.func_impl_cache[k]
            return self        
        
        if full_func_name is not None:            
            instruction = "" if not instruction else instruction
            model = "" if not model else model
            key = f"{model}_{instruction}_{full_func_name}"
            for k in list(self.func_impl_cache.keys()):                
                if k.endswith(key):
                    del self.func_impl_cache[k]
            return self        

    def prompt(self,model:Optional[str]=None,render:Optional[str]="simple",print_prompt:bool=False):              
            if model is None:
                model = self.default_model_name            

            def _impl(func):               
                @functools.wraps(func)
                def wrapper(*args, **kwargs):                                                                                                   
                    signature = inspect.signature(func)
                    arguments = signature.bind(*args, **kwargs)
                    arguments.apply_defaults()
                    input_dict = {}
                    for param in signature.parameters:
                        input_dict.update({ param: arguments.arguments[param] })
                    
                    if "self" in input_dict:
                        _ = input_dict.pop("self") 
                    
                    if render == "jinja2" or render == "jinja":                  
                        prompt_str = format_prompt_jinja2(func,**input_dict)
                    else:
                        prompt_str = format_prompt(func,**input_dict)
                                        
                    if issubclass(signature.return_annotation,pydantic.BaseModel):
                        response_class = signature.return_annotation                    
                        t = self.chat_oai(model=model,conversations=[{
                            "role":"user",
                            "content":prompt_str
                        }], 
                            response_class=response_class,                     
                            impl_func_params=input_dict)                    
                        r:LLMClassResponse = t[0]                        
                        return r.value
                    elif issubclass(signature.return_annotation,str):
                        t = self.chat_oai(model=model,conversations=[{
                            "role":"user",
                            "content":prompt_str
                        }])
                        return t[0].output
                    else:
                        raise Exception("impl function should return a pydantic model or string")
                return wrapper      
            return _impl
    
    def response(self,instruction:Optional[str]=None,
                      model:Optional[str]=None,
                      verbose:Optional[bool]=None):  
        if model is None:
            model = self.default_model_name
        if instruction is None:
            instruction = ""  
        
        if verbose is None:
            verbose = self.verbose            

        def _impl(func):               
            @functools.wraps(func)
            def wrapper(*args, **kwargs):                                                                               
                signature = inspect.signature(func)
                arguments = signature.bind(*args, **kwargs)
                arguments.apply_defaults()
                input_dict = {}
                for param in signature.parameters:
                    input_dict.update({ param: arguments.arguments[param] })
                
                if len(input_dict.keys()) != 1:
                    raise Exception("response function should have only one parameter which type should be string")

                if issubclass(signature.return_annotation,pydantic.BaseModel):
                    response_class = signature.return_annotation
                else:
                    raise Exception("impl function should return a pydantic model")
                
                start_time = time.monotonic()

                t = self.chat_oai(model=model,conversations=[{
                    "role":"user",
                    "content":list(input_dict.values())[0]
                }], 
                    response_class=response_class,                     
                    impl_func_params=input_dict)
                
                r:LLMClassResponse = t[0]                
                
                if verbose:
                    print(f'''cost {time.monotonic() - start_time} seconds''',flush=True)                
                
                return r.value

            return wrapper      
        return _impl            
    
    def impl(self,
             instruction:Optional[str]=None,
             model:Optional[str]=None,
             verbose:Optional[bool]=None,
             skip_cache:bool=False): 
        if model is None:
            model = self.default_model_name
        if instruction is None:
            instruction = ""  
        
        if verbose is None:
            verbose = self.verbose            

        def _impl(func):               
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                                                
                key = f"{model}_{instruction}_{func.__module__}.{func.__name__}"
                signature = inspect.signature(func)
                arguments = signature.bind(*args, **kwargs)
                arguments.apply_defaults()
                
                if issubclass(signature.return_annotation,pydantic.BaseModel):
                    response_class = signature.return_annotation
                else:
                    raise Exception("impl function should return a pydantic model")
                
                if not skip_cache and key in self.func_impl_cache:
                    if verbose:
                        print(f''' {key} in cache, skip impl function''')
                    return response_class.parse_obj(self.func_impl_cache[key](*args, **kwargs))
                
                
                input_dict = {}
                for param in signature.parameters:
                    input_dict.update({ param: arguments.arguments[param] })
                                                
                start_time = time.monotonic()

                t = self.chat_oai(model=model,conversations=[{
                    "role":"user",
                    "content":instruction
                }], impl_func=func,
                    response_class=response_class, 
                    execute_impl_func=True, 
                    impl_func_params=input_dict)
                
                r:LLMClassResponse = t[0]                
                
                if verbose:
                    print(f'''Generate code for {key}: 
```python
{r.metadata["raw_func"]}
``` 
cost {time.monotonic() - start_time} seconds                     
''',flush=True)

                if not skip_cache and key not in self.func_impl_cache:
                    self.func_impl_cache[key] = r.metadata["func"]
                
                return r.value

            return wrapper      
        return _impl  

    
    def raw_chat(self,model,request:Union[LLMRequest,str],extract_params:Dict[str,Any]={})->List[LLMResponse]:
        if isinstance(request,str): 
            request = LLMRequest(instruction=request)

        return self.chat(model,request,extract_params)

    def chat(self,model,request:Union[LLMRequest,str],extract_params:Dict[str,Any]={})->List[LLMResponse]:
        if not model and not self.default_model_name:
            raise Exception("model name is required")
        
        if not model:
            model = self.default_model_name

        default_config = self.mapping_extra_generation_params.get(model,{})  
        
        default_role_mapping = self.mapping_role_mapping.get(model, self.default_role_mapping)  
        
        if isinstance(request,str): 
            request = LLMRequest(instruction=request)

        if isinstance(request.instruction,str):
            
            final_input = self._generate_ins(model,request,default_role_mapping)                         
            
            v = [{
            "instruction":final_input,
            "max_length":request.max_length,
            "top_p":request.top_p,
            "temperature":request.temperature,                       
             **default_config,**extract_params
             }] 
        else: 
            v = []
            for x in request.instruction:
                
                new_request = LLMRequest(instruction=x,
                                         embedding=request.embedding,max_length=request.max_length,top_p=request.top_p,
                                         temperature=request.temperature,
                                         )
                               
                final_input = self._generate_ins(model,new_request,default_role_mapping)                                    
                
                v.append({
                "instruction":final_input, 
                "max_length":request.max_length,
                "top_p":request.top_p,
                "temperature":request.temperature, 
                **default_config,          
                **extract_params
                })
        res = self._query(model,v) 
        clean_func = self.mapping_clean_func.get(model,lambda s: s)
        return [LLMResponse(output=clean_func(item["predict"]),metadata=item.get("metadata",{}),input=item["input"]) for item in res]
    
    def apply_sql_func(self,sql:str,data:List[Dict[str,Any]],owner:str="admin",url:str="http://127.0.0.1:9003/model/predict"):
        if self.byzer_engine_url and url == "http://127.0.0.1:9003/model/predict":
            url = self.byzer_engine_url
        res = self._rest_byzer_engine(sql,data,owner,url)
        return res
    
    def _rest_byzer_script(self, sql:str,owner:str,url:str="http://127.0.0.1:9003/run/script"):
        import requests
        import json        
        data = {
                'sessionPerUser': 'true',
                'sessionPerRequest': 'true',
                'owner': owner,                
                'sql': sql,
                "includeSchema":True               
            }
        response = requests.post(url, data=data)
        
        if response.status_code != 200:
            raise Exception(f"{self.url} status:{response.status_code} content: {response.text} request: json/{json.dumps(data,ensure_ascii=False)}")
        res = json.loads(response.text)        
        return res

                   
    def _rest_byzer_engine(self, sql:str,table:List[Dict[str,Any]],owner:str,url:str):
        import requests
        import json        
        data = {
                'sessionPerUser': 'true',
                'sessionPerRequest': 'true',
                'owner': owner,
                'dataType': 'row',
                'sql': sql,
                'data': json.dumps(table,ensure_ascii=False)
            }
        response = requests.post(url, data=data)
        
        if response.status_code != 200:
            raise Exception(f"{self.url} status:{response.status_code} content: {response.text} request: json/{json.dumps(data,ensure_ascii=False)}")
        res = json.loads(response.text)        
        return res[0]

    def get_max_model_length(self,model:str):
        return self.mapping_max_model_length.get(model,None)

    def get_max_output_length(self,model:str):
        return self.mapping_max_output_length.get(model,self.default_max_output_length)

    def get_max_input_length(self,model:str):
        return self.mapping_max_input_length.get(model,None)        

    def _query(self, model:str, input_value:List[Dict[str,Any]]):  
        
        if not self.force_skip_context_length_check:
            for input in input_value:
                # if this is a embedding/tokenizer query ,skip            
                if input.get("embedding",False) or input.get("tokenizer",False):
                    continue            
                
                final_ins = input.get("instruction","")
                
                try:
                    input_size = len(self.tokenize(None,final_ins,{})[0].output[0])
                except Exception as inst:                
                    continue
                
                if self.get_max_input_length(model) and input_size > self.get_max_input_length(model):
                    raise Exception(f"input length {input_size} is larger than max_input_length {self.mapping_max_input_length[model]}")                
                
                max_output_length = self.get_max_output_length(model)

                if  self.get_max_model_length(model):                    
                    if input_size + max_output_length > self.get_max_model_length(model):
                        raise Exception(f"input_size ({input_size}) + max_output_length {max_output_length} is larget than model context length {self.mapping_max_model_length[model]}")                
                
                # dynamically update the max_length
                input["max_length"] = input_size + max_output_length

        event_result = self._trigger_event(EventName.BEFORE_CALL_MODEL, self, model, input_value)        
        if event_result is not None:            
            return event_result
        
        udf_master = ray.get_actor(model)     
        
        try:   
            new_input_value = [json.dumps(x,ensure_ascii=False) for x in input_value]
        except Exception as inst:
            raise Exception(f"input_value should be json serializable, got {input_value}") 
           
        if self.verbose:
            print(f"Send to model[{model}]:{new_input_value}")
        index = -1 
        try:    
            worker_id = -1  
            if self.pin_model_worker_mapping:
                if input_value[0].get("embedding",False):
                    worker_id = self.pin_model_worker_mapping.get("embedding",-1)
                elif input_value[0].get("tokenizer",False):
                    worker_id = self.pin_model_worker_mapping.get("tokenizer",-1)
                elif input_value[0].get("apply_chat_template",False):
                    worker_id = self.pin_model_worker_mapping.get("apply_chat_template",-1)
                elif input_value[0].get("meta",False):
                    worker_id = self.pin_model_worker_mapping.get("meta",-1)                  

            [index, worker] = ray.get(udf_master.get.remote(worker_id))                        
            res = ray.get(worker.async_apply.remote(new_input_value))
            
            event_result = self._trigger_event(EventName.AFTER_CALL_MODEL, model, json.loads(res["value"][0]))
            if event_result is not None:
                return event_result
                                                
            return json.loads(res["value"][0])
        except Exception as inst:
            raise inst
        finally:
            if index != -1:
                ray.get(udf_master.give_back.remote(index)) 

def default_chat_wrapper(llm:"ByzerLLM",conversations: Optional[List[Dict]] = None,llm_config={}):
    return llm.chat_oai(conversations=conversations,llm_config=llm_config)




##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/client/code_utils.py
import logging
import os
import pathlib
import re
import subprocess
import sys
import time
import json
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from hashlib import md5
from typing import Callable, Dict, List, Optional, Tuple, Union
try:
    import docker
except ImportError:
    docker = None



# Regular expression for finding a code block
DEFAULT_MODEL="chat"
CODE_BLOCK_PATTERN = r"```[ \t]*(\w+)?[ \t]*\r?\n(.*?)\r?\n[ \t]*```"
WORKING_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), "extensions")
UNKNOWN = "unknown"
TIMEOUT_MSG = "Timeout"
DEFAULT_TIMEOUT = 600
WIN32 = sys.platform == "win32"
PATH_SEPARATOR = WIN32 and "\\" or "/"

logger = logging.getLogger(__name__)


def get_value_from_llm_str(v:str,k:str, default_value)->Union[str,int,float,bool,None]:
    responses = extract_code(v)
    value = default_value
    for lang,code in responses:
        if lang == "json":
            try:
                value = json.loads(code)[k]
            except Exception as inst:
                pass 
    return value

def content_str(content: Union[str, List, None]) -> str:
    """Converts `content` into a string format.

    This function processes content that may be a string, a list of mixed text and image URLs, or None,
    and converts it into a string. Text is directly appended to the result string, while image URLs are
    represented by a placeholder image token. If the content is None, an empty string is returned.

    Args:
        - content (Union[str, List, None]): The content to be processed. Can be a string, a list of dictionaries
                                      representing text and image URLs, or None.

    Returns:
        str: A string representation of the input content. Image URLs are replaced with an image token.

    Note:
    - The function expects each dictionary in the list to have a "type" key that is either "text" or "image_url".
      For "text" type, the "text" key's value is appended to the result. For "image_url", an image token is appended.
    - This function is useful for handling content that may include both text and image references, especially
      in contexts where images need to be represented as placeholders.
    """
    if content is None:
        return ""
    if isinstance(content, str):
        return content
    if not isinstance(content, list):
        raise TypeError(f"content must be None, str, or list, but got {type(content)}")

    rst = ""
    for item in content:
        if not isinstance(item, dict):
            raise TypeError("Wrong content format: every element should be dict if the content is a list.")
        assert "type" in item, "Wrong content format. Missing 'type' key in content's dict."
        if item["type"] == "text":
            rst += item["text"]
        elif item["type"] == "image_url":
            rst += "<image>"
        else:
            raise ValueError(f"Wrong content format: unknown type {item['type']} within the content")
    return rst

def infer_lang(code):
    """infer the language for the code.
    TODO: make it robust.
    """
    if code.startswith("python ") or code.startswith("pip") or code.startswith("python3 "):
        return "sh"

    # check if code is a valid python code
    try:
        compile(code, "test", "exec")
        return "python"
    except SyntaxError:
        # not a valid python code
        return UNKNOWN


def check_target_codes_exists(codes: List[Tuple[str, str]], langs: List[str]) -> bool:
    """Check if there is code in a specific language in the code list.

    Args:
        codes (list): The list of code blocks.
        langs (list): The language to check.

    Returns:
        bool: True if there is code in the specified language; False otherwise.
    """
    for l, _ in codes:
        if l in langs:
            return True
    return False

def get_target_codes(codes: List[Tuple[str, str]], langs: List[str]) -> List[str]:
    """Get code in a specific language from the code list.

    Args:
        codes (list): The list of code blocks.
        langs (list): The language to check.

    Returns:
        str: The code in the specified language.
    """
    target_codes = []
    for l, code in codes:
        if l in langs:
            target_codes.append(code)
    return target_codes


def extract_code(
    text: Union[str, List], pattern: str = CODE_BLOCK_PATTERN, detect_single_line_code: bool = False
) -> List[Tuple[str, str]]:
    """Extract code from a text.

    Args:
        text (str or List): The content to extract code from. The content can be
            a string or a list, as returned by standard GPT or multimodal GPT.
        pattern (str, optional): The regular expression pattern for finding the
            code block. Defaults to CODE_BLOCK_PATTERN.
        detect_single_line_code (bool, optional): Enable the new feature for
            extracting single line code. Defaults to False.

    Returns:
        list: A list of tuples, each containing the language and the code.
          If there is no code block in the input text, the language would be "unknown".
          If there is code block but the language is not specified, the language would be "".
    """
    text = content_str(text)
    if not detect_single_line_code:
        match = re.findall(pattern, text, flags=re.DOTALL)
        return match if match else [(UNKNOWN, text)]

    # Extract both multi-line and single-line code block, separated by the | operator
    # `([^`]+)`: Matches inline code.
    code_pattern = re.compile(CODE_BLOCK_PATTERN + r"|`([^`]+)`")
    code_blocks = code_pattern.findall(text)

    # Extract the individual code blocks and languages from the matched groups
    extracted = []
    for lang, group1, group2 in code_blocks:
        if group1:
            extracted.append((lang.strip(), group1.strip()))
        elif group2:
            extracted.append(("", group2.strip()))

    return extracted


def _cmd(lang):
    if lang.startswith("python") or lang in ["bash", "sh", "powershell"]:
        return lang
    if lang in ["shell"]:
        return "sh"
    if lang in ["ps1"]:
        return "powershell"
    raise NotImplementedError(f"{lang} not recognized in code execution")


def execute_code(
    code: Optional[str] = None,
    timeout: Optional[int] = None,
    filename: Optional[str] = None,
    work_dir: Optional[str] = None,
    use_docker: Optional[Union[List[str], str, bool]] = None,
    lang: Optional[str] = "python",
) -> Tuple[int, str, str]:
    """Execute code in a docker container.
    This function is not tested on MacOS.

    Args:
        code (Optional, str): The code to execute.
            If None, the code from the file specified by filename will be executed.
            Either code or filename must be provided.
        timeout (Optional, int): The maximum execution time in seconds.
            If None, a default timeout will be used. The default timeout is 600 seconds. On Windows, the timeout is not enforced when use_docker=False.
        filename (Optional, str): The file name to save the code or where the code is stored when `code` is None.
            If None, a file with a randomly generated name will be created.
            The randomly generated file will be deleted after execution.
            The file name must be a relative path. Relative paths are relative to the working directory.
        work_dir (Optional, str): The working directory for the code execution.
            If None, a default working directory will be used.
            The default working directory is the "extensions" directory under
            "path_to_autogen".
        use_docker (Optional, list, str or bool): The docker image to use for code execution.
            If a list or a str of image name(s) is provided, the code will be executed in a docker container
            with the first image successfully pulled.
            If None, False or empty, the code will be executed in the current environment.
            Default is None, which will be converted into an empty list when docker package is available.
            Expected behaviour:
                - If `use_docker` is explicitly set to True and the docker package is available, the code will run in a Docker container.
                - If `use_docker` is explicitly set to True but the Docker package is missing, an error will be raised.
                - If `use_docker` is not set (i.e., left default to None) and the Docker package is not available, a warning will be displayed, but the code will run natively.
            If the code is executed in the current environment,
            the code must be trusted.
        lang (Optional, str): The language of the code. Default is "python".

    Returns:
        int: 0 if the code executes successfully.
        str: The error message if the code fails to execute; the stdout otherwise.
        image: The docker image name after container run when docker is used.
    """    
    if all((code is None, filename is None)):
        error_msg = f"Either {code=} or {filename=} must be provided."
        logger.error(error_msg)
        raise AssertionError(error_msg)

    # Warn if use_docker was unspecified (or None), and cannot be provided (the default).
    # In this case the current behavior is to fall back to run natively, but this behavior
    # is subject to change.
    if use_docker is None:
        if docker is None:
            use_docker = False
            logger.warning(
                "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change"
            )
        else:
            # Default to true
            use_docker = True

    timeout = timeout or DEFAULT_TIMEOUT
    original_filename = filename
    if WIN32 and lang in ["sh", "shell"] and (not use_docker):
        lang = "ps1"
    if filename is None:
        code_hash = md5(code.encode()).hexdigest()
        # create a file with a automatically generated name
        filename = f"tmp_code_{code_hash}.{'py' if lang.startswith('python') else lang}"
    if work_dir is None:
        work_dir = WORKING_DIR
    filepath = os.path.join(work_dir, filename)
    file_dir = os.path.dirname(filepath)
    os.makedirs(file_dir, exist_ok=True)
    if code is not None:
        with open(filepath, "w", encoding="utf-8") as fout:
            fout.write(code)
    # check if already running in a docker container
    in_docker_container = os.path.exists("/.dockerenv")
    if not use_docker or in_docker_container:
        # already running in a docker container
        cmd = [
            sys.executable if lang.startswith("python") else _cmd(lang),
            f".\\{filename}" if WIN32 else filename,
        ]
        if WIN32:
            logger.warning("SIGALRM is not supported on Windows. No timeout will be enforced.")
            result = subprocess.run(
                cmd,
                cwd=work_dir,
                capture_output=True,
                text=True,
            )
        else:
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(
                    subprocess.run,
                    cmd,
                    cwd=work_dir,
                    capture_output=True,
                    text=True,
                )
                try:
                    result = future.result(timeout=timeout)
                except TimeoutError:
                    if original_filename is None:
                        os.remove(filepath)
                    return 1, TIMEOUT_MSG, None
        if original_filename is None:
            os.remove(filepath)
        if result.returncode:
            logs = result.stderr
            if original_filename is None:
                abs_path = str(pathlib.Path(filepath).absolute())
                logs = logs.replace(str(abs_path), "").replace(filename, "")
            else:
                abs_path = str(pathlib.Path(work_dir).absolute()) + PATH_SEPARATOR
                logs = logs.replace(str(abs_path), "")
        else:
            logs = result.stdout
        return result.returncode, logs, None

    # create a docker client
    client = docker.from_env()
    image_list = (
        ["python:3-alpine", "python:3", "python:3-windowsservercore"]
        if use_docker is True
        else [use_docker]
        if isinstance(use_docker, str)
        else use_docker
    )
    for image in image_list:
        # check if the image exists
        try:
            client.images.get(image)
            break
        except docker.errors.ImageNotFound:
            # pull the image
            print("Pulling image", image)
            try:
                client.images.pull(image)
                break
            except docker.errors.DockerException:
                print("Failed to pull image", image)
    # get a randomized str based on current time to wrap the exit code
    exit_code_str = f"exitcode{time.time()}"
    abs_path = pathlib.Path(work_dir).absolute()
    cmd = [
        "sh",
        "-c",
        f"{_cmd(lang)} {filename}; exit_code=$?; echo -n {exit_code_str}; echo -n $exit_code; echo {exit_code_str}",
    ]
    # create a docker container
    container = client.containers.run(
        image,
        command=cmd,
        working_dir="/workspace",
        detach=True,
        # get absolute path to the working directory
        volumes={abs_path: {"bind": "/workspace", "mode": "rw"}},
    )
    start_time = time.time()
    while container.status != "exited" and time.time() - start_time < timeout:
        # Reload the container object
        container.reload()
    if container.status != "exited":
        container.stop()
        container.remove()
        if original_filename is None:
            os.remove(filepath)
        return 1, TIMEOUT_MSG, image
    # get the container logs
    logs = container.logs().decode("utf-8").rstrip()
    # commit the image
    tag = filename.replace("/", "")
    container.commit(repository="python", tag=tag)
    # remove the container
    container.remove()
    # check if the code executed successfully
    exit_code = container.attrs["State"]["ExitCode"]
    if exit_code == 0:
        # extract the exit code from the logs
        pattern = re.compile(f"{exit_code_str}(\\d+){exit_code_str}")
        match = pattern.search(logs)
        exit_code = 1 if match is None else int(match.group(1))
        # remove the exit code from the logs
        logs = logs if match is None else pattern.sub("", logs)

    if original_filename is None:
        os.remove(filepath)
    if exit_code:
        logs = logs.replace(f"/workspace/{filename if original_filename is None else ''}", "")
    # return the exit code, logs and image
    return exit_code, logs, f"python:{tag}"


def _remove_check(response):
    """Remove the check function from the response."""
    # find the position of the check function
    pos = response.find("def check(")
    if pos == -1:
        return response
    return response[:pos]


def eval_function_completions(
    responses: List[str],
    definition: str,
    test: Optional[str] = None,
    entry_point: Optional[str] = None,
    assertions: Optional[Union[str, Callable[[str], Tuple[str, float]]]] = None,
    timeout: Optional[float] = 3,
    use_docker: Optional[bool] = True,
) -> Dict:
    """(openai<1) Select a response from a list of responses for the function completion task (using generated assertions), and/or evaluate if the task is successful using a gold test.

    Args:
        responses (list): The list of responses.
        definition (str): The input definition.
        test (Optional, str): The test code.
        entry_point (Optional, str): The name of the function.
        assertions (Optional, str or Callable): The assertion code which serves as a filter of the responses, or an assertion generator.
            When provided, only the responses that pass the assertions will be considered for the actual test (if provided).
        timeout (Optional, float): The timeout for executing the code.

    Returns:
        dict: The success metrics.
    """
    n = len(responses)
    if assertions is None:
        # no assertion filter
        success_list = []
        for i in range(n):
            response = _remove_check(responses[i])
            code = (
                f"{response}\n{test}\ncheck({entry_point})"
                if response.startswith("def")
                else f"{definition}{response}\n{test}\ncheck({entry_point})"
            )
            success = execute_code(code, timeout=timeout, use_docker=use_docker)[0] == 0
            success_list.append(success)
        return {
            "expected_success": 1 - pow(1 - sum(success_list) / n, n),
            "success": any(s for s in success_list),
        }
    if callable(assertions) and n > 1:
        # assertion generator
        assertions, gen_cost = assertions(definition)
    else:
        assertions, gen_cost = None, 0
    if n > 1 or test is None:
        for i in range(n):
            response = responses[i] = _remove_check(responses[i])
            code = (
                f"{response}\n{assertions}" if response.startswith("def") else f"{definition}{response}\n{assertions}"
            )
            succeed_assertions = execute_code(code, timeout=timeout, use_docker=use_docker)[0] == 0
            if succeed_assertions:
                break
    else:
        # just test, no need to check assertions
        succeed_assertions = False
        i, response = 0, responses[0]
    if test is None:
        # no test code
        return {
            "index_selected": i,
            "succeed_assertions": succeed_assertions,
            "gen_cost": gen_cost,
            "assertions": assertions,
        }
    code_test = (
        f"{response}\n{test}\ncheck({entry_point})"
        if response.startswith("def")
        else f"{definition}{response}\n{test}\ncheck({entry_point})"
    )
    success = execute_code(code_test, timeout=timeout, use_docker=use_docker)[0] == 0
    return {
        "index_selected": i,
        "succeed_assertions": succeed_assertions,
        "success": success,
        "gen_cost": gen_cost,
        "assertions": assertions,
    }


_FUNC_COMPLETION_PROMPT = "# Python 3{definition}"
_FUNC_COMPLETION_STOP = ["\nclass", "\ndef", "\nif", "\nprint"]

class PassAssertionFilter:
    def __init__(self, assertions):
        self._assertions = assertions        
        self.cost = 0
        self.metrics = self.responses = None

    def pass_assertions(self, context, response, **_):        
        responses = response[0].output        
        metrics = eval_function_completions(responses, context["definition"], assertions=self._assertions)
        self._assertions = metrics["assertions"]
        self.cost += metrics["gen_cost"]
        self.metrics = metrics
        self.responses = responses
        return metrics["succeed_assertions"]



##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/client/entrypoints/openai/tool.py
"""
Utility functions to help with the tools api commonly used in LLMs.
"""

import inspect
from typing import Callable, Optional, Dict, List, Any, get_type_hints, get_origin

try:
    # For Python 3.9 and later
    from typing import _AnnotatedAlias  # type: ignore
except ImportError:
    # For Python versions below 3.9
    from typing_extensions import _AnnotatedAlias  # type: ignore

_type_map = {
    str: "string",
    int: "integer",
    float: "number",
    bool: "boolean",
    list: "array",
}


def _original_type_backward_compatibility(param_type):
    """
    This function is for backward compatibility with Python 3.8 and below.
    It returns the original type of the parameter, which is the same as the
    param_type if the Python version is 3.9 and above, but is the __origin__ of
    the param_type if the Python version is 3.8 and below.
    """
    try:
        return param_type.__origin__
    except AttributeError:
        return param_type


def _get_type_spec(param_type, param_annotation, default_value):
    """
    Gets the type spec in json format for a specific parameter type.

    The param type must be one of the following: str, int, float, bool, list, or typing.List. There are two ways to annotate a parameter: one is via Annotated[type, description], and one is via the default value as the description.

    The description depends on the parameter:
    - For list types, it should be a list of tuples, where each tuple is of length 3, and the first element is a string as the name, the second element is the type of the field, and the third element is the description of the field.
    - For string types, it should either be a string as the description, or a tuple of length 2, where the first element is the string description, and the second element is a list of strings representing the enum.
    - For all other types, it should be a string as the description.

    For example, for an int field, you can do
        def foo(int_param: Annotated[int, "this is an int description"])
    or
        def foo(int_param: int = "this is an int description")
    (note that the default value must be a string in this case, which is a bit hacky, but it works.
    It is recommended that you use Annotated whenever possible.)
    """
    if get_origin(param_type) in (list, List):
        param_type = list
    try:
        type_name = _type_map[param_type]
    except KeyError:
        raise TypeError(f"Type {param_type} is not supported by the api.")
    # We will first prefer the annotation, then the default value, to find the
    # description of the parameter.
    if isinstance(param_annotation, _AnnotatedAlias):
        description = param_annotation.__metadata__
        description = (
            description[0]
            if isinstance(description, tuple) and len(description) == 1
            else description
        )
    elif default_value is not inspect.Parameter.empty:
        description = default_value
    else:
        raise ValueError("Either param_annotation or default_value must be provided.")

    if param_type == list:
        if not isinstance(description, list):
            raise TypeError(
                f"For list type {param_type}(aka {type_name}), value must be a list"
                " containing the description of the field."
            )
        array_description = {"type": "object", "properties": {}}
        for i, v in enumerate(description):
            if len(v) == 2:
                sub_param_name = _original_type_backward_compatibility(v[0])
                sub_param_annotation = v[1]
                sub_param_type = v[1].__origin__
                sub_param_default_value = inspect.Parameter.empty
            elif len(v) == 3:
                sub_param_name = _original_type_backward_compatibility(v[0])
                sub_param_annotation = v[1]
                sub_param_type = v[1]
                sub_param_default_value = v[2]
            else:
                raise TypeError(
                    "For array type, each element of the list must be a tuple of"
                    " length 2 or 3, where the first element is a string, the second"
                    " element is the Annotated type (if len==2) or raw type (if"
                    " len==3) of the field, and the third element (if len==3) is the"
                    f" description of the field. Got {v} (index {i})"
                )
            try:
                type_spec = _get_type_spec(
                    sub_param_type, sub_param_annotation, sub_param_default_value
                )
            except Exception as e:
                raise TypeError(
                    f"Error when processing the {i}th element of the list {v}. Source"
                    f" exception: {e}"
                )
            array_description["properties"][sub_param_name] = type_spec
        return {"type": "array", "items": array_description}
    elif param_type == str:
        if isinstance(description, str):
            # simple string type
            return {"type": type_name, "description": description}
        elif (
            len(description) == 2
            and isinstance(description[0], str)
            and isinstance(description[1], list)
        ):
            # string type with enum
            if not all(isinstance(v, str) for v in description[1]):
                raise TypeError(
                    f"For string type {param_type}(aka {type_name}) with an enum, the"
                    " enum must be a list of strings."
                )
            return {
                "type": type_name,
                "description": description[0],
                "enum": description[1],
            }
        else:
            raise TypeError(
                "For string type, value must be a string containing the description of"
                " the field, or a tuple of length 2 where the first element is the"
                " description of the field and the second element is a list of strings"
                f" representing the enum. Got {description}."
            )
    else:
        if not isinstance(description, str):
            raise TypeError(
                f"For type {param_type}, value must be a"
                " string containing the description of the field."
            )
        return {"type": type_name, "description": description}


def get_tools_spec(func: Callable, name: Optional[str] = None) -> Dict[str, Any]:
    """
    Given a function stub, return a dictionary that is OpenAI tools api compatible
    for function calling. Note that since the OpenAI tools api is not explicitly
    documented, this is a best effort implementation.

    Args:
        func: a function, or a simple stub, which defines its parameters and properly
            annotates the types of the parameters, using Annotated[...], or default
            values to provide the description of the parameters.
        name: (optional) the name of the function. If not provided, the name of the
            function will be used.
    Returns:
        A dictionary that is OpenAI tools api compatible for function calling.
    """
    if not callable(func):
        raise TypeError("func must be a callable object.")
    function_name = name if name else func.__name__
    docstring = inspect.getdoc(func)
    # get the annotations of the function parameters
    type_hints = get_type_hints(func)
    # get the default values of the function parameters
    signature = inspect.signature(func)
    parameters = signature.parameters
    annotations = func.__annotations__

    # Constructing the JSON structure
    function_info = {
        "name": function_name,
        "description": docstring,
        "parameters": {"type": "object", "properties": {}},
    }

    # Adding parameter information to the JSON structure
    for param_name, param in parameters.items():
        # Determine the type of the parameter
        try:
            param_type = _original_type_backward_compatibility(type_hints[param_name])
        except KeyError:
            raise TypeError(f"Parameter {param_name} does not have a type annotation.")
        # Determine the annotation of the parameter
        param_annotation = annotations.get(param_name)
        # Determine the default value/description of the parameter
        default_value = param.default
        # Add parameter information to the JSON structure
        function_info["parameters"]["properties"][param_name] = _get_type_spec(
            param_type, param_annotation, default_value
        )

    return function_info


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/client/entrypoints/openai/protocol.py
# Adapted from
# https://github.com/lm-sys/FastChat/blob/168ccc29d3f7edc50823016105c024fe2282732a/fastchat/protocol/openai_api_protocol.py
import time
from typing import Dict, List, Literal, Optional, Union

from pydantic import BaseModel, Field

from vllm.utils import random_uuid


class ErrorResponse(BaseModel):
    object: str = "error"
    message: str
    type: str
    param: Optional[str] = None
    code: Optional[str] = None


class ModelPermission(BaseModel):
    id: str = Field(default_factory=lambda: f"modelperm-{random_uuid()}")
    object: str = "model_permission"
    created: int = Field(default_factory=lambda: int(time.time()))
    allow_create_engine: bool = False
    allow_sampling: bool = True
    allow_logprobs: bool = True
    allow_search_indices: bool = False
    allow_view: bool = True
    allow_fine_tuning: bool = False
    organization: str = "*"
    group: Optional[str] = None
    is_blocking: str = False


class ModelCard(BaseModel):
    id: str
    object: str = "model"
    created: int = Field(default_factory=lambda: int(time.time()))
    owned_by: str = "vllm"
    root: Optional[str] = None
    parent: Optional[str] = None
    permission: List[ModelPermission] = Field(default_factory=list)


class ModelList(BaseModel):
    object: str = "list"
    data: List[ModelCard] = Field(default_factory=list)


class UsageInfo(BaseModel):
    prompt_tokens: int = 0
    total_tokens: int = 0
    completion_tokens: Optional[int] = 0


class ChatCompletionRequest(BaseModel):
    model: str
    messages: Union[str, List[Dict[str, str]]]
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 1.0
    n: Optional[int] = 1
    max_tokens: Optional[int] = None
    stop: Optional[Union[str, List[str]]] = Field(default_factory=list)
    stream: Optional[bool] = False
    presence_penalty: Optional[float] = 0.0
    frequency_penalty: Optional[float] = 0.0
    logit_bias: Optional[Dict[str, float]] = None
    user: Optional[str] = None
    # Additional parameters supported by vLLM
    best_of: Optional[int] = None
    top_k: Optional[int] = -1
    ignore_eos: Optional[bool] = False
    use_beam_search: Optional[bool] = False
    stop_token_ids: Optional[List[int]] = Field(default_factory=list)
    skip_special_tokens: Optional[bool] = True
    spaces_between_special_tokens: Optional[bool] = True
    add_generation_prompt: Optional[bool] = True
    echo: Optional[bool] = False
    repetition_penalty: Optional[float] = 1.0
    min_p: Optional[float] = 0.0


class CompletionRequest(BaseModel):
    model: str
    # a string, array of strings, array of tokens, or array of token arrays
    prompt: Union[List[int], List[List[int]], str, List[str]]
    suffix: Optional[str] = None
    max_tokens: Optional[int] = 16
    temperature: Optional[float] = 1.0
    top_p: Optional[float] = 1.0
    n: Optional[int] = 1
    stream: Optional[bool] = False
    logprobs: Optional[int] = None
    echo: Optional[bool] = False
    stop: Optional[Union[str, List[str]]] = Field(default_factory=list)
    presence_penalty: Optional[float] = 0.0
    frequency_penalty: Optional[float] = 0.0
    best_of: Optional[int] = None
    logit_bias: Optional[Dict[str, float]] = None
    user: Optional[str] = None
    # Additional parameters supported by vLLM
    top_k: Optional[int] = -1
    ignore_eos: Optional[bool] = False
    use_beam_search: Optional[bool] = False
    stop_token_ids: Optional[List[int]] = Field(default_factory=list)
    skip_special_tokens: Optional[bool] = True
    spaces_between_special_tokens: Optional[bool] = True
    repetition_penalty: Optional[float] = 1.0
    min_p: Optional[float] = 0.0


class LogProbs(BaseModel):
    text_offset: List[int] = Field(default_factory=list)
    token_logprobs: List[Optional[float]] = Field(default_factory=list)
    tokens: List[str] = Field(default_factory=list)
    top_logprobs: Optional[List[Optional[Dict[int, float]]]] = None


class CompletionResponseChoice(BaseModel):
    index: int
    text: str
    logprobs: Optional[LogProbs] = None
    finish_reason: Optional[Literal["stop", "length"]] = None


class CompletionResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"cmpl-{random_uuid()}")
    object: str = "text_completion"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[CompletionResponseChoice]
    usage: UsageInfo


class CompletionResponseStreamChoice(BaseModel):
    index: int
    text: str
    logprobs: Optional[LogProbs] = None
    finish_reason: Optional[Literal["stop", "length"]] = None


class CompletionStreamResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"cmpl-{random_uuid()}")
    object: str = "text_completion"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[CompletionResponseStreamChoice]
    usage: Optional[UsageInfo]


class ChatMessage(BaseModel):
    role: str
    content: str


class ChatCompletionResponseChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: Optional[Literal["stop", "length"]] = None


class ChatCompletionResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"chatcmpl-{random_uuid()}")
    object: str = "chat.completion"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[ChatCompletionResponseChoice]
    usage: UsageInfo


class DeltaMessage(BaseModel):
    role: Optional[str] = None
    content: Optional[str] = None


class ChatCompletionResponseStreamChoice(BaseModel):
    index: int
    delta: DeltaMessage
    finish_reason: Optional[Literal["stop", "length"]] = None


class ChatCompletionStreamResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"chatcmpl-{random_uuid()}")
    object: str = "chat.completion.chunk"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[ChatCompletionResponseStreamChoice]
    usage: Optional[UsageInfo] = Field(
        default=None, description="data about request and response")


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/client/entrypoints/openai/api_server.py
# Adapted from
# https://github.com/lm-sys/FastChat/blob/168ccc29d3f7edc50823016105c024fe2282732a/fastchat/serve/openai_api_server.py
# Adapted from
# vLLM project

import argparse
import asyncio
import codecs
import json
import time
import ray
from http import HTTPStatus
from typing import AsyncGenerator, Dict, List, Optional, Tuple, Union

from aioprometheus import MetricsMiddleware
from aioprometheus.asgi.starlette import metrics
import fastapi
import uvicorn
from fastapi import Request
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse, Response

from byzerllm.utils.client import ByzerLLM,LLMResponse
from byzerllm.utils import SingleOutputMeta

from byzerllm.utils.client.entrypoints.openai.protocol import (
    CompletionRequest, CompletionResponse, CompletionResponseChoice,
    CompletionResponseStreamChoice, CompletionStreamResponse,
    ChatCompletionRequest, ChatCompletionResponse,
    ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice,
    ChatCompletionStreamResponse, ChatMessage, DeltaMessage, ErrorResponse,
    LogProbs, ModelCard, ModelList, ModelPermission, UsageInfo)

import logging
import uuid

def random_uuid() -> str:
    return str(uuid.uuid4().hex)



TIMEOUT_KEEP_ALIVE = 5  # seconds
logger = logging.getLogger(__name__)

ray.init(address = "auto", namespace="default",ignore_reinit_error=True)

app = fastapi.FastAPI()
response_role = "assistant"


def parse_args():
    parser = argparse.ArgumentParser(
        description="ByzerLLm OpenAI-Compatible RESTful API server.")
    parser.add_argument("--host", type=str, default=None, help="host name")
    parser.add_argument("--port", type=int, default=8000, help="port number")
    parser.add_argument("--allow-credentials",
                        action="store_true",
                        help="allow credentials")
    parser.add_argument("--allowed-origins",
                        type=json.loads,
                        default=["*"],
                        help="allowed origins")
    parser.add_argument("--allowed-methods",
                        type=json.loads,
                        default=["*"],
                        help="allowed methods")
    parser.add_argument("--allowed-headers",
                        type=json.loads,
                        default=["*"],
                        help="allowed headers")
    parser.add_argument("--served-model-name",
                        type=str,
                        default=None,
                        help="The model name used in the API. If not "
                        "specified, the model name will be the same as "
                        "the huggingface name.")            
    parser.add_argument("--ssl-keyfile",
                        type=str,
                        default=None,
                        help="The file path to the SSL key file")
    parser.add_argument("--ssl-certfile",
                        type=str,
                        default=None,
                        help="The file path to the SSL cert file")
    
    return parser.parse_args()


app.add_middleware(MetricsMiddleware)  # Trace HTTP server metrics
app.add_route("/metrics", metrics)  # Exposes HTTP metrics


def create_error_response(status_code: HTTPStatus,
                          message: str) -> JSONResponse:
    return JSONResponse(ErrorResponse(message=message,
                                      type="invalid_request_error").dict(),
                        status_code=status_code.value)


@app.exception_handler(RequestValidationError)
async def validation_exception_handler(_, exc):
    return create_error_response(HTTPStatus.BAD_REQUEST, str(exc))



@app.get("/health")
async def health() -> Response:
    """Health check."""
    return Response(status_code=200)


@app.get("/v1/models")
async def show_available_models():
    """Show available models. Right now we only have one model."""
    model_cards = [
        ModelCard(id="",
                  root="",
                  permission=[ModelPermission()])
    ]
    return ModelList(data=model_cards)


@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest,
                                 raw_request: Request):
    """Completion API similar to OpenAI's API.

    See  https://platform.openai.com/docs/api-reference/chat/create
    for the API specification. This API mimics the OpenAI ChatCompletion API.

    NOTE: Currently we do not support the following features:
        - function_call (Users should implement this by themselves)
        - logit_bias (to be supported by vLLM engine)
    """    

    if request.logit_bias is not None and len(request.logit_bias) > 0:
        # TODO: support logit_bias in vLLM engine.
        return create_error_response(HTTPStatus.BAD_REQUEST,
                                     "logit_bias is not currently supported")       

    model_name = request.model
    
    llm = ByzerLLM()
    llm.setup_template(model=model_name,template="auto")
    llm.setup_extra_generation_params(model_name,extra_generation_params={
      "temperature":request.temperature or 0.01,
      "top_p":request.top_p or 0.99
    })

    request_id = f"cmpl-{random_uuid()}"
    created_time = int(time.monotonic())
    chunk_object_type = "chat.completion.chunk"            
    
    
    def get_role() -> str:
        if request.add_generation_prompt:
            return response_role
        else:
            return request.messages[-1]["role"]
        

    async def completion_stream_generator() -> AsyncGenerator[str, None]:
        # Send first response for each request.n (index) with the role          
        result_generator = llm.async_stream_chat_oai(model=model_name,
                                           conversations=request.messages,
                                           llm_config={"gen.request_id":request_id}) 
        role = get_role()     
        for i in range(request.n):
            choice_data = ChatCompletionResponseStreamChoice(
                index=i, delta=DeltaMessage(role=role), finish_reason=None)
            chunk = ChatCompletionStreamResponse(id=request_id,
                                                 object=chunk_object_type,
                                                 created=created_time,
                                                 choices=[choice_data],
                                                 model=model_name)
            data = chunk.json(exclude_unset=True, ensure_ascii=False)
            yield f"data: {data}\n\n"

        # Send response to echo the input portion of the last message
        if request.echo:
            last_msg_content = ""
            if request.messages and isinstance(
                    request.messages, list) and request.messages[-1].get(
                        "content") and request.messages[-1].get(
                            "role") == role:
                last_msg_content = request.messages[-1]["content"]
            if last_msg_content:
                for i in range(request.n):
                    choice_data = ChatCompletionResponseStreamChoice(
                        index=i,
                        delta=DeltaMessage(content=last_msg_content),
                        finish_reason=None)
                    chunk = ChatCompletionStreamResponse(
                        id=request_id,
                        object=chunk_object_type,
                        created=created_time,
                        choices=[choice_data],
                        model=model_name)
                    data = chunk.json(exclude_unset=True, ensure_ascii=False)
                    yield f"data: {data}\n\n"

        # Send response for each token for each request.n (index)        
        finish_reason_sent = [False] * request.n
        async for (s,meta) in result_generator:  
            meta: SingleOutputMeta          
            for _ in [(s,meta)]:
                i = 0                              
                prompt_tokens = meta.input_tokens_count
                final_usage = UsageInfo(
                    prompt_tokens=prompt_tokens,
                    completion_tokens=meta.generated_tokens_count,
                    total_tokens=prompt_tokens + meta.generated_tokens_count,
                )
                choice_data = ChatCompletionResponseStreamChoice(
                    index=i, delta=DeltaMessage(content=s), finish_reason=None)
                chunk = ChatCompletionStreamResponse(
                    id=request_id,
                    object=chunk_object_type,
                    created=created_time,
                    choices=[choice_data],
                    model=model_name)
                if final_usage is not None:
                    chunk.usage = final_usage
                data = chunk.json(exclude_unset=True,
                                    exclude_none=True,
                                    ensure_ascii=False)
                yield f"data: {data}\n\n"
                finish_reason_sent[i] = True
        # Send the final done message after all response.n are finished
        yield "data: [DONE]\n\n"

    async def completion_full_generator():
        
        async def chat():
            r = llm.chat_oai(model=model_name,
                            conversations=request.messages,
                            llm_config={"gen.request_id":request_id}) 
            for _ in r:
                yield _
        
        result_generator = await asyncio.to_thread(chat) 
        
        async for res in result_generator:
            if await raw_request.is_disconnected():
                # Abort the request if the client disconnects.
                await llm.abort(request_id,model=model_name)
                return create_error_response(HTTPStatus.BAD_REQUEST,
                                             "Client disconnected")
            final_res = res
        assert final_res is not None

        choices = []
        role = get_role()
        for r in [final_res]:
            r:LLMResponse
            choice_data = ChatCompletionResponseChoice(
                index=0,
                message=ChatMessage(role=role, content=r.output),
                finish_reason=None,
            )
            choices.append(choice_data)

        if request.echo:
            last_msg_content = ""
            if request.messages and isinstance(
                    request.messages, list) and request.messages[-1].get(
                        "content") and request.messages[-1].get(
                            "role") == role:
                last_msg_content = request.messages[-1]["content"]

            for choice in choices:
                full_message = last_msg_content + choice.message.content
                choice.message.content = full_message

        num_prompt_tokens = r.metadata["input_tokens_count"]
        num_generated_tokens = r.metadata["generated_tokens_count"]

        usage = UsageInfo(
            prompt_tokens=num_prompt_tokens,
            completion_tokens=num_generated_tokens,
            total_tokens=num_prompt_tokens + num_generated_tokens,
        )
        response = ChatCompletionResponse(
            id=request_id,
            created=created_time,
            model=model_name,
            choices=choices,
            usage=usage,
        )

        return response

    # Streaming response
    if request.stream:
        return StreamingResponse(completion_stream_generator(),
                                 media_type="text/event-stream")
    else:
        return await completion_full_generator()


@app.post("/v1/completions")
async def create_completion(request: CompletionRequest, raw_request: Request):
    """Completion API similar to OpenAI's API.

    See https://platform.openai.com/docs/api-reference/completions/create
    for the API specification. This API mimics the OpenAI Completion API.

    NOTE: Currently we do not support the following features:
        - suffix (the language models we currently support do not support
          suffix)
        - logit_bias (to be supported by vLLM engine)
    """
    

    # OpenAI API supports echoing the prompt when max_tokens is 0.
    echo_without_generation = request.echo and request.max_tokens == 0   

    model_name = request.model
    request_id = f"cmpl-{random_uuid()}"
    
    created_time = int(time.monotonic())  
    
    # Similar to the OpenAI API, when n != best_of, we do not stream the
    # results. In addition, we do not stream the results when use beam search.
    stream = request.stream

    def create_stream_response_json(
        index: int,
        text: str,
        logprobs: Optional[LogProbs] = None,
        finish_reason: Optional[str] = None,
        usage: Optional[UsageInfo] = None,
    ) -> str:
        choice_data = CompletionResponseStreamChoice(
            index=index,
            text=text,
            logprobs=logprobs,
            finish_reason=finish_reason,
        )
        response = CompletionStreamResponse(
            id=request_id,
            created=created_time,
            model=model_name,
            choices=[choice_data],
        )
        if usage is not None:
            response.usage = usage
        response_json = response.json(exclude_unset=True, ensure_ascii=False)

        return response_json

    async def completion_stream_generator() -> AsyncGenerator[str, None]:
        previous_texts = [""] * request.n    
        result_generator = llm.async_stream_chat_oai(model=model_name,
                                           conversations=[{
                                                  "role": "user",
                                                  "content": request.prompt
                                           }],
                                           llm_config={"gen.request_id":request_id})

        async for res in result_generator:            
            (s,meta) = res
            meta:SingleOutputMeta
            for _ in [(s,meta)]:
                i = 0
                delta_text = s[len(previous_texts[i]):]                
                top_logprobs = None
                logprobs = None                            
                
                previous_texts[i] = s                
                finish_reason = None
                response_json = create_stream_response_json(
                    index=i,
                    text=delta_text,
                    logprobs=logprobs,
                    finish_reason=finish_reason,
                )
                yield f"data: {response_json}\n\n"                                                
                completion_tokens = meta.generated_tokens_count
                prompt_tokens = meta.input_tokens_count
                final_usage = UsageInfo(
                    prompt_tokens=prompt_tokens,
                    completion_tokens=completion_tokens,
                    total_tokens=prompt_tokens + completion_tokens,
                )
                response_json = create_stream_response_json(
                    index=i,
                    text="",
                    logprobs=logprobs,
                    finish_reason=None,
                    usage=final_usage,
                )
                yield f"data: {response_json}\n\n"
        yield "data: [DONE]\n\n"

    # Streaming response
    if stream:
        return StreamingResponse(completion_stream_generator(),
                                 media_type="text/event-stream")

    # Non-streaming response
    async def chat():
            r = llm.chat_oai(model=model_name,
                            conversations=request.messages,
                            llm_config={"gen.request_id":request_id}) 
            for _ in r:
                yield _
        
    result_generator = await asyncio.to_thread(chat) 
    final_res = None
    async for res in result_generator:
        if await raw_request.is_disconnected():
            # Abort the request if the client disconnects.
            await llm.abort(request_id,model=model_name)
            return create_error_response(HTTPStatus.BAD_REQUEST,
                                         "Client disconnected")
        final_res = res
    assert final_res is not None
    choices = []
        
    
    for r in [final_res]:  
        r:LLMResponse     
        choice_data = CompletionResponseChoice(
            index=0,
            text=r.input,
            logprobs=None,
            finish_reason=None,
        )
        choices.append(choice_data)

    num_prompt_tokens = r.metadata["input_tokens_count"]
    num_generated_tokens = r.metadata["generated_tokens_count"]
    usage = UsageInfo(
        prompt_tokens=num_prompt_tokens,
        completion_tokens=num_generated_tokens,
        total_tokens=num_prompt_tokens + num_generated_tokens,
    )
    response = CompletionResponse(
        id=request_id,
        created=created_time,
        model=model_name,
        choices=choices,
        usage=usage,
    )

    if request.stream:
        # When user requests streaming but we don't stream, we still need to
        # return a streaming response with a single event.
        response_json = response.json(ensure_ascii=False)

        async def fake_stream_generator() -> AsyncGenerator[str, None]:
            yield f"data: {response_json}\n\n"
            yield "data: [DONE]\n\n"

        return StreamingResponse(fake_stream_generator(),
                                 media_type="text/event-stream")

    return response


if __name__ == "__main__":
    args = parse_args()

    app.add_middleware(
        CORSMiddleware,
        allow_origins=args.allowed_origins,
        allow_credentials=args.allow_credentials,
        allow_methods=args.allowed_methods,
        allow_headers=args.allowed_headers,
    )

    logger.info(f"args: {args}")
    
    # Register labels for metrics
    # add_global_metrics_labels(model_name=engine_args.model)

    uvicorn.run(app,
                host=args.host,
                port=args.port,
                log_level="info",
                timeout_keep_alive=TIMEOUT_KEEP_ALIVE,
                ssl_keyfile=args.ssl_keyfile,
                ssl_certfile=args.ssl_certfile)


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/config/__init__.py
import ray

MLSQL_CONFIG = "__MLSQL_CONFIG__"

@ray.remote
class MLSQLConifg(object):
    def __init__(self,instance_name:str, json_obj):        
        self._config = json_obj
        self.instance_name = instance_name

    def getitem(self, key,defaulValue):
        return self._config.get(key,defaulValue)

    def setitem(self, key, value):
        self._config[key] = value   

    def delitem(self, key):
        del self._config[key]


def create_mlsql_config(name,json_obj):
    config = get_mlsql_config()
    if config is not None:
        ray.kill(config)
    return MLSQLConifg.options(name=MLSQL_CONFIG,lifetime="detached").remote(name,json_obj)

def get_mlsql_config():
    try:
        config = ray.get_actor(MLSQL_CONFIG)
    except:
        config = None    
    return config

def get_mlsql_config_item(key,defaultValue):
    config = get_mlsql_config()
    return ray.get(config.getitem.remote(key,defaultValue))
        
def get_mlsql_config_pushgateway_address():
    return get_mlsql_config_item("spark.mlsql.pushgateway.address",None)

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/fulltune/deepspeed_trainner.py
import subprocess
import os
import ray
import json
import paramiko
import dataclasses
import uuid
import hashlib
from pyjava import RayContext
from typing import List
import getpass
from byzerllm import transfer_from_ob
from pyjava.udf.store import transfer_to_ob
from pyjava.storage import streaming_tar as STar

@dataclasses.dataclass
class TrainParameters():
    name:str
    data_dir:str
    config_dir:str    
    tokenizer_path:str
    model_dir:str
    max_length:int=4096
    steps_per_epoch:int=4096
    checkpoint_saving_path:str="checkpoints"     
    num_workers:int=1
    use_gpu:bool=True
    cpus_per_worker:int=1
    gpus_per_worker:int=1
    user:str="byzerllm"
    password:str="byzerllm"
    ssh_port:int=22
    passwordless_ssh_node:int=0
    auto_setup_passwordless_ssh:bool=False 
    private_key_name:str="byzerllm_id_rsa"
    public_key_name:str="byzerllm_id_rsa.pub" 
          

base_deepspeed_cnofig = json.loads('''
{
  "gradient_accumulation_steps": 1,
  "train_micro_batch_size_per_gpu": 1,
  "prescale_gradients": false,
  "zero_allow_untested_optimizer": true,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 1e-8,
      "eps": 1.0e-8,
      "betas": [
        0.9,
        0.95
      ],
      "weight_decay": 0.1
    }
  },
  "tensorboard": {
    "enabled": true,
    "output_path": "logs/",
    "job_name": "baichuan-7b-pt"
  },
  "zero_optimization": {
    "stage": 3,
    "contiguous_gradients": false,
    "allgather_bucket_size": 3e8,
    "reduce_bucket_size": 3e8,
    "overlap_comm": true,
    "reduce_scatter": true
  },
  "steps_per_print": 16,
  "gradient_clipping": 1.0,
  "wall_clock_breakdown": true,
  "bf16": {
    "enabled": true
  }
}

''')

def exec_command(command):
    return start_command(command)

def start_command(command):
    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    
    # Redirect stdout and stderr to the main Python process
    while True:
        output = process.stdout.readline().decode().strip()
        error = process.stderr.readline().decode().strip()
        if output:
            print(output,flush=True)
        if error:
            print(error,flush=True)
        if output == '' and error == '' and process.poll() is not None:
            break
    
    process.stdout.close()
    process.stderr.close()
    return process.poll()

def get_cmd(config_dir:str, client_args:str="", has_hostfile=False):
    hostfile_chunk = f" --hostfile={os.path.join(config_dir,'hostfile')} " if has_hostfile else ""
    include = ""
    if not has_hostfile:
        gpus = ray.get_gpu_ids()
        include = f" --include=localhost:{','.join(gpus)}"
    return f"deepspeed {include} {hostfile_chunk} --module byzerllm.utils.fulltune.launch {client_args}  --deepspeed --deepspeed_config {os.path.join(config_dir,'deepspeed.json')}"


def check_passwordless_ssh(ipOrHost):
    import paramiko
    import paramiko.util    

    # Load the SSH config file
    ssh_config = paramiko.SSHConfig()
    with open(os.path.expanduser('~/.ssh/config')) as f:
        ssh_config.parse(f)
    
    # status = exec_command(f"ssh {ipOrHost}")
    # return status == 0

    user_config = ssh_config.lookup(ipOrHost)
    return "user" in user_config
    

def try_connect_and_get_fingerprint(target_ip):
    # try ssh -o StrictHostKeyChecking=accept-new 6baf1a94a956f208af8ca0e3dc887d64
    s = subprocess.check_output(f"ssh-keyscan -t rsa {target_ip}", shell=True, universal_newlines=True)    
    return s
    

def setup_passwordless_ssh(worker,host,ip,port, username, password):
    _username = username if username else getpass.getuser()
    home_path = os.path.expanduser("~") 
    
    if not os.path.exists(os.path.join(home_path,".ssh","byzerllm_id_rsa")): 
        key = paramiko.RSAKey.generate(bits=2048)    

        if not os.path.exists(os.path.join(home_path,".ssh")):
            os.makedirs(os.path.join(home_path,".ssh"))

        key.write_private_key_file(os.path.join(home_path,".ssh","byzerllm_id_rsa"))

        public_key = key.get_base64()
        with open(os.path.join(home_path,".ssh","byzerllm_id_rsa.pub"), "w") as f:
            f.write(f"ssh-rsa {public_key}")
    
    with open(os.path.join(home_path,".ssh","byzerllm_id_rsa.pub"), "r") as f:
        public_key = f.read() 
        
    ssh_config = f'''
Host {host}
HostName {ip}
User {_username}
Port {port}
IdentityFile ~/.ssh/byzerllm_id_rsa
    '''

    exec_command(f'''
cat <<EOF >> ~/.ssh/config
{ssh_config}
EOF''')
    ray.get(worker.setup_pubkey.remote(public_key))
    
    fingerprint = try_connect_and_get_fingerprint(ip)
    exec_command(f'''
cat <<EOF >> ~/.ssh/known_hosts
{fingerprint}
EOF''')
    
    s = check_passwordless_ssh(host)    
    if not s:
        raise Exception(f"setup passwordless ssh failed in {ip}")

    

def encode_str(input:str):
    encoded_string = hashlib.md5(input.encode("utf-8")).hexdigest()    
    return encoded_string

@ray.remote
class TrainMaster():
    def __init__(self,args:TrainParameters,data_refs,model_refs,standalone) -> None:
        self.standalone = standalone
        self.args = args
        self.data_refs = data_refs
        self.data_ref = data_refs[0]        
        self.workers = []
        self.ips = []
        self.ipToHost = {}
        self.ipToWorkers = {}
        self.model_refs = model_refs
        if not standalone:
            self.workers = [TrainWorker.options(num_cpus=args.cpus_per_worker,
                                    num_gpus=args.gpus_per_worker).remote(args,data_refs[i],model_refs) for i in range(self.args.num_workers)]
            self.ips = ray.get([worker.get_ip.remote() for worker in self.workers]) 
            self.ipToHost = {ip:encode_str(ip) for ip in self.ips}
            for ip,worker in zip(self.ips,self.workers):
                if ip not in self.ipToWorkers:                    
                    self.ipToWorkers[ip] = []
                self.ipToWorkers[ip].append(worker)
        
        self.id = str(uuid.uuid4())

    def get_model(self):
        if self.standalone:         
            new_model_refs = []
            transfer_to_ob(self.args.name,self.args.checkpoint_saving_path,new_model_refs)
            return new_model_refs    
        else:
            return ray.get(self.worker[0].get_model.remote())

    def shutdown(self):
        if len(self.workers) > 0:
            shutdown_jobs = [ray.kill(worker) for worker in self.workers]
            ray.get(shutdown_jobs)
        ray.actor.exit_actor()

    def fit(self):
        
        if not os.path.exists(self.args.config_dir):
            os.makedirs(self.args.config_dir)
            
        with open(os.path.join(self.args.config_dir,"hostfile"),"w") as f:
            temp_ips = set()           
            for ip in self.ips:
                # Todo: we should set gpus ids from ray.get_gpu_ids() in every worker in feature
                # so the gpus used in deepspeed match the gpus used in ray
                workers = self.ipToWorkers[ip]
                if ip not in temp_ips:
                    temp_ips.add(ip)
                    f.write(f"{self.ipToHost[ip]} slots={self.args.gpus_per_worker * len(workers)}\n")

        with open(os.path.join(self.args.config_dir,"deepspeed.json"),"w") as f:
            f.write(json.dumps(base_deepspeed_cnofig))

        real_data_dir = self.args.data_dir
        
        # if standalone, we should save data to in master 
        if self.standalone:            
            if not os.path.exists(real_data_dir):
                os.makedirs(real_data_dir)
        
            data_file_path = os.path.join(real_data_dir,f"data-{self.id}.jsonl")
            with open(data_file_path,"w",encoding="utf-8") as f:        
                for item in RayContext.collect_from([self.data_ref]):
                    f.write(json.dumps(item,ensure_ascii=False)+"\n")
        
        # if not standalone, we should save data to every worker for deepspeed worker
        if len(self.workers) > 0:
            prepare_data_jobs = [worker.fit.remote() for worker in self.workers]
            ray.get(prepare_data_jobs)            

        client_args = {
            "data_dir":real_data_dir,
            "tokenizer_path":self.args.tokenizer_path,
            "checkpoint_saving_path":self.args.checkpoint_saving_path,
        }
        temp_clien_args = []
        for k,v in client_args.items():
            temp_clien_args.append(f"--{k} {v}")

        command = get_cmd(self.args.config_dir," ".join(temp_clien_args),has_hostfile=len(self.ips)>0)
        print(f"[{self.args.name}] deepspeed command: {command}")
        start_command(command)

    def setup_worker(self):    
        if not self.args.auto_setup_passwordless_ssh:
            return
        
        my_ip = self.get_ip()    
        for ip in self.ips:  
            hostname = self.ipToHost[ip] 
            workers = self.ipToWorkers[ip]         
            if not check_passwordless_ssh(hostname):                
                    print(f"[{self.args.name}] try to automatically setup ssh passwordless in {ip} from {my_ip}",flush=True)
                    setup_passwordless_ssh(workers[0],hostname,ip,self.args.ssh_port,self.args.user,self.args.password)                                                               
               

    def get_ip(self):
        id = ray.get_runtime_context().get_node_id()
        ip = [node for node in ray.nodes() if node['NodeID']==id][0]['NodeManagerAddress']
        return ip

@ray.remote
class TrainWorker():
    def __init__(self,args:TrainParameters,data_ref,model_refs) -> None:
        self.data_ref = data_ref
        self.args = args        
        self.id = str(uuid.uuid4()) 
        self.model_refs = model_refs


    def get_model(self):
        new_model_refs = []
        transfer_to_ob(self.args.name,self.args.checkpoint_saving_path,new_model_refs)
        return new_model_refs

    def fit(self):
        real_data_dir = self.args.data_dir

        if not os.path.exists(real_data_dir):
            os.makedirs(real_data_dir)
    
        data_file_path = os.path.join(real_data_dir,f"data-{self.id}.jsonl")
        with open(data_file_path,"w",encoding="utf-8") as f:        
            for item in RayContext.collect_from([self.data_ref]):
                f.write(json.dumps(item,ensure_ascii=False)+"\n")
        
        if len(self.model_refs) > 0:
            transfer_from_ob(self.args.name,self.args.model_refs,self.args.model_dir)
                
    def setup_pubkey(self,pubkey):
        home_path = os.path.expanduser("~") 
        if not os.path.exists(os.path.join(home_path,".ssh")):
            os.makedirs(os.path.join(home_path,".ssh"))
        status = exec_command('echo "{}" >> ~/.ssh/authorized_keys'.format(pubkey))
        exec_command("chmod 700 ~/.ssh")
        exec_command("chmod 600 ~/.ssh/authorized_keys")
        if status != 0:
            raise Exception(f"setup pubkey failed with status {status} in ${self.get_ip()}")

    def get_gpus(self):
        return ray.get_gpu_ids()
    
    def get_ip(self):
        id = ray.get_runtime_context().get_node_id()
        ip = [node for node in ray.nodes() if node['NodeID']==id][0]['NodeManagerAddress']
        return ip   

def distribute_train(args:TrainParameters,data_refs,model_refs):
    
    assert  args.num_workers == len(data_refs), f'''
    num_workers({args.num_workers}) must equal to data_refs({len(data_refs)}).
    Try to add the following code to repartition data and make sure the number of partitions equal to num_workers:
    
    ```
    run trainData as TableRepartition.`` 
    where partitionNum="{args.num_workers}" as preTrainData;
    ```

    '''  
    standalone = args.num_workers == 1

    if standalone:
        master = TrainMaster.options(num_cpus=args.cpus_per_worker,
                                        num_gpus=args.gpus_per_worker,
                                        resources={"passwordless_ssh_node":args.passwordless_ssh_node}
                                        ).remote(args,data_refs,model_refs,standalone)
    else:
        master = TrainMaster.options(num_cpus=0,
                                        num_gpus=0,
                                        resources={"passwordless_ssh_node":args.passwordless_ssh_node}
                                        ).remote(args,data_refs,model_refs,standalone) 

    new_model_refs = [] 
           
    try:
        ray.get(master.setup_worker.remote())
        ray.get(master.fit.remote())
        new_model_refs = ray.get(master.get)
    except Exception as e:
        print(f"Error: {e}")
        ray.get(master.shutdown.remote())
    
    return new_model_refs

       
    


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/fulltune/launch.py
import json
import os

import argparse
import deepspeed
import deepspeed.comm as dist
import numpy as np
import sentencepiece as spm
import torch

from .base_model.configuration_baichuan import BaiChuanConfig
from .base_model.modeling_baichuan import BaiChuanForCausalLM


def get_argument_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=str, default="data_dir",
                        help="Text files to do pre-train on")

    parser.add_argument("--tokenizer_path", type=str,
                        default="tokenizer.model",
                        help="Tokenizer model file path")

    parser.add_argument("--max_length", type=int, default=4096,
                        help="Max tokens per sentence in corpus")

    parser.add_argument("--steps_per_epoch", type=int, default=4096,
                        help="Step intervals to save checkpoint")

    parser.add_argument("--checkpoint_saving_path", type=str,
                        default="checkpoints",
                        help="Path to store checkpoint files")

    parser.add_argument("--local_rank", type=int, default=-1,
                        help="Reserved for deepspeed framework")
    return parser


arg_parser = get_argument_parser()
arg_parser = deepspeed.add_config_arguments(arg_parser)
args = arg_parser.parse_args()
deepspeed.init_distributed()


class DataEngine():
    def __init__(self, data_dir, tokenizer_path, micro_batch_size, max_length):
        self.MIN_TEXT_LEN = 20
        self.EOS_TOKEN_ID = 2
        self.data_dir = data_dir
        self.sp = spm.SentencePieceProcessor()
        self.sp.Load(tokenizer_path)
        self.micro_batch_size = micro_batch_size
        self.max_length = max_length
        self.data = []
        self.global_input_paths = [self.data_dir + "/" + x
                                   for x in os.listdir(self.data_dir)]
        self.local_input_paths = [x for i, x in
                                  enumerate(self.global_input_paths)
                                  if i % dist.get_world_size() == dist.get_rank()]

    def load_data(self):
        for file_path in self.local_input_paths:
            data = []
            with open(file_path, encoding="utf-8", errors="ignore") as f:
                for line_id, line in enumerate(f):
                    line_json = json.loads(line.strip())
                    text = line_json("instruction") + line_json("input") + line_json("output")
                    cc = self.sp.EncodeAsIds(text) + [self.EOS_TOKEN_ID]
                    if len(cc) < self.MIN_TEXT_LEN:
                        cc = []
                    data.extend(cc)
                    if len(data) >= self.micro_batch_size * (self.max_length + 1):
                        index = self.micro_batch_size * (self.max_length + 1)
                        self.data.append(data[:index])
                        data = []
        return

    def get_data(self):
        data = self.data.pop(0)
        seq = np.asarray(data).reshape(self.micro_batch_size, self.max_length + 1)
        data = torch.LongTensor(seq)
        data = data.cuda(non_blocking=True)
        return data


def prepare_data():
    data_dir = args.data_dir
    tokenizer_path = args.tokenizer_path
    ds_config = json.load(open(args.deepspeed_config))
    micro_batch_size = ds_config["train_micro_batch_size_per_gpu"]
    max_length = args.max_length
    data_engine = DataEngine(data_dir, tokenizer_path, micro_batch_size, max_length)
    data_engine.load_data()
    return data_engine


def prepare_model():
    with deepspeed.zero.Init(config_dict_or_path=args.deepspeed_config,
                             enabled=True,
                             mem_efficient_linear=False,
                             mpu=None):
        model = BaiChuanForCausalLM(BaiChuanConfig())

    model_parameters = filter(lambda p: p.requires_grad, model.parameters())
    model_engine, _, _, _ = deepspeed.initialize(args=args,
                                                 model=model,
                                                 optimizer=None,
                                                 model_parameters=model_parameters)
    return model_engine


def train(data_engine, model_engine):
    model_engine.train()
    step = 0
    while step < args.steps_per_epoch:
        data = data_engine.get_data()
        loss = model_engine(data, labels=data).loss
        model_engine.backward(loss)
        model_engine.step()
        step += 1
    return


if __name__ == "__main__":
    data_engine = prepare_data()
    model_engine = prepare_model()
    epoch = 0
    while True:
        train(data_engine, model_engine)
        epoch += 1
        model_engine.save_checkpoint(f"{args.checkpoint_saving_path}",
                                     tag=f"Epoch-{epoch}")


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/fulltune/trainner.py
import json
import os

import deepspeed
import deepspeed.comm as dist
import numpy as np
import sentencepiece as spm
import torch
from torch.utils.data import Dataset
from pyjava import RayContext

from transformers import Trainer, TrainingArguments,default_data_collator

from .base_model.configuration_baichuan import BaiChuanConfig
from .base_model.modeling_baichuan import BaiChuanForCausalLM
from . import TrainParameters
from ray.train.huggingface import TransformersTrainer
from ray.air.config import ScalingConfig
from ray.air import session

deepspeed_confg = json.loads('''
    {
  "gradient_accumulation_steps": 1,
  "train_micro_batch_size_per_gpu": 1,
  "prescale_gradients": false,
  "zero_allow_untested_optimizer": true,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 1e-8,
      "eps": 1.0e-8,
      "betas": [
        0.9,
        0.95
      ],
      "weight_decay": 0.1
    }
  },
  "tensorboard": {
    "enabled": true,
    "output_path": "logs/",
    "job_name": "baichuan-7b-pt"
  },
  "zero_optimization": {
    "stage": 3,
    "contiguous_gradients": false,
    "allgather_bucket_size": 3e8,
    "reduce_bucket_size": 3e8,
    "overlap_comm": true,
    "reduce_scatter": true
  },
  "steps_per_print": 16,
  "gradient_clipping": 1.0,
  "wall_clock_breakdown": true,
  "bf16": {
    "enabled": true
  }
}''')
                             
class DataEngine():
    def __init__(self, data_dir, tokenizer_path, micro_batch_size, max_length):
        self.MIN_TEXT_LEN = 20
        self.EOS_TOKEN_ID = 2
        self.data_dir = data_dir
        self.sp = spm.SentencePieceProcessor()
        self.sp.Load(tokenizer_path)
        self.micro_batch_size = micro_batch_size
        self.max_length = max_length
        self.data = []
        self.global_input_paths = [self.data_dir + "/" + x
                                   for x in os.listdir(self.data_dir)]
        self.local_input_paths = [x for i, x in enumerate(self.global_input_paths)]

    def load_data(self):
        for file_path in self.local_input_paths:
            data = []
            with open(file_path, encoding="utf-8", errors="ignore") as f:
                for line_id, line in enumerate(f):
                    line_json = json.loads(line.strip())
                    text = line_json("instruction") + line_json("input") + line_json("output")
                    cc = self.sp.EncodeAsIds(text) + [self.EOS_TOKEN_ID]
                    if len(cc) < self.MIN_TEXT_LEN:
                        cc = []
                    data.extend(cc)
                    if len(data) >= self.micro_batch_size * (self.max_length + 1):
                        index = self.micro_batch_size * (self.max_length + 1)
                        self.data.append(data[:index])
                        data = []
        return

    def get_data(self):
        data = self.data.pop(0)
        seq = np.asarray(data).reshape(self.micro_batch_size, self.max_length + 1)
        data = torch.LongTensor(seq)
        # data = data.cuda(non_blocking=True)
        return data                             
                             
class FulltuneDataset(Dataset):

    def __init__(self,data_engine:DataEngine) -> None:
        self.data_engine = data_engine


    def __len__(self):
        return len(self.data_engine.data)

    def __getitem__(self, index):                
        input_ids = self.data_engine.get_data()
        inputs = {
            'input_ids': input_ids            
        }
        return inputs                       


def trainer_init_per_worker(train_dataset, eval_dataset=None, **config):
    # Use the actual number of CPUs assigned by Ray
    os.environ["OMP_NUM_THREADS"] = str(
        session.get_trial_resources().bundles[-1].get("CPU", 1)
    )
    # Enable tf32 for better performance
    # torch.backends.cuda.matmul.allow_tf32 = True

    batch_size = config.get("batch_size", 4)
    epochs = config.get("epochs", 2)
    warmup_steps = config.get("warmup_steps", 0)
    learning_rate = config.get("learning_rate", 0.00002)
    weight_decay = config.get("weight_decay", 0.01)

    print("Preparing training arguments")
    training_args = TrainingArguments(
        "output",
        per_device_train_batch_size=batch_size,
        logging_steps=1,
        save_strategy="no",
        per_device_eval_batch_size=batch_size,
        learning_rate=learning_rate,
        weight_decay=weight_decay,
        warmup_steps=warmup_steps,
        label_names=["input_ids", "attention_mask"],
        num_train_epochs=epochs,
        push_to_hub=False,
        disable_tqdm=True,  # declutter the output a little
        fp16=True,
        gradient_checkpointing=True,
        deepspeed=deepspeed_confg,
    )
    
    data_refs = config["data_refs"]
    train_args = config["train_args"] 
    index = 0 # dist.get_rank()
        
    if not os.path.exists(os.path.join(train_args.data_dir,str(index))):
        os.makedirs(os.path.join(train_args.data_dir,str(index)))
    
    data_file_path = os.path.join(train_args.data_dir,str(index),"data.json")
    with open(data_file_path,"w",encoding="utf-8") as f:        
        for item in RayContext.collect_from([data_refs[index]]):
            f.write(json.dumps(item,ensure_ascii=False)+"\n")
                                
    micro_batch_size =  deepspeed_confg["train_micro_batch_size_per_gpu"]
    data_engine = DataEngine(train_args.data_dir, train_args.tokenizer_path, micro_batch_size, train_args.max_length)
    data_engine.load_data()   
    fulltune_dataset = FulltuneDataset(data_engine) 

    os.remove(data_file_path)

    model = BaiChuanForCausalLM(BaiChuanConfig())

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=fulltune_dataset        
    )
    return trainer



def distribute_train(args:TrainParameters,data_refs):

    assert  args.num_workers == len(data_refs), f"num_workers({args.num_workers}) must equal to data_refs({len(data_refs)})"    

    distribute_trainer = TransformersTrainer(
                                trainer_init_per_worker=trainer_init_per_worker,
                                trainer_init_config={
                                    "batch_size": 16,  # per device
                                    "epochs": 1,
                                    "data_refs":data_refs,
                                    "train_args":args
                                },
                                scaling_config=ScalingConfig(
                                    num_workers=args.num_workers,
                                    use_gpu=args.use_gpu,
                                    resources_per_worker={"GPU": args.gpus_per_worker, "CPU": args.cpus_per_worker},
                                )                                                                                               
    ) 
    r = distribute_trainer.fit()
    return r.path()



##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/fulltune/__init__.py

import os
import ray
from datetime import datetime
import uuid
import json
from typing import Dict, Any,List,Generator
from ray.util.client.common import ClientObjectRef
from pyjava.storage import streaming_tar as STar
from pyjava.api.mlsql import DataServer
from byzerllm import BlockRow
from  .deepspeed_trainner import distribute_train,TrainParameters
from pyjava.udf.store import transfer_to_ob
from byzerllm import consume_model


def sfft_train(data_refs:List[DataServer],train_params:Dict[str,str],sys_conf: Dict[str, str])->Generator[BlockRow,Any,Any]:    
    localPathPrefix = train_params.get("localPathPrefix","/tmp/byzerllm")
    
    current_time = datetime.now()
    formatted_time = current_time.strftime("%Y-%m-%d-%H-%M-%S")
    rd = f"sft-{formatted_time}-{str(uuid.uuid4())}"

    sfft_name = train_params["name"] if "name" in train_params else f"sfft-{sys_conf['OWNER']}"
    
    model_dir = os.path.join(localPathPrefix,rd,"pretrained_model")
    
    model_refs = []

    if "localModelDir" in train_params:
        model_dir = train_params["localModelDir"]
        consume_model(sys_conf)
    else:    
        transfer_to_ob(sfft_name,sys_conf,model_refs)    
           
    output_dir = os.path.join(localPathPrefix,rd,"finetune_model")
    data_dir = os.path.join(localPathPrefix,rd,"finetune_data")
    config_dir = os.path.join(localPathPrefix,rd,"config_dir")    

    print(f'''
name: {sfft_name}
model_dir: {model_dir}
output_dir: {output_dir}
data_dir: {data_dir}
config_dir: {config_dir}
          ''')
    
    train_params_sfft = {}
    
    for k,v in train_params.items():
        if k.startswith("sfft."):
            # sft.float.num_train_epochs
            tpe = k.split(".")[1]
            new_k = k.split(".")[2]
            new_v = v
            if tpe == "float":
              new_v = float(v)
            elif tpe == "int":
                new_v = int(v)
            elif tpe == "bool":
                new_v = v == "true"
            elif tpe == "str":
                new_v = v
            elif tpe == "list":
                new_v = json.loads(v)
            elif tpe == "dict":
                new_v = json.loads(v)            
            train_params_sfft[new_k] = new_v

    new_model_refs = distribute_train(TrainParameters(
        name=sfft_name,
        data_dir=data_dir,
        config_dir=config_dir,
        tokenizer_path=model_dir,
        model_dir=model_dir,
        checkpoint_saving_path=output_dir,
        model_refs = model_refs,
        ** train_params_sfft       
    ),data_refs)

    return STar.build_rows_from_file((ray.get(item) for item in new_model_refs))

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/fulltune/base_model/configuration_baichuan.py
# Copyright 2023 Baichuan Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from transformers.configuration_utils import PretrainedConfig
from transformers.utils import logging


logger = logging.get_logger(__name__)


class BaiChuanConfig(PretrainedConfig):
    model_type = "baichuan"
    keys_to_ignore_at_inference = ["past_key_values"]

    def __init__(
        self,
        vocab_size=64000,
        hidden_size=4096,
        intermediate_size=11008,
        num_hidden_layers=32,
        num_attention_heads=32,
        hidden_act="silu",
        max_position_embeddings=4096,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        use_cache=True,
        pad_token_id=0,
        bos_token_id=1,
        eos_token_id=2,
        tie_word_embeddings=False,
        **kwargs,
    ):
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.hidden_act = hidden_act
        self.initializer_range = initializer_range
        self.rms_norm_eps = rms_norm_eps
        self.use_cache = use_cache
        super().__init__(
            pad_token_id=pad_token_id,
            bos_token_id=bos_token_id,
            eos_token_id=eos_token_id,
            tie_word_embeddings=tie_word_embeddings,
            **kwargs,
        )


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/fulltune/base_model/modeling_baichuan.py
# Copyright 2023 Baichuan Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math
from typing import List, Optional, Tuple, Union

import torch
from torch import nn
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
import torch.utils.checkpoint
from transformers import PreTrainedModel, add_start_docstrings
from transformers.activations import ACT2FN
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from transformers.modeling_outputs import SequenceClassifierOutputWithPast
from transformers.utils import logging, add_start_docstrings_to_model_forward, replace_return_docstrings
from xformers import ops as xops

from .configuration_baichuan import BaiChuanConfig

logger = logging.get_logger(__name__)


# Copied from transformers.models.bart.modeling_bart._make_causal_mask
def _make_causal_mask(
        input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0
):
    """
    Make causal mask used for bi-directional self-attention.
    """
    bsz, tgt_len = input_ids_shape
    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)
    mask_cond = torch.arange(mask.size(-1), device=device)
    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)
    mask = mask.to(dtype)

    if past_key_values_length > 0:
        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)
    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)


# Copied from transformers.models.bart.modeling_bart._expand_mask
def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):
    """
    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.
    """
    bsz, src_len = mask.size()
    tgt_len = tgt_len if tgt_len is not None else src_len

    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)

    inverted_mask = 1.0 - expanded_mask

    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)


class RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)

        # convert into half-precision if necessary
        if self.weight.dtype in [torch.float16, torch.bfloat16]:
            hidden_states = hidden_states.to(self.weight.dtype)

        return self.weight * hidden_states


class RotaryEmbedding(torch.nn.Module):
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))
        self.register_buffer("inv_freq", inv_freq)

        # Build here to make `torch.jit.trace` work.
        self.max_seq_len_cached = max_position_embeddings
        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos()[None, None, :, :], persistent=False)
        self.register_buffer("sin_cached", emb.sin()[None, None, :, :], persistent=False)

    def forward(self, x, seq_len=None):
        # x: [bs, num_attention_heads, seq_len, head_size]
        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.
        if seq_len > self.max_seq_len_cached:
            self.max_seq_len_cached = seq_len
            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            freqs = torch.einsum("i,j->ij", t, self.inv_freq)
            # Different from paper, but it uses a different permutation in order to obtain the same calculation
            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            self.register_buffer("cos_cached", emb.cos()[None, None, :, :], persistent=False)
            self.register_buffer("sin_cached", emb.sin()[None, None, :, :], persistent=False)
        return (
            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
        )


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2:]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.
    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]
    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]
    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


class MLP(nn.Module):
    def __init__(
            self,
            hidden_size: int,
            intermediate_size: int,
            hidden_act: str,
    ):
        super().__init__()
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.act_fn = ACT2FN[hidden_act]

    def forward(self, x):
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))


class Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: BaiChuanConfig):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.max_position_embeddings = config.max_position_embeddings

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )
        self.W_pack = nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=False)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        self.rotary_emb = RotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)
        self.cos, self.sin = None, None

    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()

    def forward(
            self,
            hidden_states: torch.Tensor,
            attention_mask: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.LongTensor] = None,
            past_key_value: Optional[Tuple[torch.Tensor]] = None,
            output_attentions: bool = False,
            use_cache: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()

        proj = self.W_pack(hidden_states)
        proj = proj.unflatten(-1, (3, self.hidden_size)).unsqueeze(0).transpose(0, -2).squeeze(-2)

        if self.training:  # for training
            query_states = proj[0].view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
            key_states = proj[1].view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
            value_states = proj[2].view(bsz, q_len, self.num_heads, self.head_dim)

            kv_seq_len = key_states.shape[-2]
            cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

            query_states = query_states.transpose(1, 2)
            key_states = key_states.transpose(1, 2)

            attn_output = xops.memory_efficient_attention(
                query_states, key_states, value_states,
                attn_bias=xops.LowerTriangularMask()
            )
            attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
            attn_output = self.o_proj(attn_output)
            return attn_output, None, None

        else:  # for inference
            query_states = proj[0].view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
            key_states = proj[1].view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
            value_states = proj[2].view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)

            kv_seq_len = key_states.shape[-2]
            if past_key_value is not None:
                kv_seq_len += past_key_value[0].shape[-2]
            cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

            if past_key_value is not None:
                key_states = torch.cat([past_key_value[0], key_states], dim=2)
                value_states = torch.cat([past_key_value[1], value_states], dim=2)

            past_key_value = (key_states, value_states) if use_cache else None
            attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

            if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
                raise ValueError(
                    f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
                    f" {attn_weights.size()}"
                )

            if attention_mask is not None:
                if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
                    raise ValueError(
                        f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
                    )
                attn_weights = attn_weights + attention_mask
                attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))

            # upcast attention to fp32
            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
            attn_output = torch.matmul(attn_weights, value_states)

            if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
                raise ValueError(
                    f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                    f" {attn_output.size()}"
                )

            attn_output = attn_output.transpose(1, 2)
            attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
            attn_output = self.o_proj(attn_output)

            if not output_attentions:
                attn_weights = None

            return attn_output, attn_weights, past_key_value


class DecoderLayer(nn.Module):
    def __init__(self, config: BaiChuanConfig):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.self_attn = Attention(config=config)
        self.mlp = MLP(
            hidden_size=self.hidden_size,
            intermediate_size=config.intermediate_size,
            hidden_act=config.hidden_act,
        )
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
            self,
            hidden_states: torch.Tensor,
            attention_mask: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.LongTensor] = None,
            past_key_value: Optional[Tuple[torch.Tensor]] = None,
            output_attentions: Optional[bool] = False,
            use_cache: Optional[bool] = False,
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            use_cache (`bool`, *optional*):
                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
                (see `past_key_values`).
            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
        """

        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights,)

        if use_cache:
            outputs += (present_key_value,)

        return outputs


class PreTrainedModel(PreTrainedModel):
    config_class = BaiChuanConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["DecoderLayer"]
    _keys_to_ignore_on_load_unexpected = [r"decoder\.version"]

    def _init_weights(self, module):
        std = self.config.initializer_range
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()

    def _set_gradient_checkpointing(self, module, value=False):
        if isinstance(module, Model):
            module.gradient_checkpointing = value


class Model(PreTrainedModel):
    """
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`DecoderLayer`]

    Args:
        config: BaiChuanConfig
    """

    def __init__(self, config: BaiChuanConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.num_hidden_layers)])
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        self.gradient_checkpointing = False
        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value

    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask
    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):
        # create causal mask
        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
        combined_attention_mask = None
        if input_shape[-1] > 1:
            combined_attention_mask = _make_causal_mask(
                input_shape,
                inputs_embeds.dtype,
                device=inputs_embeds.device,
                past_key_values_length=past_key_values_length,
            )

        if attention_mask is not None:
            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(
                inputs_embeds.device
            )
            combined_attention_mask = (
                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask
            )

        return combined_attention_mask

    def forward(
            self,
            input_ids: torch.LongTensor = None,
            attention_mask: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.LongTensor] = None,
            past_key_values: Optional[List[torch.FloatTensor]] = None,
            inputs_embeds: Optional[torch.FloatTensor] = None,
            use_cache: Optional[bool] = None,
            output_attentions: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # retrieve input_ids and inputs_embeds
        if input_ids is not None and inputs_embeds is not None:
            raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
        elif input_ids is not None:
            batch_size, seq_length = input_ids.shape
        elif inputs_embeds is not None:
            batch_size, seq_length, _ = inputs_embeds.shape
        else:
            raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")

        seq_length_with_past = seq_length
        past_key_values_length = 0

        if past_key_values is not None:
            past_key_values_length = past_key_values[0][0].shape[2]
            seq_length_with_past = seq_length_with_past + past_key_values_length

        if position_ids is None:
            device = input_ids.device if input_ids is not None else inputs_embeds.device
            position_ids = torch.arange(
                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
            )
            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
        else:
            position_ids = position_ids.view(-1, seq_length).long()

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)
        # embed positions
        if attention_mask is None:
            attention_mask = torch.ones(
                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device
            )
        attention_mask = self._prepare_decoder_attention_mask(
            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
        )

        hidden_states = inputs_embeds

        if self.gradient_checkpointing and self.training:
            if use_cache:
                logger.warning_once(
                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                )
                use_cache = False

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        next_decoder_cache = () if use_cache else None

        for idx, decoder_layer in enumerate(self.layers):
            if output_hidden_states:
                all_hidden_states += (hidden_states,)

            past_key_value = past_key_values[idx] if past_key_values is not None else None

            if self.gradient_checkpointing and self.training:

                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        # None for past_key_value
                        return module(*inputs, output_attentions, None)

                    return custom_forward

                layer_outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(decoder_layer),
                    hidden_states,
                    attention_mask,
                    position_ids,
                    None,
                )
            else:
                layer_outputs = decoder_layer(
                    hidden_states,
                    attention_mask=attention_mask,
                    position_ids=position_ids,
                    past_key_value=past_key_value,
                    output_attentions=output_attentions,
                    use_cache=use_cache,
                )

            hidden_states = layer_outputs[0]

            if use_cache:
                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        # add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        next_cache = next_decoder_cache if use_cache else None
        if not return_dict:
            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=next_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )


class BaiChuanForCausalLM(PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.model = Model(config)

        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    def forward(
            self,
            input_ids: torch.LongTensor = None,
            attention_mask: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.LongTensor] = None,
            past_key_values: Optional[List[torch.FloatTensor]] = None,
            inputs_embeds: Optional[torch.FloatTensor] = None,
            labels: Optional[torch.LongTensor] = None,
            use_cache: Optional[bool] = None,
            output_attentions: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            return_dict: Optional[bool] = None,
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        r"""
        Args:
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

        Returns:

        Example:

        ```python
        >>> from transformers import AutoTokenizer, ModelForCausalLM

        >>> model = ModelForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)

        >>> prompt = "Hey, are you consciours? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you consciours? Can you talk to me?\nI'm not consciours, but I can talk to you."
        ```"""

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        hidden_states = outputs[0]
        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)
            loss = loss_fct(shift_logits, shift_labels)

        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

    def prepare_inputs_for_generation(
            self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs
    ):
        if past_key_values:
            input_ids = input_ids[:, -1:]

        position_ids = kwargs.get("position_ids", None)
        if attention_mask is not None and position_ids is None:
            # create position_ids on the fly for batch generation
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
            if past_key_values:
                position_ids = position_ids[:, -1].unsqueeze(-1)

        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step
        if inputs_embeds is not None and past_key_values is None:
            model_inputs = {"inputs_embeds": inputs_embeds}
        else:
            model_inputs = {"input_ids": input_ids}

        model_inputs.update(
            {
                "position_ids": position_ids,
                "past_key_values": past_key_values,
                "use_cache": kwargs.get("use_cache"),
                "attention_mask": attention_mask,
            }
        )
        return model_inputs

    @staticmethod
    def _reorder_cache(past_key_values, beam_idx):
        reordered_past = ()
        for layer_past in past_key_values:
            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)
        return reordered_past


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/fulltune/pretrain/convert_to_transformers.py
from deepspeed.utils import zero_to_fp32
from typing import NoReturn
import torch
import os
import ray
from transformers import WEIGHTS_NAME, CONFIG_NAME
from transformers import AutoTokenizer,AutoModelForCausalLM

class DeepSpeedConvert(object):
    def convert_deepspeed_checkpoint_to_transformers(sefl,model_dir:str,                                                 
                                                    checkpoint_dir:str, 
                                                    output_dir:str,tag=None) -> NoReturn:
        """
        Convert the model to transformers format.
        model: you can create it like this, BaiChuanForCausalLM(BaiChuanConfig()) 
        tokenizer_dir: here we can read and then save to the output_dir
        checkpoint_dir: the deepspeed checkpoint
        tag: epoch directory. for example 'Epoch-1'
        """    

        temp_dir = os.path.join(output_dir,"temp")   

        if not os.path.exists(temp_dir):
            os.makedirs(temp_dir)

        temp_pytorch_model_file = os.path.join(output_dir,"temp",WEIGHTS_NAME)
        zero_to_fp32.convert_zero_checkpoint_to_fp32_state_dict(checkpoint_dir, temp_pytorch_model_file, tag=tag)

        model = AutoModelForCausalLM.from_pretrained(model_dir)
        model.load_state_dict(torch.load(temp_pytorch_model_file))
        model.save_pretrained(output_dir)
        
        tokenizer = AutoTokenizer.from_pretrained(model_dir)
        tokenizer.save_pretrained(output_dir)

def convert(train_params,sys_conf):    
    model_dir = train_params["modelNameOrPath"]
    checkpoint_dir = train_params["checkpointDir"]
    output_dir = train_params["savePath"]
    tag = train_params["tag"]
    
    custom_resources = [(key.split("resource.")[1], float(sys_conf[key])) for key in
                            sys_conf.keys() if
                            key.startswith("resource.")]
    worker_conf = {}

    if len(custom_resources) > 0:
        worker_conf["resources"] = dict(custom_resources)
    merge_job_name = train_params["name"] if "name" in train_params else f"convert-deepspeed-{sys_conf['OWNER']}"
    worker = ray.remote(name=merge_job_name, **worker_conf)(DeepSpeedConvert).remote()
    ray.get(worker.convert_deepspeed_checkpoint_to_transformers.remote(model_dir=model_dir,
        checkpoint_dir=checkpoint_dir,
        output_dir=output_dir,
        tag = tag))
    return []    






##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/fulltune/pretrain/__init__.py
from typing import List, Optional, Tuple,Any,Dict,Callable,Generator
from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig
import ray
import torch
import deepspeed
import deepspeed.comm as dist
import sentencepiece as spm
import numpy as np
import datetime
import uuid
import json
import os
from pyjava.storage import streaming_tar as STar
from pyjava import RayContext
from pyjava.api.mlsql import DataServer
from byzerllm import BlockRow
from ray.air.util.torch_dist import (
    ActorHandle,
    _get_node_and_gpu_ids,
    _init_torch_distributed,
    get_address_and_port,
)
import dataclasses
from ... import print_flush
import shutil
from ray.train.constants import DEFAULT_NCCL_SOCKET_IFNAME

DEFUALT_CONFIG = '''
{
  "gradient_accumulation_steps": 1,
  "train_micro_batch_size_per_gpu": 1,
  "prescale_gradients": false,
  "zero_allow_untested_optimizer": true,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 1e-8,
      "eps": 1.0e-8,
      "betas": [
        0.9,
        0.95
      ],
      "weight_decay": 0.1
    }
  },
  "tensorboard": {
    "enabled": true    
  },
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {
         "device": "cpu"         
     },           
    "offload_param": {
         "device": "cpu"
    },
    "contiguous_gradients": true,
    "allgather_bucket_size": 1e8,
    "reduce_bucket_size": 1e8,
    "overlap_comm": true,
    "reduce_scatter": true
  },
  "steps_per_print": 16,
  "gradient_clipping": 1.0,
  "wall_clock_breakdown": true,
  "bf16": {
    "enabled": true
  }
}
'''

@dataclasses.dataclass
class TrainArgs:
    model_path: str = "" 
    tokenizer_path: str = ""
    sft_name: str = ""
    steps_per_epoch: int = 4096
    is_partition_data: bool = False
    epoches:int = 1
    checkpoint_saving_path: str = "/home/byzerllm/data/checkpoints"
    max_length: int = 4096
    data_dir: str = "/home/byzerllm/data/raw_data"
    data_mode: str = "auto"
     

@dataclasses.dataclass
class DeviceID:
    node_id: int
    gpu_ids: List[int]
    rank: int

class DataEngine():
    def __init__(self, data_dir, tokenizer_path, micro_batch_size, max_length,world_size,rank):
        self.MIN_TEXT_LEN = 20
        self.EOS_TOKEN_ID = 2
        self.data_dir = data_dir
        self.sp = spm.SentencePieceProcessor()
        self.sp.Load(tokenizer_path)
        self.micro_batch_size = micro_batch_size
        self.max_length = max_length
        self.data = []
        self.global_input_paths = [self.data_dir + "/" + x
                                   for x in os.listdir(self.data_dir)]
        self.local_input_paths = [x for i, x in
                                  enumerate(self.global_input_paths)
                                  if i % world_size == rank]

    def load_data(self):
        for file_path in self.local_input_paths:
            data = []
            with open(file_path, encoding="utf-8", errors="ignore") as f:
                for line_id, line in enumerate(f):
                    cc = self.sp.EncodeAsIds(line.strip()) + [self.EOS_TOKEN_ID]
                    if len(cc) < self.MIN_TEXT_LEN:
                        cc = []
                    data.extend(cc)
                    if len(data) >= self.micro_batch_size * (self.max_length + 1):
                        index = self.micro_batch_size * (self.max_length + 1)
                        self.data.append(data[:index])
                        data = []
        return
    
    def reset(self):
        self.data = []
        self.load_data()

    def get_data(self):
        data = self.data.pop(0)
        seq = np.asarray(data).reshape(self.micro_batch_size, self.max_length + 1)
        data = torch.LongTensor(seq)
        data = data.cuda(non_blocking=True)
        return data

class ParallelConfig:
    """Configuration for the distributed execution.    
    """

    def __init__(
        self,
        num_workers:int,            
        get_model:Callable[[str,Dict],Any],        
        ds_config:Dict[Any,Any],         
        data_refs:List[DataServer] = [],
        train_args = TrainArgs(),            
        backend: str = "nccl",  
        setup_nccl_socket_ifname_by_ip:bool = False
    ) -> None:
        self.world_size = num_workers        
        self.backend = backend
        self.ds_config = ds_config if ds_config else json.loads(DEFUALT_CONFIG)
        self.train_args = train_args  
        self.get_model = get_model 
        self.data_refs = data_refs 
        # if the nodes in cluster  have different network interface name, we need to set the NCCL_SOCKET_IFNAME 
        # manually otherwise you may meet the following error in deepspeed:
        # torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1275, 
        # internal error, NCCL version 2.14.3
        # ncclInternalError: Internal check failed.
        # Last error:
        # Proxy Call to rank 8 failed (Connect)
        self.setup_nccl_socket_ifname_by_ip = setup_nccl_socket_ifname_by_ip    
    
def _init_distributed_environment(
        parallel_config: ParallelConfig,
        rank: int,
        distributed_init_method: str        
    ) -> None:        
        if parallel_config.backend == "nccl":
            # Same as in Ray Train
            os.environ["NCCL_ASYNC_ERROR_HANDLING"] = "1"
            # All workers on a same node should share the same set of
            # visible GPUs. Otherwise they can't talk among themselves.
            # os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(str(gid) for gid in gpu_ids)
            if "NCCL_SOCKET_IFNAME" not in os.environ:
                os.environ["NCCL_SOCKET_IFNAME"] = DEFAULT_NCCL_SOCKET_IFNAME

        # os.environ["RANK"] = str(rank)
        # os.environ["LOCAL_RANK"] = str(rank)
        # os.environ["WORLD_SIZE"] = str(parallel_config.world_size)
        # os.environ["LOCAL_WORLD_SIZE"] = str(parallel_config.world_size)        
        print(f'''
deepspeed worker config:
              RANK:{rank} 
              WORLD_SIZE:{parallel_config.world_size}
              CUDA_VISIBLE_DEVICES:{os.environ["CUDA_VISIBLE_DEVICES"]} 
              LOCAL_RANK:{os.environ["LOCAL_RANK"]}
              LOCAL_WORLD_SIZE:{os.environ["LOCAL_WORLD_SIZE"]}
              NCCL_SOCKET_IFNAME：{os.environ["NCCL_SOCKET_IFNAME"]}
''',flush=True) 
        # torch.cuda.set_device(rank)
        """Initialize the distributed environment."""
        deepspeed.init_distributed(
            dist_backend="nccl",
            auto_mpi_discovery=False,
            verbose=True,
            init_method=distributed_init_method,
            rank=rank,
            world_size=parallel_config.world_size,
        )
        # torch.distributed.init_process_group(
        #     backend="nccl",
        #     world_size=parallel_config.world_size,
        #     rank=rank,
        #     init_method=distributed_init_method,            
        # )
        # # A small all_reduce for warmup.
        # torch.distributed.all_reduce(torch.zeros(1).cuda())

        

class ResourceWorker:
    def __init__(
        self,        
        parallel_config: ParallelConfig,        
        rank: int               
    ) -> None:
        self.parallel_config = parallel_config        
        self.rank = rank        
        self.ds_config = self.parallel_config.ds_config

    def get_node_and_gpu_ids(self):
        """Returns the node and GPU ids of the current worker."""
        node_id, gpu_ids = _get_node_and_gpu_ids()
        return DeviceID(node_id, gpu_ids, self.rank)  

    def rank(self):
        return self.rank  
    
    def get_node_ip_address(self):
        return ray.util.get_node_ip_address()
    
    def get_address_and_port(self):
        return get_address_and_port()
    
    def get_network_interface(self):
        import netifaces
        interfaces = netifaces.interfaces()
        target_iface = ""
        for iface in interfaces:
            addrs = netifaces.ifaddresses(iface)
            if netifaces.AF_INET in addrs:
                ip = addrs[netifaces.AF_INET][0]['addr']
                address = self.get_node_ip_address()
                if ip == address:
                    target_iface = iface
                    break
        return target_iface        


class Worker:
    
    def __init__(
        self,        
        parallel_config: ParallelConfig,        
        rank: int,
        distributed_init_method:str                
       
    ) -> None:
        self.parallel_config = parallel_config        
        self.rank = rank        
        self.ds_config = self.parallel_config.ds_config
        self.get_model = self.parallel_config.get_model
        self.distributed_init_method = distributed_init_method 
        self.data_dir = os.path.join(self.parallel_config.train_args.data_dir,f"data-{self.rank}") 
        
        # if the data is not from data_refs(from Byzer) , it may
        # means that the data is prepared in every node before run the training.
        # we just respect the data_dir provied by the user.                
        if not self.parallel_config.data_refs:                  
            self.data_dir = self.parallel_config.train_args.data_dir

        self.model = None
        self.tokenizer = None
        self.tensorboard_pid = None        
    
    def get_node_and_gpu_ids(self):
        """Returns the node and GPU ids of the current worker."""
        node_id, gpu_ids = _get_node_and_gpu_ids()
        return DeviceID(node_id, gpu_ids, self.rank)        
    
    def setup_tensorboard(self)->Optional[Tuple[str,int]]:
        #         "tensorboard": {
        #     "enabled": true,
        #     "output_path": "/home/byzerllm/data/train_ck/logs/",
        #     "job_name": "7b-pt"
        # },
        tensorboard_config = self.ds_config.get("tensorboard", {"enabled":False})
        if tensorboard_config["enabled"]:   
            import subprocess               
            ip, port = get_address_and_port()
            log_dir = tensorboard_config["output_path"]
            job_name = tensorboard_config["job_name"]
            log_dir = os.path.join(log_dir,job_name)
            if not os.path.exists(log_dir):
                os.makedirs(log_dir)
            tb_process = subprocess.Popen(['tensorboard', '--logdir', log_dir,"--port",str(port),"--host",ip], stdout=subprocess.PIPE, stderr=subprocess.PIPE)                                    
            self.tensorboard_pid = tb_process.pid
            return (ip,port)
        return None

    def _train(self,data_engine, model_engine):
        model_engine.train()
        step = 0
        while step < self.parallel_config.train_args.steps_per_epoch:
            data = data_engine.get_data()
            loss = model_engine(data, labels=data).loss
            model_engine.backward(loss)
            model_engine.step()
            step += 1
        return  

    def get_checkpoint(self):
        if self.rank == 0:
            sft_name = self.parallel_config.train_args.sft_name
            final_path = self.parallel_config.train_args.checkpoint_saving_path
            # get the last checkpoint
            # the checkpoint path is like this:
            # /home/byzerllm/data/sft-20230805-1224-30-173a8dca-9e4a-411c-9fcb-fc979e3460f6/finetune_model/Epoch-1
            # /home/byzerllm/data/sft-20230805-1224-30-173a8dca-9e4a-411c-9fcb-fc979e3460f6/finetune_model/Epoch-2
            # get the last one
            dirs = os.listdir(final_path)
            dirs.sort(key=lambda x: int(x.split("-")[-1]))
            final_path = os.path.join(final_path,dirs[-1])

            result = []
            count = 0
            print_flush(f"[{sft_name}] Store model({final_path}) to Ray object store")
            for item in STar.build_rows_from_file(final_path):
                if count % 1000 == 0:
                    print_flush(f"[{sft_name}] Progress: {count} processed")
                count += 1    
                result.append(ray.put(item))
            
            print_flush(f'''
                [{sft_name}] Train Actor already finished.
                [{sft_name}] It may take a while to transfer the model from Ray object store to delta lake. 
                [{sft_name}] Try to check the progress in Byzer console or Byzer Notebook. 
                ''')    
            return (result,count) 
        return None


    
    def train(self):        
        data_engine = self.prepare_data()
        model_engine = self.prepare_model()        
        epoch = 0
        while epoch < self.parallel_config.train_args.epoches:
            self._train(data_engine, model_engine)
            epoch += 1
            model_engine.save_checkpoint(f"{self.parallel_config.train_args.checkpoint_saving_path}",
                                        tag=f"Epoch-{epoch}")
            data_engine.reset()
            
    def prepare_data(self):        
        
        if self.parallel_config.data_refs: 
            if not os.path.exists(self.data_dir):
                os.makedirs(self.data_dir)

            train_file = os.path.join(self.data_dir,f"train.txt")
            
            '''
            simplely write data to text file
            may need to be think how to handle for new line if the conversation contains new line. This is because
            the data_engine will get data line by line util touch the limit of max_length, then this seuqence will be
            used to train the model.
            But since this is for pretraining, it should be fine.
            '''
            with open(train_file,"w") as f: 
                count = 0
                data_ref = self.parallel_config.data_refs[self.rank]
                print(f"Start to read data to {data_ref.host}:{data_ref.port}. target file:{train_file}",flush=True)
                for item in RayContext.collect_from([data_ref]):                
                    if "conversation" in item:
                        item["conversation"] = item["conversation"].tolist()
                        s =  " ".join(conversation)
                        f.write(s+"\n")                    
                    elif "history" in item:
                        # support alpaca format data
                        conversation = [sub.tolist() for sub in item["history"].tolist()]
                        conversation = [{"human":x[0],"assistant":x[1]} for x in conversation]
                        latest_conversation = [{"human":item["instruction"],"assistant":item["output"]}] if "instruction" in item and item["instruction"] else []
                        s = " ".join(conversation) + " ".join(latest_conversation)
                        f.write(s+"\n")
                    elif "text" in item:
                        f.write(item["text"]+"\n")
                    else:
                        raise Exception("Unknow data format")                             
                    count += 1         

        
        tokenizer_path = self.parallel_config.train_args.tokenizer_path        
        micro_batch_size = self.ds_config["train_micro_batch_size_per_gpu"]
        max_length = self.parallel_config.train_args.max_length

        world_size = 1 if self.parallel_config.train_args.is_partition_data else self.parallel_config.world_size
        rank = 0 if self.parallel_config.train_args.is_partition_data else self.rank

        data_engine = DataEngine(self.data_dir, tokenizer_path, micro_batch_size, max_length,world_size,rank)
        data_engine.load_data()
        return data_engine     
    
    def prepare_model(self):
        # Initialize the distributed environment.
        _init_distributed_environment(self.parallel_config, self.rank,
                                      self.distributed_init_method)
        
        # check the enabled parameter here: https://github.com/microsoft/DeepSpeed/issues/3234
        
        with deepspeed.zero.Init(config_dict_or_path=self.ds_config,
                             enabled=self.ds_config["zero_optimization"]["stage"] == 3,
                             mem_efficient_linear=False,
                             mpu=None):
            model = self.get_model()            
            model_parameters = filter(lambda p: p.requires_grad, model.parameters())
            model_engine, _, _, _ = deepspeed.initialize(model=model,
                                                         config=self.ds_config,
                                                        optimizer=None,
                                                        model_parameters=model_parameters)
            return model_engine
            
           

class DeepSpeedTrain:
    def __init__(self,parallel_config: ParallelConfig):    

        

        # get resource manually. If use num_gpus, the ray will set cuda_visible_devices automatically, and this
        # will cause the deepspeed can't get the right gpu_ids enven if we set the cuda_visible_devices manually.                
        resource_workers = [] 
        workers = []       
        
        for rank in range(parallel_config.world_size):    
            worker_cls = ResourceWorker                        
            worker_cls = ray.remote(
                        num_cpus=0,
                        num_gpus=1,
                        name=f"RW-{rank}-{parallel_config.train_args.sft_name}",
                        # resources={f"node:{master_addr}": 1e-3},
                        # runtime_env=runtime_env,                        
                    )(worker_cls).remote
            worker = worker_cls(parallel_config,rank)
            resource_workers.append(worker)

        master_addr, master_port = ray.get(resource_workers[0].get_address_and_port.remote())          
        distributed_init_method = f"tcp://{master_addr}:{master_port}"  
        print(f"deepspeed: master_addr:{master_addr},master_port:{master_port}",flush=True)

        self.resource_workers  = resource_workers        
        self.node_id_to_workers = {}
        self.node_id_to_gpus = {}
        self.node_id_to_nccl_socket_device = {}

        for resource_worker in self.resource_workers:
            device = ray.get(resource_worker.get_node_and_gpu_ids.remote())            
     
            if device.node_id not in self.node_id_to_workers:
                self.node_id_to_workers[device.node_id] = []
            
            if device.node_id not in self.node_id_to_gpus:
                self.node_id_to_gpus[device.node_id] = []    
            
            self.node_id_to_workers[device.node_id].append(resource_worker)    
            self.node_id_to_gpus[device.node_id].extend(device.gpu_ids)
            self.node_id_to_gpus[device.node_id].sort()

            if parallel_config.setup_nccl_socket_ifname_by_ip:
                self.node_id_to_nccl_socket_device[device.node_id] = ray.get(resource_worker.get_network_interface.remote())

        for node_id, resource_workers in self.node_id_to_workers.items():
            for local_rank,resource_worker in enumerate(resource_workers):
                rank = ray.get(resource_worker.rank.remote()) 
                worker_cls = Worker  
                gpu_ids = self.node_id_to_gpus[node_id]
                nccl_socket_ifname = DEFAULT_NCCL_SOCKET_IFNAME
                if parallel_config.setup_nccl_socket_ifname_by_ip:
                    nccl_socket_ifname = self.node_id_to_nccl_socket_device[node_id]
                env_vars = {
                        "RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES":"true",
                        "CUDA_VISIBLE_DEVICES": ",".join([str(gid) for gid in gpu_ids]),
                        "LOCAL_RANK": str(local_rank),
                        "RANK": str(rank),
                        "NCCL_SOCKET_IFNAME": f"={nccl_socket_ifname}",
                        "LOCAL_WORLD_SIZE": str(len(gpu_ids)),
                        "WORLD_SIZE": str(parallel_config.world_size)
                        }                                        
                runtime_env = {"env_vars": env_vars}  
                node_ip = ray.get(resource_worker.get_node_ip_address.remote())  
                worker_cls = ray.remote(
                            num_cpus=0, 
                            num_gpus=0,
                            name=f"W-{rank}-{parallel_config.train_args.sft_name}",
                            resources={f"node:{node_ip}": 1e-3},
                            runtime_env=runtime_env,                        
                        )(worker_cls).remote
                worker = worker_cls(parallel_config,rank,distributed_init_method)
                workers.append(worker)  
                if rank == 0:
                    addr_port = ray.get(worker.setup_tensorboard.remote())
                    if addr_port:
                        print(f"tensorboard: http://{addr_port[0]}:{addr_port[1]}",flush=True)

        self.workers = workers                              
    
              
    def _run_workers(
        self,
        method: str,
        *args,
        get_all_outputs: bool = False,
        **kwargs,
    ) -> Any:
        """Runs the given method on all workers."""
        all_outputs = []
        for worker in self.workers:
            executor = getattr(worker, method)
            executor = executor.remote
            output = executor(*args, **kwargs)
            all_outputs.append(output)

        all_outputs = ray.get(all_outputs)            
        
        if get_all_outputs:
            return all_outputs

        # Make sure all workers have the same results.
        output = all_outputs[0]
        for other_output in all_outputs[1:]:
            assert output == other_output
        return output 

class DeepSpeedTrainer:
    def __init__(self,name:str) -> None:
        self.sft_name = name
        self.dst = None 
        self.output_dir = None

    def get_checkpoint_path(self):
        return self.output_dir    

    def sfft_train(self,data_refs:List[DataServer],train_params:Dict[str,str],sys_conf: Dict[str, str]):                

        localPathPrefix = train_params.get("localPathPrefix","/tmp/byzerllm")

        sft_name = self.sft_name
        rd = f"{sft_name}-{str(uuid.uuid4())}"        

        num_gpus = int(sys_conf.get("num_gpus",0))
        
        assert num_gpus > 0, 'num_gpus must be greater than 0. Try to fix it with `!byzerllm setup "num_gpus=4"`'
        
        is_partition_data = len(data_refs) != 0

        if is_partition_data:
            assert num_gpus == len(data_refs), f'''The number of data refs({len(data_refs)}) must be equal to the number of GPUs({num_gpus}).
            Try to fix it with `!byzerllm setup "num_gpus={len(data_refs)}"` or repartition the data with the following command:
            
            ```
            run oldTable as TableRepartition.`` where partitionNum="{num_gpus}" as newTable;
            ```
            Notice that make sure Byzer engine have CPUs more than {num_gpus}.
            '''

        data_dir = train_params["localDataDir"] if "localDataDir" in train_params else os.path.join(localPathPrefix,rd,"finetune_data")
        output_dir = os.path.join(localPathPrefix,rd,"finetune_model")
        self.output_dir = output_dir
        tensorboard_dir = os.path.join(localPathPrefix,rd,"tensorboard_dir")
        model_dir = os.path.join(localPathPrefix,rd,"pretrained_model")
        
        if "localModelDir" in train_params:
            model_dir = train_params["localModelDir"]

        pretrained_model_type = train_params.get("pretrainedModelType","")
        if "/" in  pretrained_model_type:
            pretrained_model_type = pretrained_model_type.split("/")[-1]
        
        def get_model():
            if pretrained_model_type == "llama2":            
                return AutoModelForCausalLM.from_pretrained(model_dir,
                                                            trust_remote_code=True,
                                                            ignore_mismatched_sizes=True)
            else:
                return AutoModelForCausalLM.from_pretrained(model_dir,trust_remote_code=True)
        
        setup_nccl_socket_ifname_by_ip = False
        if "sfft.bool.setup_nccl_socket_ifname_by_ip" in train_params:
            setup_nccl_socket_ifname_by_ip = train_params["sfft.bool.setup_nccl_socket_ifname_by_ip"] == "true"
        
        tokenizer_path = train_params["sfft.str.tokenizer_path"] if "sfft.str.tokenizer_path" in train_params else f"{model_dir}/tokenizer.model"        
        max_length = int(train_params.get("sfft.int.max_length",4096))
        epoches = int(train_params.get("sfft.int.epoches",1))
        steps_per_epoch = int(train_params.get("sfft.int.steps_per_epoch",10))
        
        try:
            ds_config=  json.loads(train_params.get("deepspeedConfig",DEFUALT_CONFIG))
        except Exception as e:        
            print(f'deepspeedConfig is not a valid json string:\n{train_params.get("deepspeedConfig","{}")}',flush=True)
            print(f"Byzer-LLM will ues the default deepspeed config:\n{DEFUALT_CONFIG}",flush=True)
            ds_config = json.loads(DEFUALT_CONFIG)
            

        if "tensorboard"  in ds_config and  ds_config["tensorboard"].get("enabled",False):
            if "output_path" not in ds_config["tensorboard"]:
                ds_config["tensorboard"]["output_path"] = tensorboard_dir
                ds_config["tensorboard"]["job_name"] = sft_name

        print(f'''
    Train Configuration:
        pretrained_model_type:{pretrained_model_type} 
        model_dir:{model_dir} 
        output_dir:{output_dir}
        data_dir:{data_dir}
        is_partition_data:{is_partition_data}
        max_length:{max_length}
        epoches:{epoches}
        steps_per_epoch:{steps_per_epoch} 
        setup_nccl_socket_ifname_by_ip:{setup_nccl_socket_ifname_by_ip}   
        num_gpus:{num_gpus}            
            ''',flush=True)   
        

        dst = DeepSpeedTrain(ParallelConfig(
        data_refs = data_refs,
        num_workers = num_gpus,
        get_model = get_model,
        ds_config = ds_config,     
        setup_nccl_socket_ifname_by_ip = setup_nccl_socket_ifname_by_ip,
        train_args=TrainArgs(
            model_path=model_dir,
            tokenizer_path = tokenizer_path,
            data_dir = data_dir,  
            checkpoint_saving_path = output_dir,   
            steps_per_epoch = steps_per_epoch,
            max_length = max_length,
            epoches=epoches,
            is_partition_data = is_partition_data,
            sft_name=sft_name
            )
        ))

        self.dst = dst
        ray.get([worker.train.remote() for worker in dst.workers])
        if train_params.get("detached","true") == "true":
            return [],0
        chunks,count = ray.get(dst.workers[0].get_checkpoint.remote())
        return chunks,count


def sfft_train(data_refs:List[DataServer],train_params:Dict[str,str],sys_conf: Dict[str, str])->Generator[BlockRow,Any,Any]:
    
    current_time = datetime.datetime.now()
    formatted_time = current_time.strftime("%Y%m%d-%H-%M-%S")
    sft_name = train_params["name"] if "name" in train_params else f"sft-{sys_conf['OWNER']}-{formatted_time}"        

    detached = train_params.get("detached","true") == "true"
    options = {"name":sft_name}
    if detached:        
        options["lifetime"] = "detached"
                
    worker_cls = ray.remote(**options)(DeepSpeedTrainer).remote
    trainer = worker_cls(name=sft_name)

    if detached:
        print_flush(f"[{sft_name}] Detached mode is enabled. ")
        trainer.sfft_train.remote(data_refs,train_params,sys_conf)        
        return []
        
    chunks,obj_count = ray.get(trainer.sfft_train.remote(data_refs,train_params,sys_conf))    
    checkpoint_path = ray.get(trainer.get_checkpoint_path.remote())
    node = ray.get(trainer.dst.resource_workers[0].get_node_ip_address.remote())
    print_flush(f"The model is finised training, Please check the path: {node}:{checkpoint_path}")
    
    if obj_count == 0:    
        return []

    print_flush(f"[{sft_name}] Transform Model from Ray object store to new storage(delta lake), total refs: {obj_count}. ")
    count = 0
    for item in chunks:
        if count % 1000 == 0:
            print_flush(f"[{sft_name}] Process: {float(count)/obj_count*100}%")
        count += 1
        yield ray.get(item)



    

优化 base_ability_format 函数。
1. 新增一个 BaseAbility enum 类，包含 RESPONSE_WITH_CLASS,FUNCTION_CALLING,FUNCTION_IMPL,OTHERS 四种值
2. base_ability_format 新加一个参数 base_abilities:List[BaseAbility]，用于指定的功能类型.
3. 修改 base_ability_format 函数，根据 base_abilities 参数，返回不同的字符串。
    