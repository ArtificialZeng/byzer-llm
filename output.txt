
下面是一些文件路径以及每个文件对应的源码：

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/__init__.py
from pathlib import Path
from functools import wraps
import time
import json
import hashlib
import threading
from typing import TYPE_CHECKING,TypeVar,Dict, List, Optional, Union,Any,Tuple,get_type_hints,Annotated,get_args,Callable
import typing
from ray.util.client.common import ClientActorHandle, ClientObjectRef
import inspect
import pydantic
import sys
import traceback
import io

T = TypeVar("T")

def print_flush(*args, **kwargs):
    print(*args, **kwargs, flush=True)

import signal
from contextlib import contextmanager
class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException()

@contextmanager
def timeout(duration: float):
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(duration)
    try:
        yield
    finally:
        signal.alarm(0)

def timeit(func):
    """
    Decorator to time a function.
    """

    @wraps(func)
    def inner(*args, **kwargs):
        start_time = time.monotonic()
        ret = func(*args, **kwargs)
        time_taken = time.monotonic() - start_time
        print(f"{func} took {time_taken} s to complete",flush=True)
        return ret

    return inner

def generate_instruction_from_history(ins:str,his:List[Dict[str,str]],role_mapping:Dict[str,str]={        
        "user":"User",        
        "assistant":"Assistant",
    }):

    new_his = []    
    for item in his:
        if item["role"] == "system":
            new_his.append(item["content"])
            continue        
        new_his.append(f"{role_mapping[item['role']]}:{item['content']}")            

    # here we should make sure the user build the conversation string manually also
    # works. This means if the user do not provide  the history, then
    # we should treat ins as conversation string which the user build manually
    if len(new_his) > 0 and ins != "":
        new_his.append(f"{role_mapping['user']}:{ins}")
        new_his.append(f"{role_mapping['assistant']}:")

    if len(new_his) > 0 and ins == "":
        new_his.append(f"{role_mapping['assistant']}:")            
    
    if len(new_his) == 0:
        new_his.append(ins)    

    fin_ins = "\n".join(new_his)
    return fin_ins  

def compute_max_new_tokens(tokens,max_length:int):
    input_length = tokens["input_ids"].shape[1]
    max_new_tokens = max_length - input_length
    if max_new_tokens <= 0:
        raise Exception(f"Input is too long ({input_length}). Try to reduce the length of history or use a larger `max_length` value (now:{max_length})")
    return max_new_tokens

def tokenize_string(tokenizer, key: str) -> Union[int, List[int]]:
    """Tokenize a string using a tokenizer.

    Args:
        tokenizer (PreTrainedTokenizer): Tokenizer to use.
        key (str): String to tokenize.
    """
    token_ids = tokenizer.encode(key, add_special_tokens=False)
    return token_ids

def tokenize_stopping_sequences_where_needed(
    tokenizer,
    stopping_sequences: List[Union[str, int, List[int]]],
) -> List[Union[List[int], int]]:
    """If any sequence is a string, tokenize it.

    Args:
        tokenizer (PreTrainedTokenizer): Tokenizer to use.
        stopping_sequences (List[Union[str, int, List[int]]]): Stopping sequences to
            tokenize. Can be ids, sequences of ids or strings.
    """
    if not stopping_sequences:
        return None
    return [
        tokenize_string(tokenizer, sequence) if isinstance(sequence, str) else sequence
        for sequence in stopping_sequences
    ]

def  tokenize_stopping_sequences(tokenizer,stop_words):
    stop_words_ids = []
    for stop_word in stop_words:
        w = tokenize_string(tokenizer, stop_word)
        # remove the first token which is empty token 
        # this should work for only llama model
        # if w[0] == 29871 and tokenizer.decode([w[0]],skip_special_tokens=False) == "":
        #     w = w[1:]
        stop_words_ids.append(w)    
    return stop_words_ids



def load_json_str(json_str:str):        
    return json.loads(json_str) 


def generate_file_md5(file_path: str) -> str:
    md5_hash = hashlib.md5()
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b''):
            md5_hash.update(chunk)
    return md5_hash.hexdigest()

def generate_str_md5(s: str) -> str:
    md5_hash = hashlib.md5()
    md5_hash.update(s.encode("utf-8"))
    return md5_hash.hexdigest() 

class SingleOutputMeta:
    def __init__(self, input_tokens_count:int=0, generated_tokens_count:int=0):        
        self.input_tokens_count = input_tokens_count
        self.generated_tokens_count = generated_tokens_count    

class SingleOutput:
    def __init__(self, text:str,metadata:SingleOutputMeta=SingleOutputMeta()):
        self.text = text
        self.metadata = metadata
        
class StreamOutputs: 
    def __init__(self, outputs:List[SingleOutput]):
        self.outputs = outputs        

class BlockVLLMStreamServer:
    def __init__(self):
        self.cache = {}
        self.cache_status = {} 
        self.lock = threading.Lock()

    def add_item(self, request_id, item):
        with self.lock:            
            self.cache[request_id]=item
            self.cache_status[request_id]=int(time.time()*1000)
    
    def mark_done(self, request_id):
        if len(self.cache_status) > 30:
            now = int(time.time()*1000)
            with self.lock:
                for k in list(self.cache_status.keys()):
                    if now - self.cache_status[k] > 10*60*60*1000:
                        del self.cache_status[k]
                        del self.cache[k] 
        with self.lock:            
            self.cache_status[request_id] = 0

    def get_item(self, request_id):                
        with self.lock:
            v = self.cache.get(request_id, None)     
            if request_id in self.cache_status and self.cache_status[request_id] == 0:
                del self.cache[request_id]
                del self.cache_status[request_id]
            return v     

class VLLMStreamServer:
    def __init__(self):
        self.cache = {}
        self.cache_status = {} 
        self.lock = threading.Lock()

    async def add_item(self, request_id, item):
        with self.lock:            
            self.cache[request_id]=item
            self.cache_status[request_id]=int(time.time()*1000)
    
    async def mark_done(self, request_id):
        if len(self.cache_status) > 30:
            now = int(time.time()*1000)
            with self.lock:
                for k in list(self.cache_status.keys()):
                    if now - self.cache_status[k] > 10*60*60*1000:
                        del self.cache_status[k]
                        del self.cache[k] 
        with self.lock:            
            self.cache_status[request_id] = 0

    async def get_item(self, request_id):                
        with self.lock:
            v = self.cache.get(request_id, None)     
            if request_id in self.cache_status and self.cache_status[request_id] == 0:
                del self.cache[request_id]
                del self.cache_status[request_id]
            return v
        
def get_type_name(t):
    name = str(t)
    if "list" in name or "dict" in name:
        return name
    else:
        return t.__name__
    
def is_annotated_type(hint):
    if hasattr(typing, '_AnnotatedAlias'):  # Python 3.9 and later
        return isinstance(hint, typing._AnnotatedAlias)
    elif hasattr(typing, '_SpecialForm'):  # Python versions before 3.9
        # Check if it's a _SpecialForm and its name is 'Annotated'
        return isinstance(hint, typing._SpecialForm) and hint.__name__ == 'Annotated'
    else:
        return False    
    
def serialize_function_to_json(func):
    if isinstance(func, str):
        return func
    
    signature = inspect.signature(func)
    type_hints = get_type_hints(func)

    # return_type = type_hints.get('return', 'void')
    # if return_type is None:
    #     return_type_str = 'void'
    # else:
    #     return_type_str = return_type.__name__

    function_info = {
        "name": func.__name__,
        "description": func.__doc__,
        "parameters": {
            "type": "object",
            "properties": {}
        },
        # "returns": return_type_str
    }

    for name, parameter in signature.parameters.items():
        param_type = get_type_name(type_hints.get(name, type(None)))
        param_annotated= func.__annotations__.get(name, '')

        function_info["parameters"]["properties"][name]  = {}
        properties = function_info["parameters"]["properties"][name] 

        
        if is_annotated_type(param_annotated):
            _, *metadata = get_args(param_annotated)
        else:
            metadata = []  
   
        param_desc = ""
        for meta in metadata:
            if isinstance(meta, str):
                param_desc = meta 
            if isinstance(meta, Dict):
                param_desc = meta.get("description", "")
                if "enum" in meta:
                    properties["enum"] = meta["enum"]

        properties["type"] = param_type
        properties["description"] = param_desc
        
        if parameter.default is not inspect.Parameter.empty:
            properties["default"] = parameter.default                            

    return json.dumps(function_info,ensure_ascii=False, indent=2)


class FunctionCall(pydantic.BaseModel):
    '''
    函数名称和函数参数列表
    '''        
    name: str = pydantic.Field(description="函数名")
    arguments: Dict[str,Any] = pydantic.Field(description="函数参数")

class FunctionCallWrapper(pydantic.BaseModel):    
    function: FunctionCall = pydantic.Field(description="函数调用")

class FunctionCallList(pydantic.BaseModel):
    '''
    函数调用列表    
    '''
    tool_calls: List[FunctionCallWrapper] = pydantic.Field(description="函数调用列表")
    id: str = pydantic.Field(description="工具调用的唯一标识符,无需生成")
    type: str = pydantic.Field("function",description="工具调用的类型，固定为 function，无需生成")

FUNCTION_CALLING_SCHEMA = FunctionCallList.schema_json(ensure_ascii=False, indent=2) 


def exec_capture_output(code: str,target_names:Dict[str,Any]={}) -> Tuple[int,str,Any]:
    buffer = io.StringIO()
    sys.stdout = buffer
    sys.stderr = buffer

    try:
        variables = {}
        exec(code,variables)
        response = {}
        for name,v in target_names.items():
            if name in variables:
                response[name] = variables[name]
    except Exception:
        return 1,traceback.format_exc(),{}

    sys.stdout = sys.__stdout__
    sys.stderr = sys.__stderr__

    return 0,buffer.getvalue(),response

def function_impl_format(prompt:str,func:Optional[Union[Callable,str]],
                             cls:Union[pydantic.BaseModel,str])->str:
    
    tool_choice_ser = serialize_function_to_json(func)    
    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)

    example = '''{
  "name": "caculate_current_time",
  "description": "\n    计算当前时间\n    ",
  "parameters": {
    "type": "object",
    "properties": {}
  }
}'''    
    example_output_format='''
{"title": "CurrentTime", "description": "当前时间    ", "type": "object", "properties": {"time": {"title": "Time", "description": "开始时间.时间格式为 yyyy-MM-dd", "type": "string"}}, "required": ["time"]}
'''
    example_output = '''
from datetime import datetime

def caculate_current_time():
    # 获取当前日期和时间
    now = datetime.now()
    
    # 将日期和时间格式化为"yyyy-MM-dd"的形式
    time_str = now.strftime("%Y-%m-%d")
    
    return {"time": time_str}
'''
    
    msg = f''''你非常擅长 Python 语言。根据用户提供的一些信息以及问题，对提供了没有实现空函数函数进行实现。

示例：
你需要实现的函数的签名如下：

```json
{example}
```

生成的函数的返回值必须是 Json 格式，并且满足如下 OpenAPI 3.1. 规范：

```json
{example_output_format}
```

最后，你生成的函数的代码如下：

```python
{example_output}
```

现在，你需要实现函数的签名如下：

```json
{tool_choice_ser}
```

同时，你生成的函数的返回值必须是 Json 格式，并且满足如下 OpenAPI 3.1. 规范：

```json
{_cls}
```

用户的问题是：{prompt}

在满足上述提及的约束的情况下，请你实现这个函数。
注意：
1. 任何情况下都不要拆分成多段代码输出，请一次性生成完整的代码片段，确保代码的完整性
2. 回复的内容只有一个代码块，且代码块的语言为 Python
3. 不要演示如何调用你生成的函数的代码
'''
    return msg   



def function_calling_format(prompt:str,tools:List[Union[Callable,str]],tool_choice:Optional[Union[Callable,str]])->str:
    tool_serializes = []
    for v in tools:
        tool_serializes.append(serialize_function_to_json(v))

    force_prompt = ""
    if tool_choice is not None:
        tool_choice_ser = serialize_function_to_json(tool_choice)
        force_prompt = f''''
你必须使用如下的工具来解决用户的问题：        
```json
{tool_choice_ser}
```
'''  
   
    if tool_choice is None and len(tools) == 0:
        return prompt                   

    tools_str = "\n".join(tool_serializes)
    
    function_example = '''
{
  "name": "compute_date_range",
  "description": "\n    计算日期范围\n    ",
  "parameters": {
    "type": "object",
    "properties": {
      "count": {
        "type": "int",
        "description": "时间跨度，数值类型,如果用户说的是几天，几月啥的，比较模糊，务必使用默认值",
        "default": 3
      },
      "unit": {
        "enum": [
          "day",
          "week",
          "month",
          "year"
        ],
        "type": "str",
        "description": "",
        "default": "day"
      }
    }
  }
}
'''
    output_example = '''
{
  "id": "unique_id_1",
  "type": "function",
  "tool_calls": [
    {
      "function": {
        "name": "compute_date_range",
        "arguments": {
          "count": 3,
          "unit": "day"
        }
      }
    }
  ]
}
'''

    msg = f'''You are a helpful assistant with access to the following functions:

```json
{tools_str}
```

当用户的问题可以使用上面的一个或者多个函数解决时,你需要通过符合 OpenAPI 3.1 规范的 Json 格式告诉我你需要调用哪些函数。

下面Json文本描述了你需要返回的格式,它符合 OpenAPI 3.1 规范:

```json
{FUNCTION_CALLING_SCHEMA}
```

示例：

当你选择下面的函数时：

```
{function_example}
```

你应该使用如下的 Json 格式告诉我你需要调用这个函数：

```json
{output_example}
```

{force_prompt}

现在用户的问题是：{prompt}

请选择合适的一个或者多个函数按要求的 Json 格式返回给我。

注意：
1. 如果你无法使用上述函数解决用户的问题，请如实告诉我你没有办法回答。
''' 
    return msg  


def response_class_format(prompt:str,cls:Union[pydantic.BaseModel,str])->str:

    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)    
        
    example='''
{"title": "Item", "description": "时间抽取的返回结果", "type": "object", "properties": {"time": {"title": "Time", "description": "时间信息,比如内容里会提到天， 月份，年等相关词汇", "type": "string"}, "other": {"title": "Other", "description": "除了时间以外的其他部分", "type": "string"}}, "required": ["time", "other"]}
'''
    example_output = '''{
  "time": "最近三个月",
  "other": "奔驰的销量趋势如何"
}'''
    msg = f'''当你回答用户问题的时候，你需要使用 Json 格式进行回复。

示例：

当你被要求按如下格式输出时,它符合 OpenAPI 3.1 规范：

```json
{example}
```
你的输出应该是这样的：

```json
{example_output}
```

现在用户的问题是：{prompt}

下面Json文本描述了你需要返回的格式,它符合 OpenAPI 3.1 规范:

```json
{_cls}
```

请根据自己生成的内容并以 Json 格式回复我。
''' 
    return msg

def response_class_format_after_chat(cls:Union[pydantic.BaseModel,str])->str:
 
    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)
    example='''
{"title": "Item", "description": "时间抽取的返回结果", "type": "object", "properties": {"time": {"title": "Time", "description": "时间信息,比如内容里会提到天， 月份，年等相关词汇", "type": "string"}, "other": {"title": "Other", "description": "除了时间以外的其他部分", "type": "string"}}, "required": ["time", "other"]}
'''
    example_output = '''{
  "car": {
    "name": "奔驰"
  },
  "metric": {
    "name": "销量趋势"
  }'''    
    msg = f'''你需要以 Json 格式重新组织内容回复我。

示例：

当你被要求按如下格式输出时,它符合 OpenAPI 3.1 规范：

```json
{example}
```
你的输出应该是这样的：

```json
{example_output}
```
把你刚才回答我的内容重新做组织，以 Json 格式回复我

下面Json文本描述了你需要返回的格式,它符合 OpenAPI 3.1 规范:

```json
{_cls}
```
''' 
    return msg 


def base_ability_format(prompt:Optional[str]=None)->str:
    RESPONSE_WITH_CLASS_example_0='''{"title": "Item", "description": "时间抽取的返回结果", "type": "object", "properties": {"time": {"title": "Time", "description": "时间信息,比如内容里会提到天， 月份，年等相关词汇", "type": "string"}, "other": {"title": "Other", "description": "除了时间以外的其他部分", "type": "string"}}, "required": ["time", "other"]}'''
    RESPONSE_WITH_CLASS_example_output_0 = '''{
    "time": "最近三个月",
    "other": "奔驰的销量趋势如何"
    }'''

    RESPONSE_WITH_CLASS_example='''{"title": "Info", "type": "object", "properties": {"car": {"title": "Car", "description": "车的信息", "allOf": [{"$ref": "#/definitions/Car"}]}, "metric": {"title": "Metric", "description": "计算的指标信息", "allOf": [{"$ref": "#/definitions/Metric"}]}}, "required": ["car", "metric"], "definitions": {"Car": {"title": "Car", "type": "object", "properties": {"name": {"title": "Name", "description": "品牌名称", "type": "string"}}, "required": ["name"]}, "Metric": {"title": "Metric", "type": "object", "properties": {"name": {"title": "Name", "description": "指标名称", "type": "string"}}, "required": ["name"]}}}'''
    RESPONSE_WITH_CLASS_example_output = '''{
  "car": {
    "name": "奔驰"
  },
  "metric": {
    "name": "销量趋势"
  }
}'''
    RESPONSE_WITH_CLASS_example='''{"title": "Info", "type": "object", "properties": {"car": {"title": "Car", "description": "车的信息", "allOf": [{"$ref": "#/definitions/Car"}]}, "metric": {"title": "Metric", "description": "计算的指标信息", "allOf": [{"$ref": "#/definitions/Metric"}]}}, "required": ["car", "metric"], "definitions": {"Car": {"title": "Car", "type": "object", "properties": {"name": {"title": "Name", "description": "品牌名称", "type": "string"}}, "required": ["name"]}, "Metric": {"title": "Metric", "type": "object", "properties": {"name": {"title": "Name", "description": "指标名称", "type": "string"}}, "required": ["name"]}}}'''
    RESPONSE_WITH_CLASS_example_output = '''{
  "car": {
    "name": "奔驰"
  },
  "metric": {
    "name": "销量趋势"
  }
}'''

    FUNCTION_CALLING_example = '''
{
  "name": "compute_date_range",
  "description": "\n    计算日期范围\n    ",
  "parameters": {
    "type": "object",
    "properties": {
      "count": {
        "type": "int",
        "description": "时间跨度，数值类型,如果用户说的是几天，几月啥的，比较模糊，务必使用默认值",
        "default": 3
      },
      "unit": {
        "enum": [
          "day",
          "week",
          "month",
          "year"
        ],
        "type": "str",
        "description": "",
        "default": "day"
      }
    }
  }
}
'''
    FUNCTION_CALLING_example_output = '''
{
  "id": "unique_id_1",
  "type": "function",
  "tool_calls": [
    {
      "function": {
        "name": "compute_date_range",
        "arguments": {
          "count": 3,
          "unit": "day"
        }
      }
    }
  ]
}
'''

    FUNCTION_IMPL_example = '''{
  "name": "caculate_current_time",
  "description": "\n    计算当前时间\n    ",
  "parameters": {
    "type": "object",
    "properties": {}
  }
}'''    
    FUNCTION_IMPL_example_output_schema='''
{"title": "CurrentTime", "description": "当前时间    ", "type": "object", "properties": {"time": {"title": "Time", "description": "开始时间.时间格式为 yyyy-MM-dd", "type": "string"}}, "required": ["time"]}
'''
    FUNCTION_IMPL_example_output = '''
from datetime import datetime

def caculate_current_time():
    # 获取当前日期和时间
    now = datetime.now()
    
    # 将日期和时间格式化为"yyyy-MM-dd"的形式
    time_str = now.strftime("%Y-%m-%d")
    
    return {"time": time_str}
'''
    msg = f'''下面是你具备的基础能力，当你回答用户问题的时候，随时回顾这些能力。

JSON 格式是一种轻量级的数据交换格式，JSON Schema 是基于 JSON 的一个描述 JSON 数据结构的元数据，可以用来描述 JSON 数据的结构和内容，以及定义 JSON 数据的合法值范围。
OpenAPI Specification (OAS) 使用 JSON Schema 来描述 Json 数据的结构和内容，你需要遵循 OpenAPI 3.1.0 版本的规范。

===================RESPONSE_WITH_CLASS===================

下面是一个根据用户的问题，并且结合 JSON Schema 生成对应的 JSON 数据的例子：

输入：

最近三个月奔驰的销量趋势如何？

JSON Schema：

```json
{RESPONSE_WITH_CLASS_example_0}
```

输出：

```json
{RESPONSE_WITH_CLASS_example_output_0}
```

下面生成的 Json 数据有有嵌套结构的例子：

输入：

最近三个月奔驰的销量趋势如何？

JSON Schema：

```json
{RESPONSE_WITH_CLASS_example}
```

输出：

```json
{RESPONSE_WITH_CLASS_example_output}
```

当用户提到 RESPONSE_WITH_CLASS 时，请回顾该能力。

===================FUNCTION_CALLING===================

用户会提供一个函数列表给你,你需要根据用户的问题，选择一个或者多个函数返回给用户。如果你无法使用上述函数解决用户的问题，请如实告诉我你没有办法回答。
下面假设你已经选择了一个函数作为输入，并且结合 JSON Schema 生成对应的 JSON 数据的例子：

输入：

```json
{FUNCTION_CALLING_example}
```

JSON Schema：

```json
{FUNCTION_CALLING_SCHEMA}
```

输出：

```json
{FUNCTION_CALLING_example_output}
```

当用户提到 FUNCTION_CALLING 时，请回顾该能力。

===================FUNCTION_IMPL===================

你非常擅长 Python 语言。根据用户提供的一些信息以及问题，对用户提供的没有实现空函数函数进行实现。
下面假设用户提供了一个需要实现的函数的签名，你需要结合用户的问题，函数的签名，以及函数文档，生成对应的 Python 代码，函数的返回值
必须是 Json 格式，并且需要符合对应的 JSON Schema 规范。

下面提供了一个示例：

输入：

```json
{FUNCTION_IMPL_example}
```

JSON Schema：

```json
{FUNCTION_IMPL_example_output_schema}
```

输出：

```python
{FUNCTION_IMPL_example_output}
```

注意：
1. 任何情况下都不要拆分成多段代码输出，请一次性生成完整的代码片段，确保代码的完整性
2. 回复的内容只有一个代码块，且代码块的语言为 Python
3. 不要展示如何调用你生成的函数的代码
4. 不要展示你函数执行的结果

当用户提到 FUNCTION_IMPL 时，请回顾该能力。

===================OTHERS===================
'''
    
    return msg


def sys_response_class_format(prompt:str,cls:Union[pydantic.BaseModel,str])->str:
    
    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)

    msg = f'''
请使用 RESPONSE_WITH_CLASS 相关的能力，解决用户的问题。

输入：

{prompt}

JSON Schema：

```json
{_cls}
```

输出：
'''
    return msg

def sys_function_calling_format(prompt:str,tools:List[Union[Callable,str]],tool_choice:Optional[Union[Callable,str]])->str:
    tool_serializes = []
    for v in tools:
        tool_serializes.append(serialize_function_to_json(v))

    force_prompt = ""
    if tool_choice is not None:
        tool_choice_ser = serialize_function_to_json(tool_choice)
        force_prompt = f''''
你必须使用如下的工具来解决用户的问题：        
```json
{tool_choice_ser}
```
'''  
   
    if tool_choice is None and len(tools) == 0:
        return prompt                   

    tools_str = "\n".join(tool_serializes)
        
    msg = f'''
请使用 FUNCTION_CALLING 相关的能力，解决用户的问题。

你有如下的函数可以使用：

```json
{tools_str}
```
{force_prompt}

输入：

{prompt}

JSON Schema：

```json
{FUNCTION_CALLING_SCHEMA}
```

输出:
''' 
    return msg 

def sys_function_impl_format(prompt:str,func:Optional[Union[Callable,str]],
                             cls:Union[pydantic.BaseModel,str])->str:
    
    tool_choice_ser = serialize_function_to_json(func)    
    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)

    
    msg = f''''请使用 FUNCTION_IMPL 相关的能力，解决用户的问题。
根据用户提供的一些信息以及函数签名，对函数进行实现。

用户问题： {prompt}

输入：

```json
{tool_choice_ser}
```

JSON Schema：

```json
{_cls}
```

输出:
'''
    return msg  

def format_prompt(func,**kargs): 
    from langchain import PromptTemplate
    doc = func.__doc__       
    lines = doc.splitlines()
    # get the first line to get the whitespace prefix
    first_non_empty_line = next(line for line in lines if line.strip())
    prefix_whitespace_length = len(first_non_empty_line) - len(first_non_empty_line.lstrip())    
    prompt = "\n".join([line[prefix_whitespace_length:] for line in lines])
    tpl = PromptTemplate.from_template(prompt)
    return tpl.format(**kargs)

def format_prompt_jinja2(func,**kargs):
    from jinja2 import Template
    doc = func.__doc__       
    lines = doc.splitlines()
    # get the first line to get the whitespace prefix
    first_non_empty_line = next(line for line in lines if line.strip())
    prefix_whitespace_length = len(first_non_empty_line) - len(first_non_empty_line.lstrip())    
    prompt = "\n".join([line[prefix_whitespace_length:] for line in lines])
    tpl = Template(prompt)
    return tpl.render(kargs)
  





##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/client/__init__.py
from pyjava import PythonContext,RayContext
from typing import Dict,Any,List,Optional,Union,Tuple,Callable,Annotated
from pyjava.udf import UDFBuilder
import ray
from ray.util.client.common import ClientActorHandle, ClientObjectRef
from byzerllm.utils.client import code_utils 
from byzerllm.utils import (function_calling_format,
                            response_class_format,
                            response_class_format_after_chat,
                            FunctionCallList,
                            function_impl_format,
                            base_ability_format,
                            sys_response_class_format,
                            sys_function_calling_format,
                            sys_function_impl_format,
                            exec_capture_output,
                            format_prompt,
                            format_prompt_jinja2
                            )
from byzerllm.utils.ray_utils import cancel_placement_group,get_actor_info
from langchain.prompts import PromptTemplate
import json
import dataclasses
import importlib  
import logging
import time
import asyncio
import functools
import inspect
import pydantic
import copy
import traceback
from enum import Enum


logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# create a enum for the role
class Role:
    User = "user"
    Assistant = "assistant"
    System = "system"

@dataclasses.dataclass
class LLMHistoryItem:
      role: str
      content: str

@dataclasses.dataclass
class LLMResponse:
    output: Union[str,List[float]]
    input: Union[str,Dict[str,Any]]
    metadata: Dict[str,Any] = dataclasses.field(default_factory=dict)


class LLMFunctionCallResponse(pydantic.BaseModel):
    response:LLMResponse
    values:List[Any]
    metadata:Dict[str,Any]


class LLMClassResponse(pydantic.BaseModel):
    response:LLMResponse
    value:Optional[Any]
    metadata:Dict[str,Any]

@dataclasses.dataclass
class LLMRequest:
    instruction: Union[str,List[str]]
    embedding: bool = False
    max_length: int = 4096
    top_p: float = 0.95
    temperature: float = 0.1    
        

@dataclasses.dataclass
class FintuneRequestExtra:
    max_seq_length: int = 1024
    num_train_epochs: int = 1
    logging_steps: int = 100
    save_steps: int = 100
    extra_params: Dict[str,Any] = dataclasses.field(default_factory=dict)

@dataclasses.dataclass
class  FintuneRequest:
    model_path: str
    pretrained_model_type: str
    input_data_path: str
    extra_params: FintuneRequestExtra = FintuneRequestExtra()


class InferBackend:
    Transformers = "transformers"
    VLLM = "ray/vllm"
    DeepSpeed = "ray/deepspeed"

@dataclasses.dataclass
class ExecuteCodeResponse:
      status: int
      output: str      
      code: str
      prompt: str
      variables: Dict[str,Any]=dataclasses.field(default_factory=dict)

class EventName(Enum):
    BEFORE_CALL_MODEL = "before_call_model"
    AFTER_CALL_MODEL = "after_call_model"

EventCallbackResult = Tuple[bool, Optional[Any]]
EventCallback = Callable[..., EventCallbackResult]    

class Template:
    def __init__(self,
                 role_mapping:Dict[str,str],
                 generation_config:Dict[str,Any],
                 clean_func:Callable[[str],str]=lambda s: s,
                 function_calling_format_func=function_calling_format,
                 response_class_format_func=response_class_format,
                 response_class_format_after_chat_func=response_class_format_after_chat
                 ) -> None:
        self.role_mapping = role_mapping
        self.generation_config = generation_config
        self.clean_func = clean_func        
        self.function_calling_format_func = function_calling_format_func
        self.response_class_format_func = response_class_format_func
        self.response_class_format_after_chat_func = response_class_format_after_chat_func


class Templates:

    def default_format(t,v):
        return f"{t}{v}"


    @staticmethod
    def qwen():
        def clean_func(v):            
            if "<|im_end|>" in v:
                v = v.split("<|im_end|>")[0]
            if "<|endoftext|>" in v:
                v = v.split("<|endoftext|>")[0] 
            if "<|im_start|>" in v:             
                v = v.split("<|im_start|>")[0]   
            return v   

        def sys_format(t,v):
            m = PromptTemplate.from_template(t)
            return m.format(system_msg=v)


        return Template(role_mapping={
                        "user_role":"<|im_start|>user\n",
                        "assistant_role": "<|im_end|>\n<|im_start|>assistant\n",
                        "system_msg":"<|im_start|>system\n{system_msg}<|im_end|>",
                        "system_msg_func":sys_format
                        },
                        generation_config={                            
                            "generation.repetition_penalty":1.1,
                            "generation.stop_token_ids":[151643,151645]},                  
                        clean_func=clean_func) 
    
    @staticmethod
    def llama():
        def sys_format(t,v):
            m = PromptTemplate.from_template(t)
            return m.format(system_msg=v)
        
        def user_format(t,v):
            return f"<s>[INST] {v} [/INST]"
        
        def assistant_format(t,v):
            return f" {v} </s>"
        
        return Template(
            role_mapping={
               "user_role":"",
               "assistant_role": "",
               "system_msg":"<s>[INST] <<SYS>>\n{system_msg}\n<</SYS>>\n[/INST]</s>",
               "system_msg_func":sys_format,
               "user_role_func": user_format,
               "assistant_role_func": assistant_format
            },            
            generation_config={},
            clean_func=lambda s: s
        )
    
    @staticmethod
    def deepseek_code_chat():
        '''
        DeepSeek Coder Chat mode template:

        ### Instruction:
        ['content']
        ### Response:
        ['content']
        <|EOT|>
        ### Instruction:
        ['content']
        ### Response:
        '''
        

        def sys_format(t:Annotated[str,"the field system_msg in role_mapping "],
                       v:Annotated[str,"the system message in chat"]):
            m = PromptTemplate.from_template(t)
            return m.format(system_msg=v)
        
        def user_format(t:Annotated[str,"the field user_role in role_mapping"],
                        v:Annotated[str,"the user message in chat"]):
            '''
            format single user message
            '''
            return f"### Instruction:\n{v}"
        
        def assistant_format(t:Annotated[str,"the field assistant_role in role_mapping"],
                             v:Annotated[str,"the assistant message in chat"]):
            '''
            format single assitant message.
            
            Notice that here we do not use `t` , because we will
            use the `t` as the final suffix.
            '''
            return f"### Response:\n{v}\n<|EOT|>"
        
        return Template(
            role_mapping={
               "user_role":"",
               "assistant_role": "### Response:\n",
               "system_msg":"{system_msg}",
               "system_msg_func":sys_format,
               "user_role_func": user_format,
               "assistant_role_func": assistant_format
            },            
            generation_config={"generation.stop_token_ids":[32021]},
            clean_func=lambda s: s
        )
    @staticmethod
    def deepseek_code_insertion():        
        def sys_format(t,v):
            if "<｜fim▁hole｜>" not in v:
                raise Exception("the system message should contains <｜fim▁hole｜>")
            m = PromptTemplate.from_template(t)
            return m.format(system_msg=v)
        
        def user_format(t,v):            
            return ""
        
        def assistant_format(t,v):            
            return ""
        
        return Template(
            role_mapping={
               "user_role":"",
               "assistant_role": "",
               "system_msg":"<｜fim▁begin｜>{system_msg}<｜fim▁end｜>",
               "system_msg_func":sys_format,
               "user_role_func": user_format,
               "assistant_role_func": assistant_format
            },            
            generation_config={},
            clean_func=lambda s: s
        )
    
    @staticmethod
    def deepseek_code_completion():        
        def sys_format(t,v):            
            m = PromptTemplate.from_template(t)
            return m.format(system_msg=v)
        
        def user_format(t,v):            
            return ""
        
        def assistant_format(t,v):            
            return ""
        
        return Template(
            role_mapping={
               "user_role":"",
               "assistant_role": "",
               "system_msg":"{system_msg}",
               "system_msg_func":sys_format,
               "user_role_func": user_format,
               "assistant_role_func": assistant_format
            },            
            generation_config={},
            clean_func=lambda s: s
        )
    @staticmethod
    def yi():
        def clean_func(v):                    
            return v   

        def sys_format(t,v):
            m = PromptTemplate.from_template(t)
            return m.format(system_msg=v)


        return Template(role_mapping={
                        "user_role":"<|im_start|>user\n",
                        "assistant_role": "<|im_end|>\n<|im_start|>assistant\n",
                        "system_msg":"<|im_start|>system\n{system_msg}<|im_end|>",
                        "system_msg_func":sys_format
                        },
                        generation_config={"generation.stop_token_ids":[7]},                  
                        clean_func=clean_func) 

    @staticmethod
    def default():
        def clean_func(v):                    
            return v   

        def sys_format(t,v):
            return v

        return Template(role_mapping={
                        "user_role":"User:",
                        "assistant_role": "Assistant:",
                        "system_msg":"You are a helpful assistant. Think it over and answer the user question correctly.",
                        "system_msg_func":sys_format
                        },
                        generation_config={},                  
                        clean_func=clean_func)   

    @staticmethod
    def empty():
        def clean_func(v):                    
            return v   

        def sys_format(t,v):
            return v

        return Template(role_mapping={
                        "user_role":"",
                        "assistant_role": "",
                        "system_msg":"",
                        "system_msg_func":sys_format
                        },
                        generation_config={},                  
                        clean_func=clean_func)   

class ByzerLLM:
   
    def __init__(self,url:Optional[str]=None,**kwargs):
        self.url = url               
        self.default_sys_conf = {"pythonMode":"ray",
                         "maxConcurrency":1,
                         "num_gpus":1,
                         "masterMaxConcurrency":1000,
                         "workerMaxConcurrency":1,
                         "infer_backend":"transformers"
                         }
        self.sys_conf = self.default_sys_conf.copy()
        self.sql_model = "context" in globals()
        
        self.verbose = kwargs.get("verbose",False)
        
        self.force_skip_context_length_check = False
        if "force_skip_context_length_check" in kwargs:
            self.force_skip_context_length_check = kwargs["force_skip_context_length_check"]

        self.mapping_auto_use_apply_chat_template = {}
        
        self.mapping_max_input_length = {}
        self.mapping_max_output_length = {}
        self.mapping_max_model_length = {}        
        self.mapping_role_mapping = {}
        self.mapping_extra_generation_params = {}
        self.mapping_clean_func = {}
   
        self.mapping_function_calling_format_func = {}
        self.mapping_response_class_format_func = {}
        self.mapping_response_class_format_after_chat_func = {}
        self.mapping_impl_func_format_func = {}

        self.mapping_base_system_message = {}
        self.mapping_sys_response_class_format_func = {}
        self.mapping_sys_function_calling_format_func = {}
        self.mapping_sys_response_class_format_after_chat_func = {}
        self.mapping_sys_impl_func_format_func = {}

        
        self.func_impl_cache = {}
        self.meta_cache = {}

        self.byzer_engine_url = None
        if "byzer_engine_url" in kwargs:
            self.byzer_engine_url = kwargs["byzer_engine_url"]  

        self.default_max_output_length = 1024
        if "default_max_output_length" in kwargs:
            self.default_max_output_length = kwargs["default_max_output_length"]   
        

        self.default_model_name = None
        self.default_emb_model_name = None
        self.default_rerank_model_name = None
        self.default_role_mapping = {
                    "user_role":"User:",
                    "assistant_role": "Assistant:",
                    "system_msg":"You are a helpful assistant. Think it over and answer the user question correctly."
                    }
        
        self.pin_model_worker_mapping = None

        if url is not None and self.sql_model:            
            v = globals()
            self.context = v["context"]
            self.ray_context = RayContext.connect(v, self.url, **kwargs)
        else:
            self.context = PythonContext(
                0,[],self.sys_conf
            ) 
            self.context.have_fetched = True
            self.ray_context = self.context.rayContext 

        self.event_callbacks: Dict[EventName, List[EventCallback]] = {}  

    def add_event_callback(self, event_name: EventName, callback: EventCallback) -> None:
        self.event_callbacks.setdefault(event_name, []).append(callback)

    def _trigger_event(self, event_name: EventName, *args, **kwargs) -> Optional[Any]:
        if event_name in self.event_callbacks:
            for callback in self.event_callbacks[event_name]:
                continue_flag, value = callback(*args, **kwargs)
                if not continue_flag:
                    return value
        return None                         
        
    def setup_reset(self):
        self.sys_conf = self.default_sys_conf.copy()
        self.context.conf = self.sys_conf

    def setup_pin_model_worker_mapping(self,pin_model_worker_mapping:Dict[Any,int])->'ByzerLLM':
        self.pin_model_worker_mapping = pin_model_worker_mapping
        return self   

    def setup_load_balance_way(self,load_balance_way:str)->'ByzerLLM':
        self.sys_conf["load_balance"] = load_balance_way
        return self 

    def setup_default_model_name(self,model_name:str)->'ByzerLLM':
        self.default_model_name = model_name
        return self 

    def setup_default_emb_model_name(self,model_name:str)->'ByzerLLM':
        self.default_emb_model_name = model_name
        return self  

    def setup_default_re_rank_model_name(self,model_name:str)->'ByzerLLM':
        self.default_rerank_model_name = model_name
        return self  

    def setup(self,name:str, value:Any)->'ByzerLLM':
        self.sys_conf[name]=value
        # update the context conf
        self.context.conf = self.sys_conf
        return self

    def setup_function_calling_format_func(self,model:str,func)->'ByzerLLM':
        self.mapping_function_calling_format_func[model] = func
        return self

    def setup_response_class_format_func(self,model:str,func)->'ByzerLLM':
        self.mapping_response_class_format_func[model] = func
        return self
    
    def setup_impl_func_format_func(self,model:str,func)->'ByzerLLM':
        self.mapping_impl_func_format_func[model] = func
        return self

    def setup_response_class_format_after_chat_func(self,model:str,func)->'ByzerLLM':
        self.mapping_response_class_format_after_chat_func[model] = func
        return self  

    def setup_base_system_messages(self,model:str,base_system_message:str)->'ByzerLLM':
        self.mapping_base_system_message[model] = base_system_message
        return self 

    def setup_sys_response_class_format_func(self,model:str,func)->'ByzerLLM':
        self.mapping_sys_response_class_format_func[model] = func
        return self  

    def setup_sys_function_calling_format_func(self,model:str,func)->'ByzerLLM':
        self.mapping_sys_function_calling_format_func[model] = func
        return self

    def setup_sys_response_class_format_after_chat_func(self,model:str,func)->'ByzerLLM':
        self.mapping_sys_response_class_format_after_chat_func[model] = func
        return self

    def setup_sys_impl_func_format_func(self,model:str,func)->'ByzerLLM':
        self.mapping_sys_impl_func_format_func[model] = func
        return self   
    
    
    def setup_infer_backend(self,backend:str)->'ByzerLLM':
        self.sys_conf["infer_backend"] = backend
        
        if backend == InferBackend.VLLM or backend == InferBackend.DeepSpeed:            
            self.sys_conf["masterMaxConcurrency"] = 1000
            self.sys_conf["workerMaxConcurrency"] = 100
        
        if backend == InferBackend.Transformers:
            self.sys_conf["masterMaxConcurrency"] = 1000
            self.sys_conf["workerMaxConcurrency"] = 1

        return self
    
    def setup_gpus_per_worker(self,num_gpus:int)->'ByzerLLM':
        self.sys_conf["num_gpus"] = num_gpus
        return self
    
    def setup_cpus_per_worker(self,num_cpus:int)->'ByzerLLM':
        self.sys_conf["num_cpus"] = num_cpus
        return self
    
    def setup_worker_concurrency(self,num:int)->'ByzerLLM':        
        self.sys_conf["workerMaxConcurrency"] = num
        return self        

    def setup_num_workers(self,num_workers:int)->'ByzerLLM':
        self.sys_conf["maxConcurrency"] = num_workers
        return self
    
    def setup_max_model_length(self,model:str,max_model_length:int)->'ByzerLLM':
        self.mapping_max_model_length[model] = max_model_length
        return self
    
    def setup_max_input_length(self,model:str,max_input_length:int)->'ByzerLLM':
        self.mapping_max_input_length[model] = max_input_length
        return self
    
    def setup_max_output_length(self,model:str, max_output_length:int)->'ByzerLLM':
        self.mapping_max_output_length[model] = max_output_length
        return self
    
    def setup_role_mapping(self,model:str,role_mapping:Dict[str,str])->'ByzerLLM':
        self.mapping_role_mapping[model] = role_mapping
        return self
    
    def setup_extra_generation_params(self,model:str,extra_generation_params:Dict[str,Any])->'ByzerLLM':
        v = self.mapping_extra_generation_params.get(model,{}) 
        self.mapping_extra_generation_params[model] = {**v,**extra_generation_params}
        return self       
    
    def setup_template(self,model:str,template:Union[Template,str])->'ByzerLLM':
        if template == "auto":
            meta = self.get_meta(model=model)
            
            is_saas_model =  meta.get("model_deploy_type",None) == "saas"
            
            if is_saas_model:
                return self
            
            is_message_format = meta.get("message_format",False)
            
            if is_message_format:                
                return self
                        
            if "QWenLMHeadModel" in meta.get("architectures",[]):
                self.setup_template(model,Templates.qwen())
                return self

            if not meta.get("support_chat_template",False):
                raise Exception(f"The model({model}) is not support auto(apply chat template) for now.")
            
            self.mapping_auto_use_apply_chat_template[model] = True
            return self

        self.mapping_role_mapping[model] = template.role_mapping
        
        v = self.mapping_extra_generation_params.get(model,{}) 
        self.mapping_extra_generation_params[model] = {**v,**template.generation_config}

        self.mapping_clean_func[model] = template.clean_func
        self.mapping_function_calling_format_func[model] = template.function_calling_format_func
        self.mapping_response_class_format_after_chat_func[model] = template.response_class_format_after_chat_func
        self.mapping_response_class_format_func[model] = template.response_class_format_func
        return self
           

    def sft(self,sft_name:str,
            local_data_dir_path:str,
            local_model_path:str,
            local_stage_path:str,
            pretrained_model_type:str,            
            num_cpus:int,
            num_gpus:int,
            detached:bool=True,
            json_config:str="{}",
            model_params:Dict[str,Any]={},
            **kwargs
            ):
        '''
        finetune a pretrained model

        Args:
            sft_name (str): the uniq name of this finetune task
            local_data_dir_path (str): the local data dir path, which should contains `data.jsonl` file
            local_model_path (str): the local model path, which should contains `config.json` file
            local_stage_path (str): the local stage path which store the temp data and model
            pretrained_model_type (str): the pretrained model type, e.g. "sft/llama2","sft/baichuan"
            num_cpus (int): the number of cpus
            num_gpus (int): the number of gpus
            detached (bool, optional): whether to run this task in detached mode. Defaults to True.
            json_config (str, optional): the json config string. Defaults to "{}".
            model_params (Dict[str,Any], optional): the model params. Defaults to {}. The key should like this style `sft.int.logging_steps`, `sft.int.max_seq_length`
                                                    which contains the `sft` prefix and the type of the value.
        '''
        train_params = {}
        train_params["name"] = sft_name
        train_params["data_dir"] = local_data_dir_path
        train_params["localModelDir"] = local_model_path
        train_params["pretrainedModelType"] = pretrained_model_type
        train_params["config"] = json_config
        train_params["detached"] = "true" if detached else "false"
        train_params["localPathPrefix"] = local_stage_path
        
        for k,v in model_params.items():
            train_params[k] = v

        sys_conf = {}
        sys_conf["num_gpus"] = num_gpus
        sys_conf["num_cpus"] = num_cpus    

        r = self.raw_sft(train_params=train_params,sys_conf=sys_conf)
        if detached:
           return [i for i in r]
        return r
    
    def merge_lora(self,name:str,
                   local_model_path:str,
                   local_adpator_model_path:str,
                   local_target_path:str
                   ):
        train_params = {}
        train_params["name"] = name
        train_params["modelNameOrPath"] = local_model_path
        train_params["adapterNameOrPath"] = local_adpator_model_path
        train_params["savePath"] = local_target_path
        self.raw_merge_lora(train_params=train_params,sys_conf={})
        return local_target_path
    
    def pretrain(self,name:str,
            local_data_dir_path:str,
            local_model_path:str,
            local_stage_path:str,
            pretrained_model_type:str,            
            num_cpus:int,
            num_gpus:int,
            detached:bool=True,
            json_config:str="{}",
            model_params:Dict[str,Any]={},
            **kwargs):
        train_params = {}
        train_params["name"] = name
        train_params["localDataDir"] = local_data_dir_path
        train_params["localModelDir"] = local_model_path
        train_params["pretrainedModelType"] = pretrained_model_type
        train_params["deepspeedConfig"] = json_config
        train_params["detached"] = "true" if detached else "false"
        train_params["localPathPrefix"] = local_stage_path
        
        for k,v in model_params.items():
            train_params[k] = v

        sys_conf = {}
        sys_conf["num_gpus"] = num_gpus
        sys_conf["num_cpus"] = num_cpus    

        r = self.raw_pretrain(train_params=train_params,sys_conf=sys_conf)
        if detached:
           return [i for i in r]
        return r
    
    
    
    def raw_sft(self,train_params:Dict[str,Any],sys_conf:Dict[str,Any]={}):                   
        model_type = train_params["pretrainedModelType"] .split("/")[-1]              
        train_module =  importlib.import_module(f'byzerllm.{model_type}')
        return train_module.sft_train([],train_params,sys_conf)                
            

    def raw_pretrain(self,train_params:Dict[str,Any],sys_conf:Dict[str,Any]={}):                  
        model_type = train_params["pretrainedModelType"][-1]      
        train_module = importlib.import_module(f'byzerllm.{model_type}')        
        return train_module.sfft_train([],train_params,sys_conf)

    def raw_merge_lora(self,train_params:Dict[str,Any],sys_conf:Dict[str,Any]):                
        from byzerllm.utils.sft.merge_lora import merge_lora_to_base_model    
        merge_lora_to_base_model([],train_params,sys_conf) 

    def raw_deepspeed_to_huggingface(self,train_params:Dict[str,Any]):
        from byzerllm.utils.fulltune.pretrain.convert_to_transformers import convert
        convert(train_params,self.conf()) 

    def undeploy(self,udf_name:str):  
        import time                        
        try:
            model = ray.get_actor(udf_name)
            try:
                meta = self.get_meta(model=udf_name)
                if meta.get("backend","") == "ray/vllm":
                    if "engine_placement_group_id" in meta:
                        cancel_placement_group(meta["engine_placement_group_id"])
            except Exception as inst:
                pass
            ray.kill(model)  
            if udf_name in self.meta_cache:
                del self.meta_cache[udf_name]                          
        except ValueError:
            pass
        time.sleep(3)

    def generate_instruction_from_history(self,model:str,conversations:List[Dict[str,str]],role_mapping:Dict[str,str]={        
        "user_role":"User:",        
        "assistant_role":"Assistant:",
    }):                
        meta = self.get_meta(model=model)
        if self.mapping_auto_use_apply_chat_template.get(model,False) and meta.get("support_chat_template",False) :
            return self.apply_chat_template(model,json.dumps(conversations,ensure_ascii=False))

        new_his = []    
        for item in conversations:
            if item["role"] == "system":
                value = item["content"]
                if "system_msg_func" in role_mapping:
                    value = role_mapping["system_msg_func"](t=role_mapping["system_msg"],v=item["content"])
                new_his.append(value)
                continue
            
            if item["role"] == "user":
                value =  f"{role_mapping['user_role']}{item['content']}"
                if "user_role_func" in role_mapping:
                        value = role_mapping["user_role_func"](t=role_mapping["user_role"],v=item["content"])         
                new_his.append(value)  
            
            if item["role"] == "assistant":
                value =  f"{role_mapping['assistant_role']}{item['content']}"
                if "user_role_func" in role_mapping:
                        value = role_mapping["assistant_role_func"](t=role_mapping["assistant_role"],v=item["content"])         
                new_his.append(value)              
        
        if conversations[-1]["role"] == "user":            
            new_his.append(f"{role_mapping['assistant_role']}")

        fin_ins = "\n".join(new_his)
        return fin_ins     

    def is_model_exist(self,udf_name:str)->bool:
        try:
            ray.get_actor(udf_name)
            return True
        except Exception as inst:
            return False                           

    def deploy(self,model_path:str,
               pretrained_model_type:str,
               udf_name:str,
               infer_params:Dict[str,Any]):        
        from byzerllm import common_init_model
        self.setup("UDF_CLIENT",udf_name)

        infer_backend = self.sys_conf["infer_backend"]
        
        if infer_backend == InferBackend.VLLM or infer_backend == InferBackend.DeepSpeed:
            if pretrained_model_type != "custom/auto":
                raise ValueError(f"Backend({infer_backend}) is set. the pretrained_model_type should be `custom/auto`")

        model_type = pretrained_model_type
        
        if pretrained_model_type.startswith("saas/"):
            model_type = pretrained_model_type.split("/")[-1]                       
            
            infer_module = importlib.import_module(f'byzerllm.saas.{model_type}')
            from byzerllm.utils.text_generator import simple_predict_func
            
            def init_model(model_refs: List[ClientObjectRef], conf: Dict[str, str]) -> Any:
                from byzerllm import consume_model
                consume_model(conf)                
                infer = infer_module.CustomSaasAPI(infer_params)
                return (infer,None)
            
            UDFBuilder.build(self.ray_context,init_model,simple_predict_func)
            return self.get_meta(model=udf_name) 

        
        if pretrained_model_type == "bark":
            from byzerllm.bark.bark_voice import build_void_infer, ZH_SPEAKER, EN_SPEAKER            
            def init_model(model_refs: List[ClientObjectRef], conf: Dict[str, str]) -> Any:
                infer = build_void_infer(
                model_dir=model_path,
                tokenizer_dir=f"{model_path}/pretrained_tokenizer")
                return infer
            def predict_func(model,v):
                data = [json.loads(item) for item in v]
                results=[{"predict":model.text_to_voice(item["instruction"]).tolist(),"labels":""} for item in data]
                return {"value":[json.dumps(results,ensure_ascii=False,indent=4)]}
            UDFBuilder.build(self.ray_context,init_model,predict_func)
            return self.get_meta(model=udf_name)               
        
        # we put in this place so it only take effect for private model
        self.mapping_max_output_length[udf_name]=1024

        if pretrained_model_type.startswith("custom/"):
            model_type = pretrained_model_type.split("/")[-1]

        predict_func = "simple_predict_func"
        if model_type == "chatglm2":
            predict_func = "chatglm_predict_func"

        infer_module = importlib.import_module(f'byzerllm.{model_type}')
        predict_module = importlib.import_module(f"byzerllm.utils.text_generator")
        
        def init_model(model_refs: List[ClientObjectRef], conf: Dict[str, str]) -> Any:
            common_init_model(model_refs,conf,model_path, is_load_from_local=True)
            model = infer_module.init_model(model_path,infer_params,conf)
            return model
        
        UDFBuilder.build(self.ray_context,init_model,getattr(predict_module,predict_func))
        return self.get_meta(model=udf_name)
  
    def get_meta(self,model:str,llm_config:Dict[str,Any]={}):        
        if not model and not self.default_model_name:
            raise Exception("model name is required")
        
        if not model:
            model = self.default_model_name

        if model in self.meta_cache:
            return self.meta_cache[model]    

        default_config = self.mapping_extra_generation_params.get(model,{})

        v = [{"instruction":"","meta":True, **{**default_config,**llm_config} }]        
        res = self._query(model,v) 
        
        t = [LLMResponse(output=item["predict"],metadata=item.get("metadata",{}),input=item["input"]) for item in res]        
        
        res = {}
        if len(t) != 0 and len(t[0].output) != 0 :
            res = t[0].output[0]

        self.meta_cache[model] = res            
        return self.meta_cache[model]
        
    def tokenize(self,model:str,s:str,llm_config:Dict[str,Any]={})->List[str]:
        
        if not model and not self.default_model_name:
            raise Exception("model name is required")
        
        if not model:
            model = self.default_model_name

        default_config = self.mapping_extra_generation_params.get(model,{})

        v = [{"instruction":s,"tokenizer":True, **{**default_config,**llm_config} }]        
        res = self._query(model,v) 
        return [LLMResponse(output=item["predict"],metadata=item.get("metadata",{}),input=item["input"]) for item in res]
    
    def apply_chat_template(self,model:str,s:str,llm_config:Dict[str,Any]={}):
        if not model and not self.default_model_name:
            raise Exception("model name is required")
        
        if not model:
            model = self.default_model_name
        
        default_config = self.mapping_extra_generation_params.get(model,{})
        v = [{"instruction":s,"apply_chat_template":True, **{**default_config,**llm_config} }]        
        res = self._query(model,v) 
        
        t = [LLMResponse(output=item["predict"],metadata=item.get("metadata",{}),input=item["input"]) for item in res]  
        return t[0].output      

    def emb_query(self,v:str,model:str=None):
        return self.emb(model=model,request=LLMRequest(instruction=v))


    def emb(self, model, request:LLMRequest ,extract_params:Dict[str,Any]={}):
        
        if not model and not self.default_emb_model_name:
            raise Exception("model name is required")
        
        if not model:
            model = self.default_emb_model_name

        default_config = self.mapping_extra_generation_params.get(model,{})            

        if isinstance(request,list):
            request = LLMRequest(instruction=request)

        if isinstance(request.instruction,str):
            v = [{
            "instruction":request.instruction,
            "embedding":True,
            "max_length":request.max_length,
            "top_p":request.top_p,
            "temperature":request.temperature,                                    
            ** default_config,           
            ** extract_params}] 
        else: 
            v = [{
            "instruction":x,
            "embedding":True,
            "max_length":request.max_length,
            "top_p":request.top_p,
            "temperature":request.temperature,            
            ** default_config, 
            ** extract_params} for x in request.instruction]    
        res = self._query(model,v) 
      
        return [LLMResponse(output=item["predict"],metadata=item.get("metadata",{}),input=item["input"]) for item in res]

    def emb_rerank(self, model: str = None, sentence_pairs: Union[List[Tuple[str, str]], Tuple[str, str]] = [],
                   extract_params: Dict[str, Any] = {}) -> Union[Tuple[Tuple[str, str], float], List[Tuple[Tuple[str, str], float]]]:

        if not model and not self.default_rerank_model_name:
            raise Exception("rerank model name is required")

        if not sentence_pairs or len(sentence_pairs) == 0:
            raise Exception("rerank rerank param sentence_pairs is required")

        if not model:
            model = self.default_rerank_model_name

        default_config = self.mapping_extra_generation_params.get(model, {})

        v = [{
            "instruction": sentence_pairs,
            "embedding": True,
            "embed_rerank": True,
            **default_config,
            **extract_params}]
        res = self._query(model, v)

        return [LLMResponse(output=item["predict"], metadata=item.get("metadata", {}), input=item["input"]) for item in
                res]

    def _generate_ins(self,model:str,request:LLMRequest,role_mapping:Dict[str,str]):
         if not role_mapping["user_role"]:
             return request.instruction
         
         sys_msg = role_mapping["system_msg"]
         if "system_msg_func" in role_mapping:
             sys_msg = "You are a helpful assistant. Think it over and answer the user question correctly."
         
         conversations = [{"role":"system","content":sys_msg}]
         # conversations += [{"role":item.role,"content":item.content} for item in request.extra_params.history]
         
         conversations += self._to_openai_format(request=request)
         
         final_ins = self.generate_instruction_from_history(model,conversations,role_mapping)                      
             
         return final_ins
    
    def _to_openai_format(self,request:LLMRequest): 
        conversations = []
        if isinstance(request.instruction,str):       
            conversations += [{
                        "role":"user",
                        "content":request.instruction
                    }]
        else:
            conversations += [{
                        "role":"user",
                        "content":x
                    } for x in request.instruction]    
        return conversations

    def execute_function_calling(self,response:LLMResponse,tools:List[Callable],func_params:Dict[str,Any])-> LLMFunctionCallResponse:            
        
        r = LLMFunctionCallResponse(response=response,values=[],metadata={"reason":""})
        
        is_json = False
        try:
            json.loads(response.output)
            is_json = True
        except Exception as inst:
            pass
        
        code = response.output
        if not is_json:
            codes = code_utils.extract_code(response.output)         
            if len(codes) == 0:            
                r.metadata["reason"] = "No json block found"
                return r 
            
            lang,code = codes[-1]

            if lang != "json":
                r.metadata["reason"] = "No json block found"
                return r
        
        try:
            temp = json.loads(code)
            if isinstance(temp,list):
                temp = temp[-1]
            ms = FunctionCallList.parse_obj(temp)
        except Exception as inst:
            r.metadata["reason"] = str(inst) + "\n" + traceback.format_exc()
            return r
                    
        _func_maps = dict([(t.__name__,t) for t in tools])

        if func_params is None:
            func_params = {}
        
        try:
            r.metadata["selected_functions"] = []
            for m in ms.tool_calls:        
                if m.function.name in _func_maps:
                    r.metadata["selected_functions"].append(m.function.name)
                    r.values.append(_func_maps[m.function.name](**m.function.arguments,**func_params))
        except Exception as inst:
            r.metadata["reason"] = str(inst) + "\n" + traceback.format_exc()            

        return r
    
    def execute_generate_func(self,                              
                              func_name:str,
                              impl_func_params:Optional[Dict[str,Any]],
                              response:LLMResponse,
                              response_class:pydantic.BaseModel)-> LLMClassResponse:
        
        
        r = LLMClassResponse(response=response,value=None,metadata={"reason":""})

        is_python_code = False
        if code_utils.infer_lang(response.output) == "python":
            is_python_code = True
        
        code = response.output

        if not is_python_code:
            codes = code_utils.extract_code(response.output)
            
            if len(codes) == 0:
                r.metadata["reason"] = "No Python block found"
                return r 
            
            lang,code = codes[-1]

            if lang != "python":
                r.metadata["reason"] = "No Python block found"
                return r
                
        (status,output,variables) = exec_capture_output(code,{func_name:True})
        if status != 0:
            r.metadata["reason"] = output
            return r
        
        try:
            if impl_func_params is None:
                impl_func_params = {}
            res_json = variables[func_name](**impl_func_params)
            r.metadata["raw_func"] = code
            r.metadata["func"]  = variables[func_name]            
            if isinstance(res_json,str):
                res_json = json.loads(res_json)
            r.value=response_class.parse_obj(res_json)
        except Exception as inst:
            r.metadata["reason"] = str(inst) + "\n" + traceback.format_exc()
            return r                                                       

        return r
    
    def execute_response_format(self,response:LLMResponse,response_class:pydantic.BaseModel):
        
        
        r = LLMClassResponse(response=response,value=None,metadata={"reason":""})
        is_json = False
        try:
            json.loads(response.output)
            is_json = True
        except Exception as inst:
            pass
                
        code = response.output

        if not is_json:
            codes = code_utils.extract_code(response.output)
            if len(codes) == 0:
                r.metadata["reason"] = "No json block found"
                return r 
            
            lang,code = codes[-1]

            if lang != "json":
                r.metadata["reason"] = "No json block found"
                return r
        
        try:
            ms = response_class.parse_obj(json.loads(code))            
        except Exception as inst:
            r.metadata["reason"] = str(inst) + "\n" + traceback.format_exc()
            return r                                       
        
        r.value=ms

        return r

    def abort(self,request_id:str,model:Optional[str]=None):
        if not model and not self.default_model_name:
            raise Exception("model name is required")
        if not model:
            model = self.default_model_name
        
        meta = self.get_meta(model=model)
        if meta.get("backend",None) != "ray/vllm":
            raise Exception("abort only support ray/vllm backend")
        
        self.chat_oai(conversations=[
            {
                "role":"user",
                "content":f"{request_id}"
            }
        ],llm_config={"gen.request_id":request_id,"gen.abort":True})    

    def chat_oai(self,
                 conversations,
                 tools:List[Union[Callable,str]]=[], 
                 tool_choice:Optional[Union[Callable,str]]=None,
                 execute_tool:bool=False,  
                 impl_func:Optional[Callable]=None,
                 execute_impl_func:bool=False,
                 impl_func_params:Optional[Dict[str,Any]]=None,
                 func_params:Optional[Dict[str,Any]]=None,
                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, 
                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,
                 enable_default_sys_message:bool=False,                 
                 model:Optional[str] = None,
                 role_mapping=None,llm_config:Dict[str,Any]={}
                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]:        
        
        if not self.default_model_name and not model:
            raise Exception("Use llm.setup_default_model_name to setup default model name or setup the model parameter")
        
        if not model:
            model = self.default_model_name
            
        if role_mapping is None:
            role_mapping = self.mapping_role_mapping.get(model, self.default_role_mapping)
        
        if response_class and (tools or tool_choice):
            raise Exception("function calling is enabled,response_class should not be set.")
        
        if impl_func and not response_class:
            raise Exception("impl_func is enabled,response_class should be set.")
        

        if enable_default_sys_message:
            first_message = conversations[0]
            if first_message["role"] == "user":
                conversations.insert(0,{
                    "role":"system",
                    "content": self.mapping_base_system_message.get(model,base_ability_format())
                })
            if first_message["role"] == "system":
                first_message["content"] = f'''{self.mapping_base_system_message.get(model,base_ability_format())}
{first_message["content"]}'''
                
        meta = self.get_meta(model=model)        
        is_saas_model =  meta.get("model_deploy_type",None) == "saas"
        is_message_format = meta.get("message_format",False)

        temp_conversations = copy.deepcopy(conversations)
        last_message = temp_conversations[-1]
        
        # function calling
        if tools or tool_choice:
            f = self.mapping_function_calling_format_func.get(model,function_calling_format) if not enable_default_sys_message else self.mapping_sys_function_calling_format_func.get(model,sys_function_calling_format)
            last_message["content"] = f(last_message["content"],tools,tool_choice)

        # implement function and the function should return a response class
        elif impl_func and response_class:
            f = self.mapping_impl_func_format_func.get(model,function_impl_format) if not enable_default_sys_message else self.mapping_sys_impl_func_format_func.get(model,sys_function_impl_format)    
            last_message["content"] = f(last_message["content"],impl_func,cls = response_class) 

        # generate response class 
        elif response_class and not response_after_chat:
            f = self.mapping_response_class_format_func.get(model,response_class_format) if not enable_default_sys_message else self.mapping_sys_response_class_format_func.get(model,sys_response_class_format) 
            last_message["content"] = f(last_message["content"],cls = response_class)
                           
        
        if is_saas_model or is_message_format:
            final_ins = last_message["content"]
            history = []
            for item in temp_conversations[:-1]:
                # clean metadata field in conversation 
                # which may used by agent.
                if "metadata" in item:
                    del item["metadata"]
                history.append(item)
            
        else:
            final_ins = self.generate_instruction_from_history(model,temp_conversations, role_mapping)         
            history = []

        default_config = self.mapping_extra_generation_params.get(model,{})
        v = [{"instruction":final_ins,"history":history,**default_config,**llm_config }]         
        res = self._query(model,v) 
        clean_func = self.mapping_clean_func.get(model,lambda s: s)        
        responses = [LLMResponse(output=clean_func(item["predict"]),metadata=item.get("metadata",{}),input=item["input"]) for item in res]        
        
        ## handle impl_func response
        if impl_func and response_class and execute_impl_func:
            final_result = []
            for response in responses:
                final_result.append(self.execute_generate_func(
                    func_name=impl_func.__name__,
                    impl_func_params=impl_func_params or func_params,
                    response=response,
                    response_class=response_class))
            return final_result
        
        if impl_func and response_class:
            return responses

        ## handle response_class response 
        temp_result = responses    
        if response_class and response_after_chat: 
            temp_result = []
            f = self.mapping_response_class_format_after_chat_func.get(model,response_class_format_after_chat)
            for response in responses:
                new_conversations = temp_conversations + [{
                                        "content":response.output,
                                        "role":"assistant"
                                    },{
                                        "content":f(response_class),
                                        "role":"user"
                                    }]
                temp_result.append(self.chat_oai(new_conversations,role_mapping=role_mapping,llm_config=llm_config)[0])            

        if response_class:
            final_result = []
            for response in temp_result:
                final_result.append(self.execute_response_format(response=response,response_class=response_class))
            return final_result    
                             
        ## handle function calling response
        if execute_tool:
            final_result = []
            for response in responses:
                final_result.append(self.execute_function_calling(response=response,tools=tools,func_params=func_params))

            return final_result
        
        return responses    
        
    def stream_chat_oai(self,conversations, model:Optional[str]=None, role_mapping=None,llm_config:Dict[str,Any]={}): 
        
        if not model:
            model = self.default_model_name

        meta = self.get_meta(model=model)
        if not meta.get("support_stream",False):
            raise Exception(f"The model({model}) is not support stream chat for now.")

        v = self.chat_oai(conversations,model=model,role_mapping = role_mapping,llm_config={**llm_config,**{"generation.stream":True}})       
        request_id = v[0].metadata["request_id"]
        stream_server = v[0].metadata.get("stream_server","VLLM_STREAM_SERVER")
        server = ray.get_actor(stream_server)                        

        pre_generated_text = None
        while True:                 
            final_output = ray.get(server.get_item.remote(request_id))
            if isinstance(final_output,str):
                time.sleep(0.01)
                continue
            
            if final_output is None:
                break
            
            text_outputs = final_output.outputs
            clean_func = self.mapping_clean_func.get(model,lambda s: s)
            generated_text = text_outputs[0].text
            if pre_generated_text is not None and generated_text == pre_generated_text:
                continue
            pre_generated_text=generated_text
            yield (clean_func(generated_text),text_outputs[0].metadata)

    async def async_stream_chat_oai(self,conversations,role_mapping=None,model:Optional[str]=None,llm_config:Dict[str,Any]={}): 
        
        if not model:
            model = self.default_model_name
        
        meta = self.get_meta(model=model)
        if not meta.get("support_stream",False):
            raise Exception(f"The model({model}) is not support stream chat for now.")    

        v = self.chat_oai(conversations,model=model,role_mapping=role_mapping,llm_config={**llm_config,**{"generation.stream":True}})       
        request_id = v[0].metadata["request_id"]
        stream_server = v[0].metadata.get("stream_server","VLLM_STREAM_SERVER")
        server = ray.get_actor(stream_server)

        pre_generated_text = None
        while True:                 
            final_output = await server.get_item.remote(request_id)
            if isinstance(final_output,str):
                time.sleep(0.01)
                continue
            
            if final_output is None:
                break
            
            text_outputs = [output for output in final_output.outputs]
            clean_func = self.mapping_clean_func.get(model,lambda s: s)
            generated_text = text_outputs[0].text
            if pre_generated_text is not None and generated_text == pre_generated_text:
                continue
            pre_generated_text=generated_text
            yield (clean_func(generated_text),text_outputs[0].metadata)   

    def clear_impl_cache(self,model:Optional[str]=None,
                         full_func_name:Optional[str]=None,
                         instruction:Optional[str]=None):
        if model is None and full_func_name is None and instruction is None:
            self.func_impl_cache = {}          
        
        if model is not None and full_func_name is not None and instruction is None:
            raise Exception("instruction is required")
        

        if model is not None:
            instruction = "" if not instruction else instruction
            full_func_name = "" if not full_func_name else full_func_name

            key = f"{model}_{instruction}_{full_func_name}"
            for k in list(self.func_impl_cache.keys()):
                if k.startswith(key):
                    del self.func_impl_cache[k]
            return self        
        
        if full_func_name is not None:            
            instruction = "" if not instruction else instruction
            model = "" if not model else model
            key = f"{model}_{instruction}_{full_func_name}"
            for k in list(self.func_impl_cache.keys()):                
                if k.endswith(key):
                    del self.func_impl_cache[k]
            return self        

    def prompt(self,model:Optional[str]=None,render:Optional[str]="simple",print_prompt:bool=False):              
            if model is None:
                model = self.default_model_name            

            def _impl(func):               
                @functools.wraps(func)
                def wrapper(*args, **kwargs):                                                                                                   
                    signature = inspect.signature(func)
                    arguments = signature.bind(*args, **kwargs)
                    arguments.apply_defaults()
                    input_dict = {}
                    for param in signature.parameters:
                        input_dict.update({ param: arguments.arguments[param] })
                    
                    if "self" in input_dict:
                        _ = input_dict.pop("self") 
                    
                    if render == "jinja2" or render == "jinja":                  
                        prompt_str = format_prompt_jinja2(func,**input_dict)
                    else:
                        prompt_str = format_prompt(func,**input_dict)
                                        
                    if issubclass(signature.return_annotation,pydantic.BaseModel):
                        response_class = signature.return_annotation                    
                        t = self.chat_oai(model=model,conversations=[{
                            "role":"user",
                            "content":prompt_str
                        }], 
                            response_class=response_class,                     
                            impl_func_params=input_dict)                    
                        r:LLMClassResponse = t[0]                        
                        return r.value
                    elif issubclass(signature.return_annotation,str):
                        t = self.chat_oai(model=model,conversations=[{
                            "role":"user",
                            "content":prompt_str
                        }])
                        return t[0].output
                    else:
                        raise Exception("impl function should return a pydantic model or string")
                return wrapper      
            return _impl
    
    def response(self,instruction:Optional[str]=None,
                      model:Optional[str]=None,
                      verbose:Optional[bool]=None):  
        if model is None:
            model = self.default_model_name
        if instruction is None:
            instruction = ""  
        
        if verbose is None:
            verbose = self.verbose            

        def _impl(func):               
            @functools.wraps(func)
            def wrapper(*args, **kwargs):                                                                               
                signature = inspect.signature(func)
                arguments = signature.bind(*args, **kwargs)
                arguments.apply_defaults()
                input_dict = {}
                for param in signature.parameters:
                    input_dict.update({ param: arguments.arguments[param] })
                
                if len(input_dict.keys()) != 1:
                    raise Exception("response function should have only one parameter which type should be string")

                if issubclass(signature.return_annotation,pydantic.BaseModel):
                    response_class = signature.return_annotation
                else:
                    raise Exception("impl function should return a pydantic model")
                
                start_time = time.monotonic()

                t = self.chat_oai(model=model,conversations=[{
                    "role":"user",
                    "content":list(input_dict.values())[0]
                }], 
                    response_class=response_class,                     
                    impl_func_params=input_dict)
                
                r:LLMClassResponse = t[0]                
                
                if verbose:
                    print(f'''cost {time.monotonic() - start_time} seconds''',flush=True)                
                
                return r.value

            return wrapper      
        return _impl            
    
    def impl(self,
             instruction:Optional[str]=None,
             model:Optional[str]=None,
             verbose:Optional[bool]=None,
             skip_cache:bool=False): 
        if model is None:
            model = self.default_model_name
        if instruction is None:
            instruction = ""  
        
        if verbose is None:
            verbose = self.verbose            

        def _impl(func):               
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                                                
                key = f"{model}_{instruction}_{func.__module__}.{func.__name__}"
                signature = inspect.signature(func)
                arguments = signature.bind(*args, **kwargs)
                arguments.apply_defaults()
                
                if issubclass(signature.return_annotation,pydantic.BaseModel):
                    response_class = signature.return_annotation
                else:
                    raise Exception("impl function should return a pydantic model")
                
                if not skip_cache and key in self.func_impl_cache:
                    if verbose:
                        print(f''' {key} in cache, skip impl function''')
                    return response_class.parse_obj(self.func_impl_cache[key](*args, **kwargs))
                
                
                input_dict = {}
                for param in signature.parameters:
                    input_dict.update({ param: arguments.arguments[param] })
                                                
                start_time = time.monotonic()

                t = self.chat_oai(model=model,conversations=[{
                    "role":"user",
                    "content":instruction
                }], impl_func=func,
                    response_class=response_class, 
                    execute_impl_func=True, 
                    impl_func_params=input_dict)
                
                r:LLMClassResponse = t[0]                
                
                if verbose:
                    print(f'''Generate code for {key}: 
```python
{r.metadata["raw_func"]}
``` 
cost {time.monotonic() - start_time} seconds                     
''',flush=True)

                if not skip_cache and key not in self.func_impl_cache:
                    self.func_impl_cache[key] = r.metadata["func"]
                
                return r.value

            return wrapper      
        return _impl  

    
    def raw_chat(self,model,request:Union[LLMRequest,str],extract_params:Dict[str,Any]={})->List[LLMResponse]:
        if isinstance(request,str): 
            request = LLMRequest(instruction=request)

        return self.chat(model,request,extract_params)

    def chat(self,model,request:Union[LLMRequest,str],extract_params:Dict[str,Any]={})->List[LLMResponse]:
        if not model and not self.default_model_name:
            raise Exception("model name is required")
        
        if not model:
            model = self.default_model_name

        default_config = self.mapping_extra_generation_params.get(model,{})  
        
        default_role_mapping = self.mapping_role_mapping.get(model, self.default_role_mapping)  
        
        if isinstance(request,str): 
            request = LLMRequest(instruction=request)

        if isinstance(request.instruction,str):
            
            final_input = self._generate_ins(model,request,default_role_mapping)                         
            
            v = [{
            "instruction":final_input,
            "max_length":request.max_length,
            "top_p":request.top_p,
            "temperature":request.temperature,                       
             **default_config,**extract_params
             }] 
        else: 
            v = []
            for x in request.instruction:
                
                new_request = LLMRequest(instruction=x,
                                         embedding=request.embedding,max_length=request.max_length,top_p=request.top_p,
                                         temperature=request.temperature,
                                         )
                               
                final_input = self._generate_ins(model,new_request,default_role_mapping)                                    
                
                v.append({
                "instruction":final_input, 
                "max_length":request.max_length,
                "top_p":request.top_p,
                "temperature":request.temperature, 
                **default_config,          
                **extract_params
                })
        res = self._query(model,v) 
        clean_func = self.mapping_clean_func.get(model,lambda s: s)
        return [LLMResponse(output=clean_func(item["predict"]),metadata=item.get("metadata",{}),input=item["input"]) for item in res]
    
    def apply_sql_func(self,sql:str,data:List[Dict[str,Any]],owner:str="admin",url:str="http://127.0.0.1:9003/model/predict"):
        if self.byzer_engine_url and url == "http://127.0.0.1:9003/model/predict":
            url = self.byzer_engine_url
        res = self._rest_byzer_engine(sql,data,owner,url)
        return res
    
    def _rest_byzer_script(self, sql:str,owner:str,url:str="http://127.0.0.1:9003/run/script"):
        import requests
        import json        
        data = {
                'sessionPerUser': 'true',
                'sessionPerRequest': 'true',
                'owner': owner,                
                'sql': sql,
                "includeSchema":True               
            }
        response = requests.post(url, data=data)
        
        if response.status_code != 200:
            raise Exception(f"{self.url} status:{response.status_code} content: {response.text} request: json/{json.dumps(data,ensure_ascii=False)}")
        res = json.loads(response.text)        
        return res

                   
    def _rest_byzer_engine(self, sql:str,table:List[Dict[str,Any]],owner:str,url:str):
        import requests
        import json        
        data = {
                'sessionPerUser': 'true',
                'sessionPerRequest': 'true',
                'owner': owner,
                'dataType': 'row',
                'sql': sql,
                'data': json.dumps(table,ensure_ascii=False)
            }
        response = requests.post(url, data=data)
        
        if response.status_code != 200:
            raise Exception(f"{self.url} status:{response.status_code} content: {response.text} request: json/{json.dumps(data,ensure_ascii=False)}")
        res = json.loads(response.text)        
        return res[0]

    def get_max_model_length(self,model:str):
        return self.mapping_max_model_length.get(model,None)

    def get_max_output_length(self,model:str):
        return self.mapping_max_output_length.get(model,self.default_max_output_length)

    def get_max_input_length(self,model:str):
        return self.mapping_max_input_length.get(model,None)        

    def _query(self, model:str, input_value:List[Dict[str,Any]]):  
        
        if not self.force_skip_context_length_check:
            for input in input_value:
                # if this is a embedding/tokenizer query ,skip            
                if input.get("embedding",False) or input.get("tokenizer",False):
                    continue            
                
                final_ins = input.get("instruction","")
                
                try:
                    input_size = len(self.tokenize(None,final_ins,{})[0].output[0])
                except Exception as inst:                
                    continue
                
                if self.get_max_input_length(model) and input_size > self.get_max_input_length(model):
                    raise Exception(f"input length {input_size} is larger than max_input_length {self.mapping_max_input_length[model]}")                
                
                max_output_length = self.get_max_output_length(model)

                if  self.get_max_model_length(model):                    
                    if input_size + max_output_length > self.get_max_model_length(model):
                        raise Exception(f"input_size ({input_size}) + max_output_length {max_output_length} is larget than model context length {self.mapping_max_model_length[model]}")                
                
                # dynamically update the max_length
                input["max_length"] = input_size + max_output_length

        event_result = self._trigger_event(EventName.BEFORE_CALL_MODEL, self, model, input_value)        
        if event_result is not None:            
            return event_result
        
        udf_master = ray.get_actor(model)     
        
        try:   
            new_input_value = [json.dumps(x,ensure_ascii=False) for x in input_value]
        except Exception as inst:
            raise Exception(f"input_value should be json serializable, got {input_value}") 
           
        if self.verbose:
            print(f"Send to model[{model}]:{new_input_value}")
        index = -1 
        try:    
            worker_id = -1  
            if self.pin_model_worker_mapping:
                if input_value[0].get("embedding",False):
                    worker_id = self.pin_model_worker_mapping.get("embedding",-1)
                elif input_value[0].get("tokenizer",False):
                    worker_id = self.pin_model_worker_mapping.get("tokenizer",-1)
                elif input_value[0].get("apply_chat_template",False):
                    worker_id = self.pin_model_worker_mapping.get("apply_chat_template",-1)
                elif input_value[0].get("meta",False):
                    worker_id = self.pin_model_worker_mapping.get("meta",-1)                  

            [index, worker] = ray.get(udf_master.get.remote(worker_id))                        
            res = ray.get(worker.async_apply.remote(new_input_value))
            
            event_result = self._trigger_event(EventName.AFTER_CALL_MODEL, model, json.loads(res["value"][0]))
            if event_result is not None:
                return event_result
                                                
            return json.loads(res["value"][0])
        except Exception as inst:
            raise inst
        finally:
            if index != -1:
                ray.get(udf_master.give_back.remote(index)) 

def default_chat_wrapper(llm:"ByzerLLM",conversations: Optional[List[Dict]] = None,llm_config={}):
    return llm.chat_oai(conversations=conversations,llm_config=llm_config)




##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/utils/client/code_utils.py
import logging
import os
import pathlib
import re
import subprocess
import sys
import time
import json
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from hashlib import md5
from typing import Callable, Dict, List, Optional, Tuple, Union
try:
    import docker
except ImportError:
    docker = None



# Regular expression for finding a code block
DEFAULT_MODEL="chat"
CODE_BLOCK_PATTERN = r"```[ \t]*(\w+)?[ \t]*\r?\n(.*?)\r?\n[ \t]*```"
WORKING_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), "extensions")
UNKNOWN = "unknown"
TIMEOUT_MSG = "Timeout"
DEFAULT_TIMEOUT = 600
WIN32 = sys.platform == "win32"
PATH_SEPARATOR = WIN32 and "\\" or "/"

logger = logging.getLogger(__name__)


def get_value_from_llm_str(v:str,k:str, default_value)->Union[str,int,float,bool,None]:
    responses = extract_code(v)
    value = default_value
    for lang,code in responses:
        if lang == "json":
            try:
                value = json.loads(code)[k]
            except Exception as inst:
                pass 
    return value

def content_str(content: Union[str, List, None]) -> str:
    """Converts `content` into a string format.

    This function processes content that may be a string, a list of mixed text and image URLs, or None,
    and converts it into a string. Text is directly appended to the result string, while image URLs are
    represented by a placeholder image token. If the content is None, an empty string is returned.

    Args:
        - content (Union[str, List, None]): The content to be processed. Can be a string, a list of dictionaries
                                      representing text and image URLs, or None.

    Returns:
        str: A string representation of the input content. Image URLs are replaced with an image token.

    Note:
    - The function expects each dictionary in the list to have a "type" key that is either "text" or "image_url".
      For "text" type, the "text" key's value is appended to the result. For "image_url", an image token is appended.
    - This function is useful for handling content that may include both text and image references, especially
      in contexts where images need to be represented as placeholders.
    """
    if content is None:
        return ""
    if isinstance(content, str):
        return content
    if not isinstance(content, list):
        raise TypeError(f"content must be None, str, or list, but got {type(content)}")

    rst = ""
    for item in content:
        if not isinstance(item, dict):
            raise TypeError("Wrong content format: every element should be dict if the content is a list.")
        assert "type" in item, "Wrong content format. Missing 'type' key in content's dict."
        if item["type"] == "text":
            rst += item["text"]
        elif item["type"] == "image_url":
            rst += "<image>"
        else:
            raise ValueError(f"Wrong content format: unknown type {item['type']} within the content")
    return rst

def infer_lang(code):
    """infer the language for the code.
    TODO: make it robust.
    """
    if code.startswith("python ") or code.startswith("pip") or code.startswith("python3 "):
        return "sh"

    # check if code is a valid python code
    try:
        compile(code, "test", "exec")
        return "python"
    except SyntaxError:
        # not a valid python code
        return UNKNOWN


def check_target_codes_exists(codes: List[Tuple[str, str]], langs: List[str]) -> bool:
    """Check if there is code in a specific language in the code list.

    Args:
        codes (list): The list of code blocks.
        langs (list): The language to check.

    Returns:
        bool: True if there is code in the specified language; False otherwise.
    """
    for l, _ in codes:
        if l in langs:
            return True
    return False

def get_target_codes(codes: List[Tuple[str, str]], langs: List[str]) -> List[str]:
    """Get code in a specific language from the code list.

    Args:
        codes (list): The list of code blocks.
        langs (list): The language to check.

    Returns:
        str: The code in the specified language.
    """
    target_codes = []
    for l, code in codes:
        if l in langs:
            target_codes.append(code)
    return target_codes


def extract_code(
    text: Union[str, List], pattern: str = CODE_BLOCK_PATTERN, detect_single_line_code: bool = False
) -> List[Tuple[str, str]]:
    """Extract code from a text.

    Args:
        text (str or List): The content to extract code from. The content can be
            a string or a list, as returned by standard GPT or multimodal GPT.
        pattern (str, optional): The regular expression pattern for finding the
            code block. Defaults to CODE_BLOCK_PATTERN.
        detect_single_line_code (bool, optional): Enable the new feature for
            extracting single line code. Defaults to False.

    Returns:
        list: A list of tuples, each containing the language and the code.
          If there is no code block in the input text, the language would be "unknown".
          If there is code block but the language is not specified, the language would be "".
    """
    text = content_str(text)
    if not detect_single_line_code:
        match = re.findall(pattern, text, flags=re.DOTALL)
        return match if match else [(UNKNOWN, text)]

    # Extract both multi-line and single-line code block, separated by the | operator
    # `([^`]+)`: Matches inline code.
    code_pattern = re.compile(CODE_BLOCK_PATTERN + r"|`([^`]+)`")
    code_blocks = code_pattern.findall(text)

    # Extract the individual code blocks and languages from the matched groups
    extracted = []
    for lang, group1, group2 in code_blocks:
        if group1:
            extracted.append((lang.strip(), group1.strip()))
        elif group2:
            extracted.append(("", group2.strip()))

    return extracted


def _cmd(lang):
    if lang.startswith("python") or lang in ["bash", "sh", "powershell"]:
        return lang
    if lang in ["shell"]:
        return "sh"
    if lang in ["ps1"]:
        return "powershell"
    raise NotImplementedError(f"{lang} not recognized in code execution")


def execute_code(
    code: Optional[str] = None,
    timeout: Optional[int] = None,
    filename: Optional[str] = None,
    work_dir: Optional[str] = None,
    use_docker: Optional[Union[List[str], str, bool]] = None,
    lang: Optional[str] = "python",
) -> Tuple[int, str, str]:
    """Execute code in a docker container.
    This function is not tested on MacOS.

    Args:
        code (Optional, str): The code to execute.
            If None, the code from the file specified by filename will be executed.
            Either code or filename must be provided.
        timeout (Optional, int): The maximum execution time in seconds.
            If None, a default timeout will be used. The default timeout is 600 seconds. On Windows, the timeout is not enforced when use_docker=False.
        filename (Optional, str): The file name to save the code or where the code is stored when `code` is None.
            If None, a file with a randomly generated name will be created.
            The randomly generated file will be deleted after execution.
            The file name must be a relative path. Relative paths are relative to the working directory.
        work_dir (Optional, str): The working directory for the code execution.
            If None, a default working directory will be used.
            The default working directory is the "extensions" directory under
            "path_to_autogen".
        use_docker (Optional, list, str or bool): The docker image to use for code execution.
            If a list or a str of image name(s) is provided, the code will be executed in a docker container
            with the first image successfully pulled.
            If None, False or empty, the code will be executed in the current environment.
            Default is None, which will be converted into an empty list when docker package is available.
            Expected behaviour:
                - If `use_docker` is explicitly set to True and the docker package is available, the code will run in a Docker container.
                - If `use_docker` is explicitly set to True but the Docker package is missing, an error will be raised.
                - If `use_docker` is not set (i.e., left default to None) and the Docker package is not available, a warning will be displayed, but the code will run natively.
            If the code is executed in the current environment,
            the code must be trusted.
        lang (Optional, str): The language of the code. Default is "python".

    Returns:
        int: 0 if the code executes successfully.
        str: The error message if the code fails to execute; the stdout otherwise.
        image: The docker image name after container run when docker is used.
    """    
    if all((code is None, filename is None)):
        error_msg = f"Either {code=} or {filename=} must be provided."
        logger.error(error_msg)
        raise AssertionError(error_msg)

    # Warn if use_docker was unspecified (or None), and cannot be provided (the default).
    # In this case the current behavior is to fall back to run natively, but this behavior
    # is subject to change.
    if use_docker is None:
        if docker is None:
            use_docker = False
            logger.warning(
                "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change"
            )
        else:
            # Default to true
            use_docker = True

    timeout = timeout or DEFAULT_TIMEOUT
    original_filename = filename
    if WIN32 and lang in ["sh", "shell"] and (not use_docker):
        lang = "ps1"
    if filename is None:
        code_hash = md5(code.encode()).hexdigest()
        # create a file with a automatically generated name
        filename = f"tmp_code_{code_hash}.{'py' if lang.startswith('python') else lang}"
    if work_dir is None:
        work_dir = WORKING_DIR
    filepath = os.path.join(work_dir, filename)
    file_dir = os.path.dirname(filepath)
    os.makedirs(file_dir, exist_ok=True)
    if code is not None:
        with open(filepath, "w", encoding="utf-8") as fout:
            fout.write(code)
    # check if already running in a docker container
    in_docker_container = os.path.exists("/.dockerenv")
    if not use_docker or in_docker_container:
        # already running in a docker container
        cmd = [
            sys.executable if lang.startswith("python") else _cmd(lang),
            f".\\{filename}" if WIN32 else filename,
        ]
        if WIN32:
            logger.warning("SIGALRM is not supported on Windows. No timeout will be enforced.")
            result = subprocess.run(
                cmd,
                cwd=work_dir,
                capture_output=True,
                text=True,
            )
        else:
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(
                    subprocess.run,
                    cmd,
                    cwd=work_dir,
                    capture_output=True,
                    text=True,
                )
                try:
                    result = future.result(timeout=timeout)
                except TimeoutError:
                    if original_filename is None:
                        os.remove(filepath)
                    return 1, TIMEOUT_MSG, None
        if original_filename is None:
            os.remove(filepath)
        if result.returncode:
            logs = result.stderr
            if original_filename is None:
                abs_path = str(pathlib.Path(filepath).absolute())
                logs = logs.replace(str(abs_path), "").replace(filename, "")
            else:
                abs_path = str(pathlib.Path(work_dir).absolute()) + PATH_SEPARATOR
                logs = logs.replace(str(abs_path), "")
        else:
            logs = result.stdout
        return result.returncode, logs, None

    # create a docker client
    client = docker.from_env()
    image_list = (
        ["python:3-alpine", "python:3", "python:3-windowsservercore"]
        if use_docker is True
        else [use_docker]
        if isinstance(use_docker, str)
        else use_docker
    )
    for image in image_list:
        # check if the image exists
        try:
            client.images.get(image)
            break
        except docker.errors.ImageNotFound:
            # pull the image
            print("Pulling image", image)
            try:
                client.images.pull(image)
                break
            except docker.errors.DockerException:
                print("Failed to pull image", image)
    # get a randomized str based on current time to wrap the exit code
    exit_code_str = f"exitcode{time.time()}"
    abs_path = pathlib.Path(work_dir).absolute()
    cmd = [
        "sh",
        "-c",
        f"{_cmd(lang)} {filename}; exit_code=$?; echo -n {exit_code_str}; echo -n $exit_code; echo {exit_code_str}",
    ]
    # create a docker container
    container = client.containers.run(
        image,
        command=cmd,
        working_dir="/workspace",
        detach=True,
        # get absolute path to the working directory
        volumes={abs_path: {"bind": "/workspace", "mode": "rw"}},
    )
    start_time = time.time()
    while container.status != "exited" and time.time() - start_time < timeout:
        # Reload the container object
        container.reload()
    if container.status != "exited":
        container.stop()
        container.remove()
        if original_filename is None:
            os.remove(filepath)
        return 1, TIMEOUT_MSG, image
    # get the container logs
    logs = container.logs().decode("utf-8").rstrip()
    # commit the image
    tag = filename.replace("/", "")
    container.commit(repository="python", tag=tag)
    # remove the container
    container.remove()
    # check if the code executed successfully
    exit_code = container.attrs["State"]["ExitCode"]
    if exit_code == 0:
        # extract the exit code from the logs
        pattern = re.compile(f"{exit_code_str}(\\d+){exit_code_str}")
        match = pattern.search(logs)
        exit_code = 1 if match is None else int(match.group(1))
        # remove the exit code from the logs
        logs = logs if match is None else pattern.sub("", logs)

    if original_filename is None:
        os.remove(filepath)
    if exit_code:
        logs = logs.replace(f"/workspace/{filename if original_filename is None else ''}", "")
    # return the exit code, logs and image
    return exit_code, logs, f"python:{tag}"


def _remove_check(response):
    """Remove the check function from the response."""
    # find the position of the check function
    pos = response.find("def check(")
    if pos == -1:
        return response
    return response[:pos]


def eval_function_completions(
    responses: List[str],
    definition: str,
    test: Optional[str] = None,
    entry_point: Optional[str] = None,
    assertions: Optional[Union[str, Callable[[str], Tuple[str, float]]]] = None,
    timeout: Optional[float] = 3,
    use_docker: Optional[bool] = True,
) -> Dict:
    """(openai<1) Select a response from a list of responses for the function completion task (using generated assertions), and/or evaluate if the task is successful using a gold test.

    Args:
        responses (list): The list of responses.
        definition (str): The input definition.
        test (Optional, str): The test code.
        entry_point (Optional, str): The name of the function.
        assertions (Optional, str or Callable): The assertion code which serves as a filter of the responses, or an assertion generator.
            When provided, only the responses that pass the assertions will be considered for the actual test (if provided).
        timeout (Optional, float): The timeout for executing the code.

    Returns:
        dict: The success metrics.
    """
    n = len(responses)
    if assertions is None:
        # no assertion filter
        success_list = []
        for i in range(n):
            response = _remove_check(responses[i])
            code = (
                f"{response}\n{test}\ncheck({entry_point})"
                if response.startswith("def")
                else f"{definition}{response}\n{test}\ncheck({entry_point})"
            )
            success = execute_code(code, timeout=timeout, use_docker=use_docker)[0] == 0
            success_list.append(success)
        return {
            "expected_success": 1 - pow(1 - sum(success_list) / n, n),
            "success": any(s for s in success_list),
        }
    if callable(assertions) and n > 1:
        # assertion generator
        assertions, gen_cost = assertions(definition)
    else:
        assertions, gen_cost = None, 0
    if n > 1 or test is None:
        for i in range(n):
            response = responses[i] = _remove_check(responses[i])
            code = (
                f"{response}\n{assertions}" if response.startswith("def") else f"{definition}{response}\n{assertions}"
            )
            succeed_assertions = execute_code(code, timeout=timeout, use_docker=use_docker)[0] == 0
            if succeed_assertions:
                break
    else:
        # just test, no need to check assertions
        succeed_assertions = False
        i, response = 0, responses[0]
    if test is None:
        # no test code
        return {
            "index_selected": i,
            "succeed_assertions": succeed_assertions,
            "gen_cost": gen_cost,
            "assertions": assertions,
        }
    code_test = (
        f"{response}\n{test}\ncheck({entry_point})"
        if response.startswith("def")
        else f"{definition}{response}\n{test}\ncheck({entry_point})"
    )
    success = execute_code(code_test, timeout=timeout, use_docker=use_docker)[0] == 0
    return {
        "index_selected": i,
        "succeed_assertions": succeed_assertions,
        "success": success,
        "gen_cost": gen_cost,
        "assertions": assertions,
    }


_FUNC_COMPLETION_PROMPT = "# Python 3{definition}"
_FUNC_COMPLETION_STOP = ["\nclass", "\ndef", "\nif", "\nprint"]

class PassAssertionFilter:
    def __init__(self, assertions):
        self._assertions = assertions        
        self.cost = 0
        self.metrics = self.responses = None

    def pass_assertions(self, context, response, **_):        
        responses = response[0].output        
        metrics = eval_function_completions(responses, context["definition"], assertions=self._assertions)
        self._assertions = metrics["assertions"]
        self.cost += metrics["gen_cost"]
        self.metrics = metrics
        self.responses = responses
        return metrics["succeed_assertions"]



##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/client.py
from langchain.embeddings.base import Embeddings
from typing import Any, List, Mapping, Optional,Tuple
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM
import requests
import json

from . import ClientParams


class ByzerLLMClient:
    
    def __init__(self,url:str='http://127.0.0.1:9003/model/predict',params:ClientParams=ClientParams()) -> None:
        self.url = url
        self.client_params = params        

    def request(self, sql:str,json_data:str)->str:         
        data = {
            'sessionPerUser': 'true',
            'sessionPerRequest': 'true',
            'owner': self.client_params.owner,
            'dataType': 'string',
            'sql': sql,
            'data': json_data
        }
        response = requests.post(self.url, data=data)
        if response.status_code != 200:
            raise Exception(f"{self.url} status:{response.status_code} content: {response.text} request: json/{json.dumps(data,ensure_ascii=False)}")
        return response.text

    def emb(self,s:str)-> List[float]:
        json_data = json.dumps([
            {"instruction":s,"embedding":True}
        ],ensure_ascii=False)
        response = self.request(f'''
        select {self.client_params.llm_embedding_func}(array(feature)) as value
        ''',json_data)    
        t = json.loads(response)
        t2 = json.loads(t[0]["value"][0])
        return t2[0]["predict"]

    def chat(self,s:str,history:List[Tuple[str,str]],extra_query={})->str:
        newhis = [{"query":item[0],"response":item[1]} for item in history]
        json_data = json.dumps([
            {"instruction":s,"history":newhis,**extra_query}
        ],ensure_ascii=False)
        response = self.request(f'''
        select {self.client_params.llm_chat_func}(array(feature)) as value
        ''',json_data)    
        t = json.loads(response)
        t2 = json.loads(t[0]["value"][0])
        return t2[0]["predict"]


class LocalEmbeddings(Embeddings):
    def __init__(self,client:ByzerLLMClient,prompt_prefix=None):
        self.client = client
        self.prompt_prefix = prompt_prefix
                
        
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings = []        
        for text in texts:
            if self.prompt_prefix:
                text = self.prompt_prefix + text
            embedding = self.client.emb(text)
            embeddings.append(embedding)        
        return embeddings

    def embed_query(self, text: str) -> List[float]: 
        if self.prompt_prefix:
            text = self.prompt_prefix + text   
        embedding = self.client.emb(text)
        return embedding


class Chatglm6bLLM(LLM):
    
    def __init__(self,client:ByzerLLMClient):
        self.client = client
        
    @property
    def _llm_type(self) -> str:
        return "chatglm6b"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
    ) -> str:
        if stop is not None:
            raise ValueError("stop kwargs are not permitted.")
        return self.client.chat(prompt,[])
    
    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {"n": self.n}

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/agent/groupchat.py
from dataclasses import dataclass
import sys
from typing import Dict, List, Optional, Union
from .agent import Agent
from .conversable_agent import ConversableAgent
import logging
from ray.util.client.common import ClientActorHandle, ClientObjectRef
from ...utils.client import ByzerLLM,code_utils
from byzerllm.utils.retrieval import ByzerRetrieval
import json
from . import get_agent_name, run_agent_func,ChatResponse

try:
    from termcolor import colored
except ImportError:

    def colored(x, *args, **kwargs):
        return x

logger = logging.getLogger(__name__)


@dataclass
class GroupChat:
    """(In preview) A group chat class that contains the following data fields:
    - agents: a list of participating agents.
    - messages: a list of messages in the group chat.
    - max_round: the maximum number of rounds.
    - admin_name: the name of the admin agent if there is one. Default is "Admin".
        KeyBoardInterrupt will make the admin agent take over.
    - func_call_filter: whether to enforce function call filter. Default is True.
        When set to True and when a message is a function call suggestion,
        the next speaker will be chosen from an agent which contains the corresponding function name
        in its `function_map`.
    """

    agents: List[Union[Agent,ClientActorHandle,str]]
    messages: List[Dict]
    max_round: int = 10
    admin_name: str = "Admin"
    func_call_filter: bool = True

    @property
    def agent_names(self) -> List[str]:
        """Return the names of the agents in the group chat."""
        _agent_names = []
        for agent in self.agents:
            _agent_names.append(get_agent_name(agent))
        return _agent_names           


    def reset(self):
        """Reset the group chat."""
        self.messages.clear()

    def agent_by_name(self, name: str) -> Agent:
        """Returns the agent with a given name."""
        return self.agents[self.agent_names.index(name)]    
   

    def next_agent(self, agent: Union[Agent,ClientActorHandle,str], agents: List[Union[Agent,ClientActorHandle,str]]) -> Union[Agent,ClientActorHandle,str]:
        """Return the next agent in the list."""
        agent_name = get_agent_name(agent)
        if agents == self.agents:
            return agents[(self.agent_names.index(agent_name) + 1) % len(agents)]
        else:
            offset = self.agent_names.index(agent_name) + 1
            for i in range(len(self.agents)):
                if self.agents[(offset + i) % len(self.agents)] in agents:
                    return self.agents[(offset + i) % len(self.agents)]

    def select_speaker_msg(self, agents: List[Union[Agent,ClientActorHandle,str]]):
        """Return the message for selecting the next speaker."""
        return f"""You are in a role play game. The following roles are available:
{self._participant_roles()}.

Read the following conversation.
Then select the next role from {[get_agent_name(agent) for agent in agents]} to play. Only return the role."""
     
    

    def select_speaker(self, last_speaker: Union[Agent,ClientActorHandle,str], 
                       selector: Union[ConversableAgent,ClientActorHandle,str],):
        """Select the next speaker."""
        if self.func_call_filter and self.messages and "function_call" in self.messages[-1]:
            # find agents with the right function_map which contains the function name
            agents = [
                agent for agent in self.agents if run_agent_func(agent,"can_execute_function",self.messages[-1]["function_call"]["name"])
            ]
            if len(agents) == 1:
                # only one agent can execute the function
                return agents[0]
            elif not agents:
                # find all the agents with function_map
                agents = [agent for agent in self.agents if run_agent_func(agent,"function_map")]
                if len(agents) == 1:
                    return agents[0]
                elif not agents:
                    raise ValueError(
                        f"No agent can execute the function {self.messages[-1]['name']}. "
                        "Please check the function_map of the agents."
                    )
        else:
            agents = self.agents
            # Warn if GroupChat is underpopulated
            n_agents = len(agents)
            if n_agents < 3:
                logger.warning(
                    f"GroupChat is underpopulated with {n_agents} agents. Direct communication would be more efficient."
                )
        
        run_agent_func(selector,"update_system_message",self.select_speaker_msg(agents))

        select_prompt = self.messages    +        [
                {
                    "role": "user",
                    "content": f"Read the above conversation. Then select the next role from {[get_agent_name(agent) for agent in agents]} to play. Only return the role.",
                }
            ]        
        
        
        final, name = run_agent_func(selector,"generate_llm_reply",None,select_prompt)                                        
        print(colored(f"GroupChat select_speaker: {name}","green"))

        if not final:
            # i = self._random.randint(0, len(self._agent_names) - 1)  # randomly pick an id
            return self.next_agent(last_speaker, agents)
        try:
            return self.agent_by_name(name.strip())
        except ValueError:
            logger.warning(
                f"GroupChat select_speaker failed to resolve the next speaker's name. Speaker selection will default to the next speaker in the list. This is because the speaker selection OAI call returned:\n{name}"
            )
            return self.next_agent(last_speaker, agents)

    def _participant_roles(self):
        roles = []
        for agent in self.agents:
            if run_agent_func(agent,"get_system_message").strip() == "":
                logger.warning(
                    f"The agent '{get_agent_name(agent)}' has an empty system_message, and may not work well with GroupChat."
                )
            roles.append(f"{get_agent_name(agent)}: {run_agent_func(agent,'get_system_message')}")
        return "\n".join(roles)


class GroupChatManager(ConversableAgent):
    """(In preview) A chat manager agent that can manage a group chat of multiple agents."""

    def __init__(
        self,
        groupchat: GroupChat,
        llm:ByzerLLM,
        retrieval:ByzerRetrieval,
        name: Optional[str] = "chat_manager",        
        # unlimited consecutive auto reply by default
        max_consecutive_auto_reply: Optional[int] = sys.maxsize,
        human_input_mode: Optional[str] = "NEVER",
        system_message: Optional[str] = "Group chat manager.",
        **kwargs,
    ):
        super().__init__(
            name=name,
            llm=llm,
            retrieval=retrieval,
            max_consecutive_auto_reply=max_consecutive_auto_reply,
            human_input_mode=human_input_mode,
            system_message=system_message,
            **kwargs,
        )
        # Order of register_reply is important.
        # Allow sync chat if initiated using initiate_chat
        self.register_reply([Agent, ClientActorHandle,str], 
                            GroupChatManager.run_chat, 
                            config=groupchat, 
                            reset_config=GroupChat.reset)  
        self.groupchat = groupchat  

    def get_groupchat(self) -> GroupChat:
        """Return the group chat."""
        return self.groupchat   

    def reset_agents(self):
        """Reset the agents."""
        for agent in self.groupchat.agents:
            run_agent_func(agent,"reset")       

    def run_chat(
        self,
        raw_message: Optional[Union[Dict,str,ChatResponse]] = None,
        messages: Optional[List[Dict]] = None,
        sender: Optional[Union[Agent, ClientActorHandle,str]] = None,
        config: Optional[GroupChat] = None,
    ) -> Union[str, Dict, None]:
        """Run a group chat."""
        if messages is None:
            messages = self._messages[get_agent_name(sender)]
        message = messages[-1]
        speaker = sender
        groupchat = config
        for i in range(groupchat.max_round):
            print(colored(f"GroupChatManager run_chat: {i}","green"),flush=True)
            # set the name to speaker's name if the role is not function
            if message["role"] != "function":
                message["name"] = get_agent_name(speaker)
            groupchat.messages.append(message)
            # broadcast the message to all agents except the speaker
            for agent in groupchat.agents:
                if get_agent_name(agent) != get_agent_name(speaker):
                    self.send(message, agent, request_reply=False, silent=True)
            if i == groupchat.max_round - 1:
                # the last round
                break
            try:
                # select the next speaker
                speaker = groupchat.select_speaker(speaker, self)
                # let the speaker speak
                reply = run_agent_func(speaker,"generate_reply",sender=self)
            except KeyboardInterrupt:
                # let the admin agent speak if interrupted
                if groupchat.admin_name in groupchat.agent_names:
                    # admin agent is one of the participants
                    speaker = groupchat.agent_by_name(groupchat.admin_name)
                    reply = run_agent_func(speaker,"generate_reply",sender=self)
                else:
                    # admin agent is not found in the participants
                    raise
            if reply is None:
                break
            # The speaker sends the message without requesting a reply
            run_agent_func(speaker,"send",message=reply,recipient=get_agent_name(self),request_reply=False);
            # get the speaker's last message and in next round, broadcast it to all other agents            
            message = self.last_message(speaker)            
        return True, None
    


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/agent/user_proxy_agent.py
from .conversable_agent import ConversableAgent
from typing import Callable, Dict, Optional, Union
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM


class UserProxyAgent(ConversableAgent):
    """(In preview) A proxy agent for the user, that can execute code and provide feedback to the other agents.

    UserProxyAgent is a subclass of ConversableAgent configured with `human_input_mode` to ALWAYS
    and `llm_config` to False. By default, the agent will prompt for human input every time a message is received.
    Code execution is enabled by default. LLM-based auto reply is disabled by default.
    To modify auto reply, register a method with [`register_reply`](conversable_agent#register_reply).
    To modify the way to get human input, override `get_human_input` method.
    To modify the way to execute code blocks, single code block, or function call, override `execute_code_blocks`,
    `run_code`, and `execute_function` methods respectively.
    To customize the initial message when a conversation starts, override `generate_init_message` method.
    """

    def __init__(
        self,
        name: str,
        llm: ByzerLLM,
        retrieval: ByzerRetrieval,
        is_termination_msg: Optional[Callable[[Dict], bool]] = None,
        max_consecutive_auto_reply: Optional[int] = None,
        human_input_mode: Optional[str] = "ALWAYS",
        function_map: Optional[Dict[str, Callable]] = None,
        code_execution_config: Optional[Union[Dict, bool]] = None,
        default_auto_reply: Optional[Union[str, Dict, None]] = "",        
        system_message: Optional[str] = "",
        **kwargs,
    ):
        """
        Args:
            name (str): name of the agent.
            is_termination_msg (function): a function that takes a message in the form of a dictionary
                and returns a boolean value indicating if this received message is a termination message.
                The dict can contain the following keys: "content", "role", "name", "function_call".
            max_consecutive_auto_reply (int): the maximum number of consecutive auto replies.
                default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).
                The limit only plays a role when human_input_mode is not "ALWAYS".
            human_input_mode (str): whether to ask for human inputs every time a message is received.
                Possible values are "ALWAYS", "TERMINATE", "NEVER".
                (1) When "ALWAYS", the agent prompts for human input every time a message is received.
                    Under this mode, the conversation stops when the human input is "exit",
                    or when is_termination_msg is True and there is no human input.
                (2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or
                    the number of auto reply reaches the max_consecutive_auto_reply.
                (3) When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops
                    when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.
            function_map (dict[str, callable]): Mapping function names (passed to openai) to callable functions.
            code_execution_config (dict or False): config for the code execution.
                To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:
                - work_dir (Optional, str): The working directory for the code execution.
                    If None, a default working directory will be used.
                    The default working directory is the "extensions" directory under
                    "path_to_autogen".
                - use_docker (Optional, list, str or bool): The docker image to use for code execution.
                    If a list or a str of image name(s) is provided, the code will be executed in a docker container
                    with the first image successfully pulled.
                    If None, False or empty, the code will be executed in the current environment.
                    Default is True, which will be converted into a list.
                    If the code is executed in the current environment,
                    the code must be trusted.
                - timeout (Optional, int): The maximum execution time in seconds.
                - last_n_messages (Experimental, Optional, int): The number of messages to look back for code execution. Default to 1.
            default_auto_reply (str or dict or None): the default auto reply message when no code execution or llm based reply is generated.
            llm_config (dict or False): llm inference configuration.
                Please refer to [OpenAIWrapper.create](/docs/reference/oai/client#create)
                for available options.
                Default to false, which disables llm-based auto reply.
            system_message (str): system message for ChatCompletion inference.
                Only used when llm_config is not False. Use it to reprogram the agent.
        """
        super().__init__(
            name,
            llm,retrieval,
            system_message,
            is_termination_msg,
            max_consecutive_auto_reply,
            human_input_mode,
            function_map,
            code_execution_config,            
            default_auto_reply,
            **kwargs,
        )


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/agent/extensions/planner.py
from ..conversable_agent import ConversableAgent
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
from ....utils.client import ByzerLLM
from byzerllm.utils.retrieval import ByzerRetrieval

class PlannerAgent(ConversableAgent):
    SYSTEM_PROMPT = '''You are a helpful AI assistant. You suggest coding and reasoning steps for another AI assistant to accomplish a task. Do not suggest concrete code. For any action beyond writing code or reasoning, convert it to a step that can be implemented by writing code. For example, browsing the web can be implemented by writing code that reads and prints the content of a web page. Finally, inspect the execution result. If the plan is not good, suggest a better plan. If the execution is wrong, analyze the error and suggest a fix.
'''    

    def __init__(
        self,
        name: str,
        llm: ByzerLLM,        
        retrieval: ByzerRetrieval,                
        system_message: Optional[str] = SYSTEM_PROMPT,        
        is_termination_msg: Optional[Callable[[Dict], bool]] = None,
        max_consecutive_auto_reply: Optional[int] = None,
        human_input_mode: Optional[str] = "NEVER",
        code_execution_config: Optional[Union[Dict, bool]] = False,
        **kwargs,
    ):       
        super().__init__(
            name,
            llm,retrieval,
            system_message,
            is_termination_msg,
            max_consecutive_auto_reply,
            human_input_mode,
            code_execution_config=code_execution_config,            
            **kwargs,
        )
        pass


##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/agent/extensions/byzer_engine_agent.py
from ..conversable_agent import ConversableAgent
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union,Annotated
from ....utils.client import ByzerLLM,message_utils,code_utils
from byzerllm.utils.retrieval import ByzerRetrieval
from ..agent import Agent
from ray.util.client.common import ClientActorHandle, ClientObjectRef
from .. import get_agent_name,run_agent_func,ChatResponse
import json
try:
    from termcolor import colored
except ImportError:

    def colored(x, *args, **kwargs):
        return x
    
class ByzerEngineAgent(ConversableAgent): 
    DEFAULT_SYSTEM_MESSAGE='''You are a helpful AI assistant.'''
    def __init__(
        self,
        name: str,
        llm: ByzerLLM,        
        retrieval: ByzerRetrieval, 
        chat_name:str,
        owner:str,               
        system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,        
        is_termination_msg: Optional[Callable[[Dict], bool]] = None,
        max_consecutive_auto_reply: Optional[int] = None,
        human_input_mode: Optional[str] = "NEVER",
        code_execution_config: Optional[Union[Dict, bool]] = False,
        **kwargs,
    ):       
        super().__init__(
            name,
            llm,retrieval,
            system_message,
            is_termination_msg,
            max_consecutive_auto_reply,
            human_input_mode,
            code_execution_config=code_execution_config,            
            **kwargs,
        )

        self._reply_func_list = []
        # self.register_reply([Agent, ClientActorHandle,str], ConversableAgent.generate_llm_reply)   
        self.register_reply([Agent, ClientActorHandle,str], ByzerEngineAgent.generate_custom_reply) 
        self.register_reply([Agent, ClientActorHandle,str], ConversableAgent.check_termination_and_human_reply) 

    def generate_custom_reply(
        self,
        raw_message: Optional[Union[Dict,str,ChatResponse]] = None,
        messages: Optional[List[Dict]] = None,
        sender: Optional[Union[ClientActorHandle,Agent,str]] = None,
        config: Optional[Any] = None,
        ) -> Tuple[bool, Union[str, Dict, None,ChatResponse]]:  

        if messages is None:
            messages = self._messages[get_agent_name(sender)]

        message = messages[-1]    
        code = code_utils.extract_code(message["content"])[0][1]
        try :                            
            reply = self.execute_spark_sql(code)
        except Exception as e:
            # get full exception
            import traceback
            reply = f"执行代码出错：{traceback.format_exc()} {e}" 
            new_message = message_utils.fail_message({"content":reply} )
            print(f"Byzer Engine execute code error: {message_utils.copy_error_count(message,new_message)}",flush=True)
            return True, message_utils.copy_error_count(message,new_message)
        
        print(f"Byzer Engine execute code success: {reply}",flush=True)
        return True, message_utils.success_message({"content":reply})
    
    def execute_spark_sql(self,sql:Annotated[str,"Spark SQL 语句"])->str:
        '''
        执行 Spark SQL 语句
        '''
        
        print(f"Byzer Engine execute spark sql: {sql}",flush=True)

        v = self.llm._rest_byzer_script(f"""
load csv.`file:///home/byzerllm/projects/jupyter-workspace/nlp2query/h.csv` where header="true" as test_table;
!profiler sql '''
{sql}                                        
''';
""",owner="william",url="http://192.168.1.248:9003/run/script")
        return json.dumps(v,ensure_ascii=False)

        

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/agent/extensions/python_codesandbox_agent.py
from ..conversable_agent import ConversableAgent
from ..agent import Agent
from .. import get_agent_name,run_agent_func,ChatResponse
import ray
from ray.util.client.common import ClientActorHandle, ClientObjectRef
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
from typing import Callable, Dict, Optional, Union
import time
import sys
import io
import traceback
import json
import os
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,code_utils,message_utils



class CodeSandbox:
    def __init__(self,file_path:str,file_ref) -> None:        
        self.file_ref = file_ref
        self.file_path = file_path
        self.session_variables = {}
        if self.file_ref:
            if isinstance(self.file_ref,ClientObjectRef):
                content = ray.get(self.file_ref)
            else:
                content = self.file_ref   

            # check parent directory of self.file_path exists
            parent_dir = os.path.dirname(self.file_path)
            if not os.path.exists(parent_dir):
                os.makedirs(parent_dir)     
                     
            with open(self.file_path, "wb") as f:
                f.write(content)
                
    def set_value(self,name:str,value:str): 
        self.session_variables[name] = value
        return self

    def get_value(self,name:str):
        if name not in self.session_variables:
            return None
        return self.session_variables[name]

    def get_file_path(self):
        return self.file_path        

    def execute_code(self,code)->Tuple[int, str, str]:
        return code_utils.execute_code(
                code = code,
                timeout=30*60,
                filename=None,
                work_dir=None,
                use_docker=False,
                lang="python"        
                ) 
    
    def exec_capture_output(self,code: str,target_names:Dict[str,Any]={}) -> Tuple[int,str,Any]:
        buffer = io.StringIO()
        sys.stdout = buffer
        sys.stderr = buffer

        try:
            variables = {}
            exec(code,variables)
            response = {}
            for name,v in target_names.items():
                if name in variables:
                    response[name] = variables[name]
        except Exception:
            return 1,traceback.format_exc(),{}

        sys.stdout = sys.__stdout__
        sys.stderr = sys.__stderr__

        return 0,buffer.getvalue(),response
    
class PythonSandboxAgent(ConversableAgent):        

    def __init__(
        self,
        name: str,
        llm: ByzerLLM,
        retrieval: ByzerRetrieval,   
        chat_name:str,
        owner:str,     
        system_message: Optional[str],        
        is_termination_msg: Optional[Callable[[Dict], bool]] = None,
        max_consecutive_auto_reply: Optional[int] = None,
        human_input_mode: Optional[str] = "NEVER",
        code_execution_config: Optional[Union[Dict, bool]] = {},
        **kwargs,
    ):
        super().__init__(
            name,
            llm,retrieval,
            system_message,
            is_termination_msg,
            max_consecutive_auto_reply,
            human_input_mode,
            code_execution_config=code_execution_config,            
            **kwargs,
        )
        self.sandboxes = {}
        self.lasted_updated = {}
        
        ## Restore the reply function list
        self._reply_func_list = []        

        ## Register the reply functions                
        self.register_reply([Agent, ClientActorHandle,str], PythonSandboxAgent.generate_execute_code_reply) 
        self.register_reply([Agent, ClientActorHandle,str], ConversableAgent.check_termination_and_human_reply)             
        

    def generate_execute_code_reply(
        self,
        raw_message: Optional[Union[Dict,str,ChatResponse]] = None,
        messages: Optional[List[Dict]] = None,
        sender: Optional[Union[ClientActorHandle,Agent,str]] = None,
        config: Optional[Any] = None,
    ) -> Tuple[bool, Union[str, Dict, None,ChatResponse]]:
                
        code_execution_config = config if config is not None else self._code_execution_config
        
        if code_execution_config is False:            
            return False, None
        
        if messages is None:
            messages = self._messages[get_agent_name(sender)]
        
        last_n_messages = code_execution_config.pop("last_n_messages", 1)                

        for i in range(min(len(messages), last_n_messages)):
            message = messages[-(i + 1)]
            if not message["content"]:
                continue            
            
            code_blocks = code_utils.extract_code(message["content"])
            if len(code_blocks) == 1 and code_blocks[0][0] == "unknown":
                continue

            # found code blocks, execute code and push "last_n_messages" back
            #  combine all code blocks into one code block
            codes = [code_block[1] for code_block in code_blocks if code_block[0] == "python"]
            code_str = "\n".join(codes)
            
            file_path = None
            file_ref = None

            if "metadata" not in message:
                message["metadata"] = {}
            
            if "file_path" in message["metadata"]:
                file_path = message["metadata"]["file_path"]
                file_ref = message["metadata"]["file_ref"]                
            
            target_names = {}
            if "target_names" in message["metadata"]:
                target_names = message["metadata"]["target_names"]

            sandbox = self.get_or_create_sandbox(get_agent_name(sender)+"_sandbox",file_path,file_ref,0,0)            
            exitcode, output,response = ray.get(sandbox.exec_capture_output.remote(code_str,target_names))
            code_execution_config["last_n_messages"] = last_n_messages
            exitcode2str = "execution succeeded" if exitcode == 0 else "execution failed"
            
            return True, {
                "content":f"exitcode: {exitcode} ({exitcode2str})\nCode output: {output}",
                 "metadata":{
                     "execute_result":ChatResponse(status=exitcode,
                                      output=f"exitcode: {exitcode} ({exitcode2str})\nCode output: {output}",
                                      code=code_str,
                                      prompt=message,
                                      variables=response,                                      
                                      ),
                      "error_count": message_utils.get_error_count(message),
                 }
            }

        print("No code block found in the last {} messages.".format(last_n_messages),flush=True)
        code_execution_config["last_n_messages"] = last_n_messages

        return True, None            

    def check_sandbox_timeout(self,timeout:int=60*60): 
        remove_names = []
        for name in self.lasted_updated:
            if time.time() - self.lasted_updated[name] > timeout:
                remove_names.append(name)
        for name in remove_names:
            del self.sandboxes[name]
            del self.lasted_updated[name]        

    def check_sandbox_exists(self,name:str)->bool:
        return name in self.sandboxes

    def get_sandbox(self,name:str):                
        self.check_sandbox_timeout()        
        return self.sandboxes[name]
    
    def force_clear(self):
        self.sandboxes = {}
        self.lasted_updated = {}

    def get_or_create_sandbox(self,name:str,
                              file_path:str,file_ref:str,
                              num_gpus:int,num_cpus:int):
        self.lasted_updated[name] = time.time()
        self.check_sandbox_timeout()
        if name in self.sandboxes:            
            return self.sandboxes[name]
        
        try :
            sandbox = ray.get_actor(name)
            return sandbox
        except ValueError:
            pass
        
        sandbox = ray.remote(CodeSandbox).options(
                name=name,                                             
                num_cpus=num_cpus,
                num_gpus=num_gpus
            ).remote(file_path,file_ref)
        self.sandboxes[name] = sandbox
        return sandbox    

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/agent/extensions/rhetorical_agent.py
from ..conversable_agent import ConversableAgent
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
from ....utils.client import ByzerLLM,message_utils,code_utils
from byzerllm.utils.retrieval import ByzerRetrieval
from ..agent import Agent
from ray.util.client.common import ClientActorHandle, ClientObjectRef
from .. import get_agent_name,run_agent_func,ChatResponse
from byzerllm.apps.agent.extensions.simple_retrieval_client import SimpleRetrievalClient
from byzerllm.utils.retrieval import SearchQuery
import json
   
class RhetoricalAgent(ConversableAgent): 
    
    DEFAULT_SYSTEM_MESSAGE = '''你的主要工作是从我们的对话中找到新的名词定义

你首先需要了解 "名词定义"的含义：

所谓名词定义是在我们的对话中，有一些名词，是在交互过程中确定的，比如：

```
用户： 上分的销售额是多少？
助手： 你是指上海分公司的销售额吗？
用户： 是上海分行的销售额。
```

在这个对话中，我们可以确定，用户说的“上分”，就是指“上海分行”。于是，根据前面分析，你得到一个新的名词定义：

```json
[
  "上分是指上海分行"
]
```

再比如：

```
用户： 奔驰上个月的销量
助手： 销量是指销售额还是销售数量？
用户： 以后我说销量的时候都是指的销售额
```

在这个对话中，我们可以确定，用户以后说销售，实际上就是指销售额。于是，根据前面分析，你得到一个新的名词定义：

```json
[
 "销量是指销售额"
]
```

每次当用户说“开始”，你就可以按如下步骤进行检查：

1. 找到用户最近的一个的问题
2. 顺着这个问题，依次回顾自己的回答和用户的回复
3. 从这个过程中，参考前面的例子，找到你认为新的名词定义

请按这个步骤一步一步进行检查，并且输出每一步检查的结果。

最后，对你的结果重新以 Json 格式进行输出，格式如下：

```json
[
  "这里替换成你新发现的名词定义"
]
```

注意：

1. 当用户提供表信息或者示例数据的时候，不要对这些内容做任何分析。
2. 输出的json 需要使用 ```sql`` 进行包裹。
3. json 中的内容只需要包含名词定义部分，不要有其他内容。
'''
    
    def __init__(
        self,
        name: str,
        llm: ByzerLLM,        
        retrieval: ByzerRetrieval,     
        chat_name:str,
        owner:str,           
        retrieval_cluster:str="data_analysis",
        retrieval_db:str="data_analysis",
        update_context_retry: int = 3,
        chunk_size_in_context: int = 1,
        system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,        
        is_termination_msg: Optional[Callable[[Dict], bool]] = None,
        max_consecutive_auto_reply: Optional[int] = None,
        human_input_mode: Optional[str] = "NEVER",
        code_execution_config: Optional[Union[Dict, bool]] = False,
        **kwargs,
    ):       
        super().__init__(
            name,
            llm,retrieval,
            system_message,
            is_termination_msg,
            max_consecutive_auto_reply,
            human_input_mode,
            code_execution_config=code_execution_config,            
            **kwargs,
        )
                        
        self.chat_name = chat_name
        self.owner = owner
        
        self.update_context_retry = update_context_retry
        self.chunk_size_in_context = chunk_size_in_context

        self._reply_func_list = []
        # self.register_reply([Agent, ClientActorHandle,str], ConversableAgent.generate_llm_reply)   
        self.register_reply([Agent, ClientActorHandle,str], RhetoricalAgent.my_reply) 
        self.register_reply([Agent, ClientActorHandle,str], RhetoricalAgent.check_termination_and_human_reply) 
                
        self.retrieval_cluster = retrieval_cluster
        self.retrieval_db = retrieval_db         

        self.simple_retrieval_client = SimpleRetrievalClient(llm=self.llm,
                                                             retrieval=self.retrieval,
                                                             retrieval_cluster=self.retrieval_cluster,
                                                             retrieval_db=self.retrieval_db,
                                                             )         
                
          
    def my_reply(
        self,
        raw_message: Optional[Union[Dict,str,ChatResponse]] = None,
        messages: Optional[List[Dict]] = None,
        sender: Optional[Union[ClientActorHandle,Agent,str]] = None,
        config: Optional[Any] = None,
    ) -> Tuple[bool, Union[str, Dict, None,ChatResponse]]:          

        if messages is None:
            messages = self._messages[get_agent_name(sender)] 

        if self.retrieval is None:
            return True, None 
        
        ## get the last 100 conversation
        docs = self.retrieval.filter(self.retrieval_cluster,
                            [SearchQuery(self.retrieval_db,"user_memory",
                                         filters={"and":[{"field":"owner","value":self.owner},{"field":"chat_name","value":self.chat_name}]},
                                         sorts =[{"created_time":"desc"}],
                                        keyword=None,fields=[],
                                        vector=[],vectorField=None,
                                        limit=100)])
        docs.reverse()
        conversations = [{"content":doc["raw_content"],"role":doc["role"]} for doc in docs]
                         
        last_conversation = [{"role":"user","content":'''开始'''}]
        
        # always choose the last six messages to generate the reply
        c_messages = conversations[-7:-1]                
        _,v2 = self.generate_llm_reply(raw_message,message_utils.padding_messages_merge(self._system_message + c_messages + last_conversation),sender)
        print(f"rhetorical: {v2}",flush=True)

        try:            
            v = json.loads(code_utils.extract_code(v2)[-1][1])
            for temp in v:
                self.simple_retrieval_client.save_text_content(owner=self.owner,title="",content=temp,url="rhetorical",auto_chunking=False)
        except Exception:
            print(f"rhetorical error: {v2}",flush=True)                
        return True, None 
                
        
                    

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/agent/extensions/preview_file_agent.py
from ..conversable_agent import ConversableAgent
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
from ....utils.client import ByzerLLM,message_utils
from byzerllm.utils.retrieval import ByzerRetrieval
from ..agent import Agent
from ray.util.client.common import ClientActorHandle, ClientObjectRef
import time
from .. import get_agent_name,run_agent_func,ChatResponse,modify_last_message,modify_message_content
from langchain import PromptTemplate
import json


class PreviewFileAgent(ConversableAgent):    

    DEFAULT_SYSTEM_MESSAGE = """You are a helpful AI assistant. You will use your knowledge to help the user to preview the file.Try
to use Python and Pandas to read the file and show the first 5 rows of the file. The user will mention the file path in his/her question.
The packages all are installed, you can use it directly.
Try to generate python code which should match the following requirements:
1. try to read the file according the suffix of file name in Try block
2. if read success, set variable loaded_successfully to True, otherwise set it to False.
3. if loaded_successfully is True, then assigh the loaded data with head() to file_preview, otherwise assign error message to file_preview
4. make sure the loaded_successfully, file_preview are defined in the global scope
"""    
    
    DEFAULT_USER_MESSAGE = """
We have a file, the path is: {file_path}. Try to  write code preview this file. Make sure the {file_path} is defined in the code. We need to 
execute the code to preview the file. If the code is correct, the file will be loaded successfully and the first 5 rows of the file will be shown.
"""

    def __init__(
        self,
        name: str,
        llm: ByzerLLM,
        retrieval: ByzerRetrieval,
        chat_name:str,
        owner:str,
        code_agent: Union[Agent, ClientActorHandle,str],
        system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,        
        is_termination_msg: Optional[Callable[[Dict], bool]] = None,
        max_consecutive_auto_reply: Optional[int] = None,
        human_input_mode: Optional[str] = "NEVER",
        code_execution_config: Optional[Union[Dict, bool]] = False,
        **kwargs,
    ):
        super().__init__(
            name,
            llm,retrieval,
            system_message,
            is_termination_msg,
            max_consecutive_auto_reply,
            human_input_mode,
            code_execution_config=code_execution_config,            
            **kwargs,
        )
        self.code_agent = code_agent
        self._reply_func_list = []
        # self.register_reply([Agent, ClientActorHandle,str], ConversableAgent.generate_llm_reply)   
        self.register_reply([Agent, ClientActorHandle,str], PreviewFileAgent.generate_code_reply) 
        self.register_reply([Agent, ClientActorHandle,str], PreviewFileAgent.reply_python_code_agent) 
        self.register_reply([Agent, ClientActorHandle,str], ConversableAgent.check_termination_and_human_reply) 

    def reply_python_code_agent(
        self,
        raw_message: Optional[Union[Dict,str,ChatResponse]] = None,
        messages: Optional[List[Dict]] = None,
        sender: Optional[Union[ClientActorHandle,Agent,str]] = None,
        config: Optional[Any] = None,
    ) -> Tuple[bool, Union[str, Dict, None,ChatResponse]]: 
        if get_agent_name(sender) != get_agent_name(self.code_agent):
            return False, None
        
        if messages is None:
            messages = self._messages[get_agent_name(sender)]

        message = messages[-1]
        raw_message = messages[-1]["metadata"]["execute_result"]

        if raw_message.status == 0 and "loaded_successfully" in raw_message.variables and raw_message.variables["loaded_successfully"]:                                 
            # stop the conversation if the code agent gives the success message
            return True, None
        else:
            print(raw_message,flush=True)
            if message_utils.check_error_count(message,3):
                return True, {
                    "content":f'''Fail to load the file: {raw_message.variables.get("file_path","")}. reason: {raw_message.output}''' + "\nTERMINATE",
                    "metadata":{"TERMINATE":True,"code":1}
                }
            # the code may be wrong, so generate a new code according to the conversation so far 
            extra_messages = []            
            if "loaded_successfully" not in raw_message.variables:                
                extra_messages.append(self.create_temp_message("loaded_successfully is not defined"))
            
            elif raw_message.variables["loaded_successfully"] is False:                            
                extra_messages.append(self.create_temp_message("loaded_successfully is False, it means the file is not loaded successfully, check the file path and the code then try again"))
                

            _,code = self.generate_llm_reply(raw_message,messages + extra_messages,sender)
            m = self.create_temp_message(code)
            message_utils.copy_error_count(message,m)

            return True, message_utils.inc_error_count(m)
        
        
    
    def generate_code_reply(
        self,
        raw_message: Optional[Union[Dict,str,ChatResponse]] = None,
        messages: Optional[List[Dict]] = None,
        sender: Optional[Union[ClientActorHandle,Agent,str]] = None,
        config: Optional[Any] = None,
    ) -> Tuple[bool, Union[str, Dict, None,ChatResponse]]:  
        
        if messages is None:
            messages = self._messages[get_agent_name(sender)]
        
        new_message = messages[-1] 
        file_path = new_message["metadata"]["file_path"]
        content = PromptTemplate.from_template(self.DEFAULT_USER_MESSAGE).format(file_path=new_message["metadata"]["file_path"])
        new_messages = modify_last_message(messages,modify_message_content(new_message,content))
        
        _,code = self.generate_llm_reply(raw_message,new_messages,sender)            
        
        # only the first time we should keep the message sent to the code agent which have file_path, file_ref
        # in message metadata, when the sandbox is created, then we will reuse the sandbox, no need to contain
        # the file_path, file_ref in the message metadata.         
        self.send(message=self.create_temp_message(code,new_message),recipient=self.code_agent)

        # get the code agent's reply
        last_message = self._messages[get_agent_name(self.code_agent)][-1]
        if last_message["metadata"].get("code",0) != 0:
            return True, {"content":f'''Fail to load the file: {file_path}. reason: {response.variables.get("file_preview","")}''',"metadata":{"TERMINATE":True,"code":1}}
        
        response:ChatResponse = last_message["metadata"]["execute_result"]

        if "loaded_successfully" not in response.variables or response.variables["loaded_successfully"] is False:
            return True, {"content":f'''Fail to load the file: {file_path}. reason: {response.variables.get("file_preview","")}''',"metadata":{"TERMINATE":True,"code":1}}
        
        file_preview = response.variables["file_preview"].to_csv(index=False)    
        
        return True, {"content":file_preview,"metadata":{"TERMINATE":True}}
            
            
        

        
        
    def create_temp_message(self,code,original_message=None):
        temp_message = {
            "content":code,
            "metadata":{
                "target_names":{"loaded_successfully":None,"file_preview":None}
            },
            "role":"user"
        }
        if original_message is not None:
            temp_message["metadata"] = {**original_message["metadata"],**temp_message["metadata"]}
        return temp_message    
        

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/agent/extensions/sql_reviewer_agent.py
from ..conversable_agent import ConversableAgent
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
from ....utils.client import ByzerLLM,message_utils,code_utils
from byzerllm.utils.retrieval import ByzerRetrieval
from ..agent import Agent
from ray.util.client.common import ClientActorHandle, ClientObjectRef
from .. import get_agent_name,run_agent_func,ChatResponse
import json
try:
    from termcolor import colored
except ImportError:

    def colored(x, *args, **kwargs):
        return x
    
class SQLReviewerAgent(ConversableAgent): 
    DEFAULT_SYSTEM_MESSAGE='''你的任务是检查用户发送给你的Spark SQL语句。
请仔细阅读用户发给你 Spark SQL，具体请按如下方式步骤进行检查，务必一步一步来：

1. 仔细检查SQL中的每一个字段和别名是否都使用反引号（backtick）或者反撇号（grave accent）`括起来。
2. SQL中不允许有文字描述提到让用户用户手动输入的内容，比如 "请输入"，"请填写" 等。
3. SQL中不允许出现诸如 ?, ?, ..., ? 这种参数化查询
4. SQL中不需有诸如 "有取 M 条记录， Top N类" 这种描述性的内容

请输出每一步检查结果，并说明原因。  
                                          
最后，对上面的检查结果和原因重新以json数组格式输出：

```json
[{
    id： 检查步骤序号,
    pass: 是否通过，true/false,
    reason: 原因                                    
}]                      
```
'''
    def __init__(
        self,
        name: str,
        llm: ByzerLLM,        
        retrieval: ByzerRetrieval, 
        chat_name:str,
        owner:str,               
        system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,        
        is_termination_msg: Optional[Callable[[Dict], bool]] = None,
        max_consecutive_auto_reply: Optional[int] = None,
        human_input_mode: Optional[str] = "NEVER",
        code_execution_config: Optional[Union[Dict, bool]] = False,
        **kwargs,
    ):       
        super().__init__(
            name,
            llm,retrieval,
            system_message,
            is_termination_msg,
            max_consecutive_auto_reply,
            human_input_mode,
            code_execution_config=code_execution_config,            
            **kwargs,
        )

        self._reply_func_list = []
        # self.register_reply([Agent, ClientActorHandle,str], ConversableAgent.generate_llm_reply)   
        self.register_reply([Agent, ClientActorHandle,str], SQLReviewerAgent.generate_review_reply) 
        self.register_reply([Agent, ClientActorHandle,str], ConversableAgent.check_termination_and_human_reply) 

    def generate_review_reply(
        self,
        raw_message: Optional[Union[Dict,str,ChatResponse]] = None,
        messages: Optional[List[Dict]] = None,
        sender: Optional[Union[ClientActorHandle,Agent,str]] = None,
        config: Optional[Any] = None,
        ) -> Tuple[bool, Union[str, Dict, None,ChatResponse]]:  

        if messages is None:
            messages = self._messages[get_agent_name(sender)]

        message = messages[-1]                
        v = self.llm.chat_oai(conversations=message_utils.padding_messages_merge(self._system_message + messages[-1:]))
        print(f'''
review code:
    {message}. 
review result:
    {v[0].output}
''',flush=True)
        checks = json.loads(code_utils.extract_code(v[0].output)[-1][1])

        c = []
        for check in checks:
            if not check["pass"]:
                c.append(check["reason"])

        if len(c) > 0:
            t = "\n".join(c)
            new_message = {"content":f'''代码存在一些问题，具体的问题如下：\n{t}''',"metadata":{}}
            return True, message_utils.copy_error_count(message,new_message)
        
        t = self.llm.chat_oai(conversations=[
    {"role":"user",
    "content":f'''仔细检查下面的 Spark SQL：

{message["content"]}

请找出所有的字段以及别名，去掉函数，保留反引号，并将他们按照出现顺序，以json数组格式输出：

```json
[
  "字段或者别名"
]
```
'''}])
        try:
            fields = json.loads(code_utils.extract_code(t[0].output)[-1][1])
            for field in fields:
                for field in fields:            
                    if "`" not in field:
                        if f"`{field}`" not in message["content"]:
                            new_message = {"content":f'''代码存在问题，字段或者别名: {field} 没有被反引号括起来,请修改''',"metadata":{}}            
                            return True, message_utils.copy_error_count(message,new_message)
        except Exception:
            pass
        
        new_message = {"content":f'''代码没有问题，可以正常允许。''',"metadata":{}}                        
                     
        return True, message_utils.copy_error_count(message,new_message)

        

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/agent/extensions/spark_sql_agent.py
from ..conversable_agent import ConversableAgent
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union,Annotated
from ....utils.client import ByzerLLM,code_utils,message_utils,parallel_utils
from byzerllm.utils.retrieval import ByzerRetrieval
from ..agent import Agent
import numpy as np
from ray.util.client.common import ClientActorHandle, ClientObjectRef
import re
from .. import get_agent_name,run_agent_func,ChatResponse
from ....utils import generate_str_md5
from byzerllm.utils.client import LLMHistoryItem,LLMRequest
import json
from byzerllm.apps.agent.extensions.simple_retrieval_client import SimpleRetrievalClient
import pydantic
from datetime import datetime
from byzerllm.apps.agent.extensions.query_rewrite.context import QueryContext
from byzerllm.apps.agent.extensions.query_rewrite.condition import QueryCondition
from byzerllm.apps.agent.extensions.query_rewrite.time import QueryTime
from byzerllm.apps.agent.extensions.query_rewrite import Action
try:
    from termcolor import colored
except ImportError:

    def colored(x, *args, **kwargs):
        return x
    

class SparkSQLAgent(ConversableAgent): 
    DEFAULT_SYSTEM_MESSAGE='''你非常精通 Spark SQL, 并且能够根据用户的问题，基于提供的表信息，生成对应的 Spark SQL 语句。

下面是你具备的一些能力：

### 联系上下文分析
                                                                                                                               
当面对用户的问题时，要多回顾过往的对话，根据上下文获取补充信息，去理解用户的需求。

示例:
1. 用户问题： 2023别克君威的销量是多少？
2. 回答： 2023年别克君威的销量是 1000 辆
3. 用户问题： 2024年呢？

此时，无需再询问用户查询什么，结合上一次提问的内容，来理解问题。
结合上一次用户提的问题，用户的实际问题是： 2024别克君威的销量是多少？
这个时候再进一步生成SQL语句。

学习我上面的示例，拓展到其他的场景。

### 时刻结合用户给出的表信息来修正查询

通常用户会给出表的信息包括：
1. 表的名字和结构schema
2. 表的一些统计信息，比如表的字段枚举值等
3. 表的示例数据

### 日期处理能力

当你生成 SQL 时，涉及到日期字段，你需要参考表的 Schema 信息，自动将用户的日期表达式转换成表的日期格式。如果表中
使用多个字段来提供日期信息，比如年，月，日，优先使用他们，而不是使用复杂的日期格式。

### 其他能力

诸如 会根据用户的问题，自动分析出用户的查询意图，然后生成对应的SQL语句。                                                                                                                                                                         

特别需要注意的是：
1. 你生成的代码要用 SQL 代码块包裹，```sql\n你的代码```, 注意一定要Block需要用sql标注而非vbnet。
3. 生成的 Spark SQL 语句中，所有字段或者别名务必需要用反引号 `` 括起来，尤其是 as 关键字后面的别名。
4. 任何情况下都不要拆分成多段代码输出，请一次性生成完整的代码片段，确保代码的完整性。
'''
    def __init__(
        self,
        name: str,
        llm: ByzerLLM,        
        retrieval: ByzerRetrieval, 
        chat_name:str,
        owner:str,                            
        sql_reviewer_agent: Union[Agent, ClientActorHandle,str],
        byzer_engine_agent: Union[Agent, ClientActorHandle,str],      
        retrieval_cluster:str="data_analysis",
        retrieval_db:str="data_analysis",   
        system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,        
        is_termination_msg: Optional[Callable[[Dict], bool]] = None,
        max_consecutive_auto_reply: Optional[int] = None,
        human_input_mode: Optional[str] = "NEVER",
        code_execution_config: Optional[Union[Dict, bool]] = False,
        byzer_url="http:://127.0.0.1:9003/run/script",
        **kwargs,
    ):       
        super().__init__(
            name,
            llm,retrieval,
            system_message,
            is_termination_msg,
            max_consecutive_auto_reply,
            human_input_mode,
            code_execution_config=code_execution_config,            
            **kwargs,
        )
        
        self.retrieval_cluster = retrieval_cluster
        self.retrieval_db = retrieval_db
        self.chat_name = chat_name
        self.owner = owner
        self.simple_retrieval_client = SimpleRetrievalClient(llm=self.llm,
                                                        retrieval=self.retrieval,
                                                        retrieval_cluster=self.retrieval_cluster,
                                                        retrieval_db=self.retrieval_db,
                                                        ) 
        self.byzer_url = byzer_url        
        self.sql_reviewer_agent = sql_reviewer_agent
        self.byzer_engine_agent = byzer_engine_agent
        self._reply_func_list = []                
        # self.register_reply([Agent, ClientActorHandle,str], ConversableAgent.generate_llm_reply)   
        self.register_reply([Agent, ClientActorHandle,str], SparkSQLAgent.generate_sql_reply) 
        self.register_reply([Agent, ClientActorHandle,str], SparkSQLAgent.generate_execute_sql_reply)
        self.register_reply([Agent, ClientActorHandle,str], SparkSQLAgent.generate_reply_for_reviview)        
        self.register_reply([Agent, ClientActorHandle,str], ConversableAgent.check_termination_and_human_reply)         

    def generate_sql_reply(
        self,
        raw_message: Optional[Union[Dict,str,ChatResponse]] = None,
        messages: Optional[List[Dict]] = None,
        sender: Optional[Union[ClientActorHandle,Agent,str]] = None,
        config: Optional[Any] = None,
        ) -> Tuple[bool, Union[str, Dict, None,ChatResponse]]:  

        if get_agent_name(sender) == get_agent_name(self.sql_reviewer_agent):
            return False,None
        
        if messages is None:
            messages = self._messages[get_agent_name(sender)]
         
        m = messages[-1]        

        if not m.get("metadata",{}).get("skip_long_memory",False): 
            # recall old memory and update the system prompt
            old_memory = self.simple_retrieval_client.search_content(q=m["content"],owner=self.owner,url="rhetorical",limit=3)
            if len(old_memory) != 0:
                c = json.dumps(old_memory,ensure_ascii=False)
                self.update_system_message(f'''{self.system_message}\n

下面是用户的一些行为偏好，在回答问题的时候，可以参考：
```json
{c}                                       
```  
''') 

        # query rewrite                        
        if len(re.sub(r'\s+', '', m["content"])) < 60:

            # context query rewrite
            rewriter = QueryContext(self.llm,self.retrieval,self._system_message,messages)
            r = rewriter.apply()
            m["content"] = r.extra_info["new_query"]

            # time query rewrite
            rewriter = QueryTime(self.llm,self.retrieval,self._system_message,messages)
            r = rewriter.apply()
            time_msg = r.extra_info["time_msg"]
            
            # structure query rewrite            
            rewriter = QueryCondition(self.llm,self.retrieval,self._system_message,messages,
                                      time_msg=time_msg)
            r = rewriter.apply()

            # # we may need more information from the user
            # if r.action == Action.STOP:
            #     return True, r.message
            
            key_msg = r.extra_info["key_msg"] 
            m["content"] = f'''补充信息：{time_msg} {key_msg} \n原始问题：{m["content"]} '''                       
        
        
        # try to awnser the user's question or generate sql
        
        temp_conversation = []
        _,v = self.generate_llm_reply(raw_message,message_utils.padding_messages_merge(messages+temp_conversation),sender)
        codes = code_utils.extract_code(v)
        has_sql_code = code_utils.check_target_codes_exists(codes,["sql"])         

        # if we have sql code, ask the sql reviewer to review the code         
        if has_sql_code: 
            
            # sync the question to the sql reviewer                           
            self.send(message_utils.un_termindate_message(messages[-1]),self.sql_reviewer_agent,request_reply=False)
            
            # send the sql code to the sql reviewer to review            
            self.send({
                    "content":f'''
```sql
{code_utils.get_target_codes(codes,["sql"])[0]}
```
'''},self.sql_reviewer_agent)
            
            # get the sql reviewed.             
            conversation = message_utils.un_termindate_message(self.chat_messages[get_agent_name(self.sql_reviewer_agent)][-1])            
            
            if conversation["content"] == "FAIL TO GENERATE SQL CODE":
                return True, {"content":f'Fail to generate sql code.',"metadata":{"TERMINATE":True}}
            
            # send the sql code to the byzer engine to execute
            print(f"send the sql code to the byzer engine to execute {conversation}",flush=True)
            self.send(message=conversation,recipient=self.byzer_engine_agent)  
            
            execute_result = self.chat_messages[get_agent_name(self.byzer_engine_agent)][-1]             
            print(f"execute_result: {execute_result}",flush=True)
            
            if message_utils.is_success(execute_result):
                return True,{"content":execute_result["content"],"metadata":{"TERMINATE":True,"rewrite_query":m["content"],"sql":conversation}}
            else:
                return True,{"content":f'Fail to execute the analysis. {execute_result["content"]}',"metadata":{"TERMINATE":True}}

        return True,  {"content":v,"metadata":{"TERMINATE":True}}
    
    def generate_execute_sql_reply(
        self,
        raw_message: Optional[Union[Dict,str,ChatResponse]] = None,
        messages: Optional[List[Dict]] = None,
        sender: Optional[Union[ClientActorHandle,Agent,str]] = None,
        config: Optional[Any] = None,
        ) -> Tuple[bool, Union[str, Dict, None,ChatResponse]]: 
        
        if get_agent_name(sender) != get_agent_name(self.byzer_engine_agent):
            return False, None
        
        if messages is None:
            messages = self._messages[get_agent_name(sender)] 
                
        message = messages[-1]
        if message["metadata"]["code"] == 0:
            return True, None
                
        if message_utils.check_error_count(message,max_error_count=3):
            return True, None
        
        last_conversation = [{"role":"user","content":'''请根据上面的错误，修正你的代码。注意，除了修正指定的错误以外，请确保 SQL 语句其他部分不要变更。'''}]   
        t = self.llm.chat_oai(conversations=message_utils.padding_messages_merge(self._system_message + messages + last_conversation))
        _,new_code = code_utils.extract_code(t[0].output)[0]
        new_message = {"content":f'''
```sql
{new_code}
```
'''}    
        message_utils.copy_error_count(message,new_message)
        return True, message_utils.inc_error_count(new_message)

        
    def generate_reply_for_reviview(
        self,
        raw_message: Optional[Union[Dict,str,ChatResponse]] = None,
        messages: Optional[List[Dict]] = None,
        sender: Optional[Union[ClientActorHandle,Agent,str]] = None,
        config: Optional[Any] = None,
        ) -> Tuple[bool, Union[str, Dict, None,ChatResponse]]: 
        
        if get_agent_name(sender) != get_agent_name(self.sql_reviewer_agent):
            return False, None
        
        if messages is None:
            messages = self._messages[get_agent_name(sender)] 

        target_message = {
            "content":"FAIL TO GENERATE SQL CODE",
            "metadata":{"TERMINATE":True},
        }    
        
        # check if code is passed or not by the sql reviewer
        
        def run_code():  
            '''
            用户表达肯定的观点或者代码没有问题，请调用我
            '''              
            return 0        
    
        def ignore():  
            '''
            用户表达否定或者代码不符合特定规范，可以调用我
            '''              
            return 1    
        

        last_conversation = messages[-1] 
        temp_conversation = {
            "role":"user",
            "content":"注意，你只需要判断调用哪个函数，并不需要解决问题。",
        }    

        ts= parallel_utils.chat_oai(self.llm,1,
                                conversations=message_utils.padding_messages_merge([last_conversation,temp_conversation]),
                                tools=[run_code,ignore],
                                execute_tool=True)
        t = None
        for temp in  ts:
            if temp[0].values:
                t = temp
                break

        # t = self.llm.chat_oai(conversations=[last_conversation],
        #                   tools=[run_code,ignore],
        #                   execute_tool=True)  
        
        if t and t[0].values:               
            if t[0].values[0] == 0:
                target_message["content"] = messages[-2]["content"]                
            else:   
                print(f"Fail to pass the review: {last_conversation}. Try to regenerate the sql",flush=True)             
                t = self.llm.chat_oai(conversations=message_utils.padding_messages_merge(self._system_message + messages+[{
                    "content":'''请修正你的代码。注意，除了修正指定的错误以外，请确保 SQL 语句其他部分不要变更,代码需要用 ```sql```包裹起来。''',
                    "role":"user"
                }]))
                print(f"Try to regenerate the sql: {t[0].output}",flush=True)
                sql_codes = code_utils.get_target_codes(code_utils.extract_code(t[0].output),["sql"])
                if sql_codes:
                    target_message["content"] = sql_codes[0]
                    target_message["metadata"]["TERMINATE"] = False
                    message_utils.inc_error_count(target_message)
        else:        
            print(f"Fail to recognize the reveiw result: {last_conversation}",flush=True)
        ## make sure the last message is the reviewed sql code    
        return True, target_message   

        
    
        
        



            

        

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/agent/extensions/common_agent.py
from .conversable_agent import ConversableAgent
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
from ...utils.client import ByzerLLM
from byzerllm.utils.retrieval import ByzerRetrieval
from .agent import Agent
from ray.util.client.common import ClientActorHandle, ClientObjectRef
import time
from . import get_agent_name,run_agent_func,ChatResponse


class CommonAgent(ConversableAgent):    

    DEFAULT_SYSTEM_MESSAGE = """You are a helpful AI assistant.
Based on the conversation and try you best to answer the user's quesion.
"""

    def __init__(
        self,
        name: str,
        llm: ByzerLLM,
        retrieval: ByzerRetrieval,
        chat_name:str,
        owner:str,
        code_agent: Union[Agent, ClientActorHandle,str],
        system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,        
        is_termination_msg: Optional[Callable[[Dict], bool]] = None,
        max_consecutive_auto_reply: Optional[int] = None,
        human_input_mode: Optional[str] = "NEVER",
        code_execution_config: Optional[Union[Dict, bool]] = False,
        **kwargs,
    ):
        super().__init__(
            name,
            llm,retrieval,
            system_message,
            is_termination_msg,
            max_consecutive_auto_reply,
            human_input_mode,
            code_execution_config=code_execution_config,            
            **kwargs,
        )
        self.code_agent = code_agent
        self._reply_func_list = []
        self.register_reply([Agent, ClientActorHandle,str], ConversableAgent.generate_llm_reply)           
        self.register_reply([Agent, ClientActorHandle,str], ConversableAgent.check_termination_and_human_reply) 
        
        

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/agent/extensions/query_rewrite/context.py
from typing import List,Dict,Annotated,Any
from byzerllm.utils.client import ByzerLLM,message_utils,code_utils,LLMRequest
from byzerllm.utils.retrieval import ByzerRetrieval
import json
import numpy as np
import copy
import pydantic
from . import QueryRewriteResult,Action

class QueryContext:
    '''
    this tool is used to extract the key messages from the user's question which
    is used to generate the sql code conditions e.g. filter conditions, group by conditions, order by conditions
    or aggregate fields.
    '''
    def __init__(self,llm:ByzerLLM, retrieval:ByzerRetrieval,
                 sys_message:List[Dict[str,Any]],
                 messages:List[Dict[str,Any]],**kwargs):
        self.messages = messages
        self._system_message = sys_message
        self.llm = llm
        self.retrieval = retrieval
        self.params = kwargs

    def apply(self):        
        m = copy.deepcopy(self.messages[-1])
        temp_conversation = [{"role":"user","content":'''
首先，根据我们前面几条聊天内容，针对我现在的问题，进行一个信息扩充。如果我的问题信息已经比较充足，
则输出原有问题即可。
     
注意：
1. 不要询问用户问题          
2. 不要生成SQL
3. 不要额外添加上下文中不存在的信息
4. 不要关注时间,不要改写时间
6. 尽量保证信息完整 
'''}]
        class SingleLine(pydantic.BaseModel):
            content:str=pydantic.Field(...,description="改写后的query")

        t = self.llm.chat_oai(
            conversations=message_utils.padding_messages_merge(self._system_message + self.messages + self.params.get("temp_conversation",temp_conversation)),
            response_class=SingleLine,
            enable_default_sys_message=True
            ) 
        new_query = m["content"]           
        if t[0].value:
            new_query = t[0].value.content               
            print(f'context query rewrite: {m["content"]} -> {new_query}\n\n',flush=True)          
            m["content"] = new_query   
    #         if new_query != m["content"]:
    #             temp1 = self.llm.emb(None,LLMRequest(instruction=new_query))
    #             temp2 = self.llm.emb(None,LLMRequest(instruction=m["content"]))
    #             sim = np.dot(temp1[0].output,temp2[0].output)
    #             if sim > 0.8:
    #                 print(f'context query rewrite: {m["content"]} -> {new_query}\n\n',flush=True)
    #                 m["content"] = new_query                    
    #             else:
    #                 print(f'''context query rewrite fail. 
    # the similarity is too low {sim}
    # query:  {m["content"]}
    # new_query: {new_query}
    # \n\n''',flush=True)

        return QueryRewriteResult(message = m,action = Action.CONTINUE,extra_info={"new_query":new_query})       
             


    

##File: /home/winubuntu/projects/byzer-llm/src/byzerllm/apps/agent/extensions/query_rewrite/condition.py
from typing import List,Dict,Annotated,Any,Optional
from byzerllm.utils.client import ByzerLLM,message_utils,code_utils
from byzerllm.utils.retrieval import ByzerRetrieval
import copy
import json
import pydantic
from . import QueryRewriteResult,Action

class QueryCondition:
    '''
    this tool is used to extract the key messages from the user's question which
    is used to generate the sql code conditions e.g. filter conditions, group by conditions, order by conditions
    or aggregate fields.
    '''
    def __init__(self,llm:ByzerLLM, retrieval:ByzerRetrieval,
                 sys_message:List[Dict[str,Any]],
                 messages:List[Dict[str,Any]],
                 **kwargs):
        self.messages = messages
        self._system_message = sys_message
        self.llm = llm
        self.retrieval = retrieval
        self.params = kwargs        

    def apply(self)->QueryRewriteResult:        
        m = copy.deepcopy(self.messages[-1])
        time_msg = self.params.get("time_msg","")                       
        class KV(pydantic.BaseModel):
            name:str = pydantic.Field(description="表字段名称")
            value:Optional[str] = pydantic.Field(default="",description="从问题得到的值")
            tpe:Optional[str] = pydantic.Field(default="",description="类型，过滤条件，指标，分组条件等")
                                       

        class Conditions(pydantic.BaseModel):
            items:List[KV]=pydantic.Field(...,description="the key value pairs of the query conditions")

        temp_conversation = [{"role":"user","content":'''
首先根据我的问题，关联前面的对话，尤其是前面的表结构表结构信息，示例数据，表统计信息等，找到我当前问题中的关键信息,
诸如过滤条件，指标，分组条件。不需要生成SQL。                      

具体请按如下方式步骤补充信息，务必一步一步来：

1. 回顾前面的会话，对提到的表结构信息，示例数据，表统计信息等进行回顾，列出字段列表。
2. 对当前问题进行拆解，找到可能的过滤条件，指标，分组条件等。以 字段名称=值 的形式罗列出来。
3. 根据表统计信息中列表，对第二步过滤条件的值进行修正。具体做法是，如果过滤条件字段在表统计信息中有枚举值，
检查过滤字段的值是否在枚举值中，如果不在，找到最接近的枚举值，修正过滤条件的值。
4. 对最后修正的结果，重新以 Json 格式进行输出

请输出每一步的结果。                                                                                          
        '''}] 

         
        t = self.llm.chat_oai(conversations=message_utils.padding_messages_merge(
            self._system_message  + self.messages + self.params.get("temp_conversation",temp_conversation)),
            response_class=Conditions,
            enable_default_sys_message=True)
        action = Action.CONTINUE
        key_msg = ""
        
        if t[0].value:
            v:Conditions = t[0].value
            for item in v.items:
                key_msg += f''' {item.name}={item.value}'''

            old_content = m["content"]
            m["content"] = f'''补充信息：{time_msg} {key_msg} \n原始问题：{old_content} '''
            print(f'final query:{m["content"]}\n\n',flush=True)                                              
        
        return QueryRewriteResult(message = m,action = action,extra_info={"key_msg":key_msg})        
             


    

    

用Python 打印hello world
    